{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gan1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "neg = pd.read_csv(\"neg4x.csv\")\n",
    "pos = pd.read_csv(\"pos.csv\")\n",
    "neg_t = pd.read_csv(\"neg1x.csv\")\n",
    "cols = list(pos)\n",
    "test = pd.concat([pos,neg_t])[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>npi</th>\n",
       "      <th>year</th>\n",
       "      <th>line_srvc_cnt_sum</th>\n",
       "      <th>bene_unique_cnt_sum</th>\n",
       "      <th>bene_day_srvc_cnt_sum</th>\n",
       "      <th>average_submitted_chrg_amt_sum</th>\n",
       "      <th>average_medicare_payment_amt_sum</th>\n",
       "      <th>line_srvc_cnt_mean</th>\n",
       "      <th>bene_unique_cnt_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Slide Preparation Facility</th>\n",
       "      <th>Speech Language Pathologist</th>\n",
       "      <th>Sports Medicine</th>\n",
       "      <th>Surgical Oncology</th>\n",
       "      <th>Thoracic Surgery</th>\n",
       "      <th>Unknown Physician Specialty Code</th>\n",
       "      <th>Unknown Supplier/Provider</th>\n",
       "      <th>Urology</th>\n",
       "      <th>Vascular Surgery</th>\n",
       "      <th>exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3397</td>\n",
       "      <td>1043217052</td>\n",
       "      <td>2012</td>\n",
       "      <td>5270.0</td>\n",
       "      <td>428</td>\n",
       "      <td>1213</td>\n",
       "      <td>886.771365</td>\n",
       "      <td>341.642700</td>\n",
       "      <td>658.750000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3398</td>\n",
       "      <td>1043217052</td>\n",
       "      <td>2013</td>\n",
       "      <td>5436.0</td>\n",
       "      <td>430</td>\n",
       "      <td>1241</td>\n",
       "      <td>700.076923</td>\n",
       "      <td>272.936370</td>\n",
       "      <td>679.500000</td>\n",
       "      <td>53.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3399</td>\n",
       "      <td>1043217052</td>\n",
       "      <td>2014</td>\n",
       "      <td>2915.0</td>\n",
       "      <td>265</td>\n",
       "      <td>568</td>\n",
       "      <td>484.222222</td>\n",
       "      <td>148.490504</td>\n",
       "      <td>485.833333</td>\n",
       "      <td>44.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13987</td>\n",
       "      <td>1871702613</td>\n",
       "      <td>2014</td>\n",
       "      <td>2697.0</td>\n",
       "      <td>141</td>\n",
       "      <td>242</td>\n",
       "      <td>503.419391</td>\n",
       "      <td>207.500389</td>\n",
       "      <td>449.500000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13988</td>\n",
       "      <td>1871702613</td>\n",
       "      <td>2015</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.813600</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17267</td>\n",
       "      <td>1043302250</td>\n",
       "      <td>2012</td>\n",
       "      <td>7944.9</td>\n",
       "      <td>131</td>\n",
       "      <td>897</td>\n",
       "      <td>812.951122</td>\n",
       "      <td>167.058038</td>\n",
       "      <td>3972.450000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41320</td>\n",
       "      <td>1659699049</td>\n",
       "      <td>2012</td>\n",
       "      <td>31612.0</td>\n",
       "      <td>96</td>\n",
       "      <td>6034</td>\n",
       "      <td>492.398417</td>\n",
       "      <td>186.636139</td>\n",
       "      <td>15806.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>125226</td>\n",
       "      <td>1356341911</td>\n",
       "      <td>2012</td>\n",
       "      <td>466.0</td>\n",
       "      <td>189</td>\n",
       "      <td>362</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>133.751316</td>\n",
       "      <td>116.500000</td>\n",
       "      <td>47.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>141724</td>\n",
       "      <td>1487605515</td>\n",
       "      <td>2012</td>\n",
       "      <td>394.0</td>\n",
       "      <td>388</td>\n",
       "      <td>394</td>\n",
       "      <td>2256.997000</td>\n",
       "      <td>254.949211</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>141725</td>\n",
       "      <td>1487605515</td>\n",
       "      <td>2013</td>\n",
       "      <td>425.0</td>\n",
       "      <td>419</td>\n",
       "      <td>425</td>\n",
       "      <td>4408.315637</td>\n",
       "      <td>488.568043</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>104.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>141726</td>\n",
       "      <td>1487605515</td>\n",
       "      <td>2014</td>\n",
       "      <td>480.0</td>\n",
       "      <td>475</td>\n",
       "      <td>480</td>\n",
       "      <td>4511.869231</td>\n",
       "      <td>463.270718</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>118.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>141727</td>\n",
       "      <td>1487605515</td>\n",
       "      <td>2015</td>\n",
       "      <td>572.0</td>\n",
       "      <td>558</td>\n",
       "      <td>572</td>\n",
       "      <td>7517.059997</td>\n",
       "      <td>637.988100</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>144877</td>\n",
       "      <td>1508837097</td>\n",
       "      <td>2012</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>1165</td>\n",
       "      <td>1370</td>\n",
       "      <td>1525.858611</td>\n",
       "      <td>843.125508</td>\n",
       "      <td>124.545455</td>\n",
       "      <td>105.909091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>144878</td>\n",
       "      <td>1508837097</td>\n",
       "      <td>2013</td>\n",
       "      <td>3658.0</td>\n",
       "      <td>2460</td>\n",
       "      <td>3658</td>\n",
       "      <td>1447.705113</td>\n",
       "      <td>805.625168</td>\n",
       "      <td>281.384615</td>\n",
       "      <td>189.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>144879</td>\n",
       "      <td>1508837097</td>\n",
       "      <td>2014</td>\n",
       "      <td>211.0</td>\n",
       "      <td>209</td>\n",
       "      <td>211</td>\n",
       "      <td>298.615084</td>\n",
       "      <td>77.959419</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>52.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>153599</td>\n",
       "      <td>1578510723</td>\n",
       "      <td>2012</td>\n",
       "      <td>254.0</td>\n",
       "      <td>102</td>\n",
       "      <td>253</td>\n",
       "      <td>10859.487097</td>\n",
       "      <td>357.368673</td>\n",
       "      <td>50.800000</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>153600</td>\n",
       "      <td>1578510723</td>\n",
       "      <td>2013</td>\n",
       "      <td>142.0</td>\n",
       "      <td>44</td>\n",
       "      <td>142</td>\n",
       "      <td>603.000000</td>\n",
       "      <td>171.365226</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>162881</td>\n",
       "      <td>1639284060</td>\n",
       "      <td>2012</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>417.272727</td>\n",
       "      <td>83.770909</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>170267</td>\n",
       "      <td>1699748228</td>\n",
       "      <td>2012</td>\n",
       "      <td>6905.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>6248</td>\n",
       "      <td>2160.725163</td>\n",
       "      <td>617.517568</td>\n",
       "      <td>690.500000</td>\n",
       "      <td>200.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>170268</td>\n",
       "      <td>1699748228</td>\n",
       "      <td>2013</td>\n",
       "      <td>6592.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>6592</td>\n",
       "      <td>2226.032836</td>\n",
       "      <td>649.982967</td>\n",
       "      <td>659.200000</td>\n",
       "      <td>172.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>170269</td>\n",
       "      <td>1699748228</td>\n",
       "      <td>2014</td>\n",
       "      <td>6459.0</td>\n",
       "      <td>1767</td>\n",
       "      <td>6456</td>\n",
       "      <td>2590.000000</td>\n",
       "      <td>604.485438</td>\n",
       "      <td>587.181818</td>\n",
       "      <td>160.636364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>170270</td>\n",
       "      <td>1699748228</td>\n",
       "      <td>2015</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229</td>\n",
       "      <td>229</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>30.434866</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>114.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>174961</td>\n",
       "      <td>1720280183</td>\n",
       "      <td>2012</td>\n",
       "      <td>9617.0</td>\n",
       "      <td>2837</td>\n",
       "      <td>9617</td>\n",
       "      <td>2092.244773</td>\n",
       "      <td>715.309470</td>\n",
       "      <td>739.769231</td>\n",
       "      <td>218.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>177038</td>\n",
       "      <td>1740280957</td>\n",
       "      <td>2013</td>\n",
       "      <td>163.0</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>6065.647230</td>\n",
       "      <td>391.432713</td>\n",
       "      <td>40.750000</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>177039</td>\n",
       "      <td>1740280957</td>\n",
       "      <td>2014</td>\n",
       "      <td>177.0</td>\n",
       "      <td>171</td>\n",
       "      <td>177</td>\n",
       "      <td>8698.227757</td>\n",
       "      <td>526.338552</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>177040</td>\n",
       "      <td>1740280957</td>\n",
       "      <td>2015</td>\n",
       "      <td>119.0</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "      <td>8435.207876</td>\n",
       "      <td>481.644864</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>180952</td>\n",
       "      <td>1770555724</td>\n",
       "      <td>2012</td>\n",
       "      <td>162.0</td>\n",
       "      <td>135</td>\n",
       "      <td>162</td>\n",
       "      <td>1458.876812</td>\n",
       "      <td>236.171232</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>183861</td>\n",
       "      <td>1790764207</td>\n",
       "      <td>2012</td>\n",
       "      <td>3116.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>2654</td>\n",
       "      <td>19118.263215</td>\n",
       "      <td>2845.409753</td>\n",
       "      <td>100.516129</td>\n",
       "      <td>61.096774</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>183862</td>\n",
       "      <td>1790764207</td>\n",
       "      <td>2013</td>\n",
       "      <td>4756.0</td>\n",
       "      <td>2910</td>\n",
       "      <td>4074</td>\n",
       "      <td>27107.607986</td>\n",
       "      <td>4003.926729</td>\n",
       "      <td>97.061224</td>\n",
       "      <td>59.387755</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>183863</td>\n",
       "      <td>1790764207</td>\n",
       "      <td>2014</td>\n",
       "      <td>6387.0</td>\n",
       "      <td>1939</td>\n",
       "      <td>6387</td>\n",
       "      <td>14712.752921</td>\n",
       "      <td>1869.489009</td>\n",
       "      <td>245.653846</td>\n",
       "      <td>74.576923</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>1895116</td>\n",
       "      <td>1760749204</td>\n",
       "      <td>2013</td>\n",
       "      <td>257.0</td>\n",
       "      <td>28</td>\n",
       "      <td>257</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>76.774630</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>553910</td>\n",
       "      <td>1528051448</td>\n",
       "      <td>2014</td>\n",
       "      <td>462.0</td>\n",
       "      <td>86</td>\n",
       "      <td>462</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>151.761809</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>1067757</td>\n",
       "      <td>1033206420</td>\n",
       "      <td>2015</td>\n",
       "      <td>227.0</td>\n",
       "      <td>157</td>\n",
       "      <td>227</td>\n",
       "      <td>403.281667</td>\n",
       "      <td>190.382006</td>\n",
       "      <td>45.400000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>97492</td>\n",
       "      <td>1144320136</td>\n",
       "      <td>2013</td>\n",
       "      <td>261.0</td>\n",
       "      <td>213</td>\n",
       "      <td>259</td>\n",
       "      <td>4018.871996</td>\n",
       "      <td>502.247462</td>\n",
       "      <td>32.625000</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>1925521</td>\n",
       "      <td>1851307136</td>\n",
       "      <td>2015</td>\n",
       "      <td>2612.0</td>\n",
       "      <td>1511</td>\n",
       "      <td>2067</td>\n",
       "      <td>1176.378189</td>\n",
       "      <td>838.346336</td>\n",
       "      <td>153.647059</td>\n",
       "      <td>88.882353</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>295007</td>\n",
       "      <td>1568637858</td>\n",
       "      <td>2015</td>\n",
       "      <td>4343.0</td>\n",
       "      <td>2572</td>\n",
       "      <td>4330</td>\n",
       "      <td>4954.832025</td>\n",
       "      <td>2238.663293</td>\n",
       "      <td>180.958333</td>\n",
       "      <td>107.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>649135</td>\n",
       "      <td>1437130333</td>\n",
       "      <td>2012</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>71.181875</td>\n",
       "      <td>52.534375</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>352638</td>\n",
       "      <td>1598770836</td>\n",
       "      <td>2013</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127</td>\n",
       "      <td>127</td>\n",
       "      <td>38.800000</td>\n",
       "      <td>38.041869</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>3300608</td>\n",
       "      <td>1548296734</td>\n",
       "      <td>2013</td>\n",
       "      <td>84.0</td>\n",
       "      <td>60</td>\n",
       "      <td>84</td>\n",
       "      <td>131.320000</td>\n",
       "      <td>71.209030</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>2300041</td>\n",
       "      <td>1053589184</td>\n",
       "      <td>2013</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>4732.000000</td>\n",
       "      <td>509.973095</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>203849</td>\n",
       "      <td>1942348123</td>\n",
       "      <td>2014</td>\n",
       "      <td>187.0</td>\n",
       "      <td>171</td>\n",
       "      <td>187</td>\n",
       "      <td>6936.673329</td>\n",
       "      <td>1267.429311</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>24.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>2542416</td>\n",
       "      <td>1992005227</td>\n",
       "      <td>2012</td>\n",
       "      <td>77.0</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>425.057352</td>\n",
       "      <td>289.010000</td>\n",
       "      <td>25.666667</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>2916125</td>\n",
       "      <td>1821102732</td>\n",
       "      <td>2013</td>\n",
       "      <td>3079.0</td>\n",
       "      <td>1098</td>\n",
       "      <td>3077</td>\n",
       "      <td>2655.869000</td>\n",
       "      <td>1946.581000</td>\n",
       "      <td>162.052600</td>\n",
       "      <td>57.789470</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>141679</td>\n",
       "      <td>1487601043</td>\n",
       "      <td>2014</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>1406.121457</td>\n",
       "      <td>220.426275</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1563769</td>\n",
       "      <td>1740242932</td>\n",
       "      <td>2012</td>\n",
       "      <td>4836.0</td>\n",
       "      <td>1205</td>\n",
       "      <td>4790</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>952.389467</td>\n",
       "      <td>268.666667</td>\n",
       "      <td>66.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>1844366</td>\n",
       "      <td>1639178023</td>\n",
       "      <td>2012</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>3178</td>\n",
       "      <td>3649.650794</td>\n",
       "      <td>1474.437991</td>\n",
       "      <td>144.545455</td>\n",
       "      <td>90.909091</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1191907</td>\n",
       "      <td>1427068642</td>\n",
       "      <td>2014</td>\n",
       "      <td>3558.0</td>\n",
       "      <td>737</td>\n",
       "      <td>3558</td>\n",
       "      <td>3789.000000</td>\n",
       "      <td>1026.062970</td>\n",
       "      <td>323.454545</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>3378272</td>\n",
       "      <td>1952601882</td>\n",
       "      <td>2012</td>\n",
       "      <td>48.0</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>60.606460</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1524918</td>\n",
       "      <td>1407856131</td>\n",
       "      <td>2014</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>705</td>\n",
       "      <td>1015</td>\n",
       "      <td>820.265584</td>\n",
       "      <td>308.049311</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>183449</td>\n",
       "      <td>1790715159</td>\n",
       "      <td>2012</td>\n",
       "      <td>266.0</td>\n",
       "      <td>232</td>\n",
       "      <td>266</td>\n",
       "      <td>8312.739233</td>\n",
       "      <td>930.952503</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1921914</td>\n",
       "      <td>1841222742</td>\n",
       "      <td>2015</td>\n",
       "      <td>2860.0</td>\n",
       "      <td>1201</td>\n",
       "      <td>2860</td>\n",
       "      <td>4089.976852</td>\n",
       "      <td>2222.257425</td>\n",
       "      <td>84.117647</td>\n",
       "      <td>35.323529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>923077</td>\n",
       "      <td>1275779399</td>\n",
       "      <td>2014</td>\n",
       "      <td>39.0</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>1130.000000</td>\n",
       "      <td>177.127250</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>1443216</td>\n",
       "      <td>1821046608</td>\n",
       "      <td>2012</td>\n",
       "      <td>2447.0</td>\n",
       "      <td>1091</td>\n",
       "      <td>2418</td>\n",
       "      <td>815.200000</td>\n",
       "      <td>284.390144</td>\n",
       "      <td>203.916667</td>\n",
       "      <td>90.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>220253</td>\n",
       "      <td>1417128323</td>\n",
       "      <td>2012</td>\n",
       "      <td>697.0</td>\n",
       "      <td>649</td>\n",
       "      <td>697</td>\n",
       "      <td>214.951580</td>\n",
       "      <td>89.375989</td>\n",
       "      <td>139.400000</td>\n",
       "      <td>129.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>1686864</td>\n",
       "      <td>1215943832</td>\n",
       "      <td>2015</td>\n",
       "      <td>3411.0</td>\n",
       "      <td>1698</td>\n",
       "      <td>3411</td>\n",
       "      <td>3209.578938</td>\n",
       "      <td>1505.416058</td>\n",
       "      <td>200.647059</td>\n",
       "      <td>99.882353</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>3390354</td>\n",
       "      <td>1255580130</td>\n",
       "      <td>2015</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>728.881100</td>\n",
       "      <td>116.263500</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>217944</td>\n",
       "      <td>1316175391</td>\n",
       "      <td>2013</td>\n",
       "      <td>449.0</td>\n",
       "      <td>438</td>\n",
       "      <td>449</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>99.678414</td>\n",
       "      <td>89.800000</td>\n",
       "      <td>87.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>3416607</td>\n",
       "      <td>1255320495</td>\n",
       "      <td>2015</td>\n",
       "      <td>685.0</td>\n",
       "      <td>330</td>\n",
       "      <td>685</td>\n",
       "      <td>776.818200</td>\n",
       "      <td>398.513200</td>\n",
       "      <td>97.857140</td>\n",
       "      <td>47.142860</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>78984</td>\n",
       "      <td>1003912353</td>\n",
       "      <td>2015</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>660</td>\n",
       "      <td>1073</td>\n",
       "      <td>3179.314634</td>\n",
       "      <td>681.904266</td>\n",
       "      <td>268.250000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>2146237</td>\n",
       "      <td>1811254493</td>\n",
       "      <td>2015</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>31.448571</td>\n",
       "      <td>21.770714</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2818 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         npi  year  line_srvc_cnt_sum  bene_unique_cnt_sum  \\\n",
       "0           3397  1043217052  2012             5270.0                  428   \n",
       "1           3398  1043217052  2013             5436.0                  430   \n",
       "2           3399  1043217052  2014             2915.0                  265   \n",
       "3          13987  1871702613  2014             2697.0                  141   \n",
       "4          13988  1871702613  2015               25.0                   13   \n",
       "5          17267  1043302250  2012             7944.9                  131   \n",
       "6          41320  1659699049  2012            31612.0                   96   \n",
       "7         125226  1356341911  2012              466.0                  189   \n",
       "8         141724  1487605515  2012              394.0                  388   \n",
       "9         141725  1487605515  2013              425.0                  419   \n",
       "10        141726  1487605515  2014              480.0                  475   \n",
       "11        141727  1487605515  2015              572.0                  558   \n",
       "12        144877  1508837097  2012             1370.0                 1165   \n",
       "13        144878  1508837097  2013             3658.0                 2460   \n",
       "14        144879  1508837097  2014              211.0                  209   \n",
       "15        153599  1578510723  2012              254.0                  102   \n",
       "16        153600  1578510723  2013              142.0                   44   \n",
       "17        162881  1639284060  2012               11.0                   11   \n",
       "18        170267  1699748228  2012             6905.0                 2008   \n",
       "19        170268  1699748228  2013             6592.0                 1725   \n",
       "20        170269  1699748228  2014             6459.0                 1767   \n",
       "21        170270  1699748228  2015              229.0                  229   \n",
       "22        174961  1720280183  2012             9617.0                 2837   \n",
       "23        177038  1740280957  2013              163.0                  161   \n",
       "24        177039  1740280957  2014              177.0                  171   \n",
       "25        177040  1740280957  2015              119.0                  115   \n",
       "26        180952  1770555724  2012              162.0                  135   \n",
       "27        183861  1790764207  2012             3116.0                 1894   \n",
       "28        183862  1790764207  2013             4756.0                 2910   \n",
       "29        183863  1790764207  2014             6387.0                 1939   \n",
       "...          ...         ...   ...                ...                  ...   \n",
       "1379     1895116  1760749204  2013              257.0                   28   \n",
       "1380      553910  1528051448  2014              462.0                   86   \n",
       "1381     1067757  1033206420  2015              227.0                  157   \n",
       "1382       97492  1144320136  2013              261.0                  213   \n",
       "1383     1925521  1851307136  2015             2612.0                 1511   \n",
       "1384      295007  1568637858  2015             4343.0                 2572   \n",
       "1385      649135  1437130333  2012               16.0                   15   \n",
       "1386      352638  1598770836  2013              127.0                  127   \n",
       "1387     3300608  1548296734  2013               84.0                   60   \n",
       "1388     2300041  1053589184  2013               49.0                   49   \n",
       "1389      203849  1942348123  2014              187.0                  171   \n",
       "1390     2542416  1992005227  2012               77.0                   71   \n",
       "1391     2916125  1821102732  2013             3079.0                 1098   \n",
       "1392      141679  1487601043  2014               32.0                   32   \n",
       "1393     1563769  1740242932  2012             4836.0                 1205   \n",
       "1394     1844366  1639178023  2012             3180.0                 2000   \n",
       "1395     1191907  1427068642  2014             3558.0                  737   \n",
       "1396     3378272  1952601882  2012               48.0                   38   \n",
       "1397     1524918  1407856131  2014             1015.0                  705   \n",
       "1398      183449  1790715159  2012              266.0                  232   \n",
       "1399     1921914  1841222742  2015             2860.0                 1201   \n",
       "1400      923077  1275779399  2014               39.0                   38   \n",
       "1401     1443216  1821046608  2012             2447.0                 1091   \n",
       "1402      220253  1417128323  2012              697.0                  649   \n",
       "1403     1686864  1215943832  2015             3411.0                 1698   \n",
       "1404     3390354  1255580130  2015               37.0                   26   \n",
       "1405      217944  1316175391  2013              449.0                  438   \n",
       "1406     3416607  1255320495  2015              685.0                  330   \n",
       "1407       78984  1003912353  2015             1073.0                  660   \n",
       "1408     2146237  1811254493  2015               14.0                   14   \n",
       "\n",
       "      bene_day_srvc_cnt_sum  average_submitted_chrg_amt_sum  \\\n",
       "0                      1213                      886.771365   \n",
       "1                      1241                      700.076923   \n",
       "2                       568                      484.222222   \n",
       "3                       242                      503.419391   \n",
       "4                        25                       28.000000   \n",
       "5                       897                      812.951122   \n",
       "6                      6034                      492.398417   \n",
       "7                       362                      333.000000   \n",
       "8                       394                     2256.997000   \n",
       "9                       425                     4408.315637   \n",
       "10                      480                     4511.869231   \n",
       "11                      572                     7517.059997   \n",
       "12                     1370                     1525.858611   \n",
       "13                     3658                     1447.705113   \n",
       "14                      211                      298.615084   \n",
       "15                      253                    10859.487097   \n",
       "16                      142                      603.000000   \n",
       "17                       11                      417.272727   \n",
       "18                     6248                     2160.725163   \n",
       "19                     6592                     2226.032836   \n",
       "20                     6456                     2590.000000   \n",
       "21                      229                      165.000000   \n",
       "22                     9617                     2092.244773   \n",
       "23                      163                     6065.647230   \n",
       "24                      177                     8698.227757   \n",
       "25                      119                     8435.207876   \n",
       "26                      162                     1458.876812   \n",
       "27                     2654                    19118.263215   \n",
       "28                     4074                    27107.607986   \n",
       "29                     6387                    14712.752921   \n",
       "...                     ...                             ...   \n",
       "1379                    257                      283.000000   \n",
       "1380                    462                      295.000000   \n",
       "1381                    227                      403.281667   \n",
       "1382                    259                     4018.871996   \n",
       "1383                   2067                     1176.378189   \n",
       "1384                   4330                     4954.832025   \n",
       "1385                     16                       71.181875   \n",
       "1386                    127                       38.800000   \n",
       "1387                     84                      131.320000   \n",
       "1388                     49                     4732.000000   \n",
       "1389                    187                     6936.673329   \n",
       "1390                     77                      425.057352   \n",
       "1391                   3077                     2655.869000   \n",
       "1392                     32                     1406.121457   \n",
       "1393                   4790                     3231.000000   \n",
       "1394                   3178                     3649.650794   \n",
       "1395                   3558                     3789.000000   \n",
       "1396                     42                      120.000000   \n",
       "1397                   1015                      820.265584   \n",
       "1398                    266                     8312.739233   \n",
       "1399                   2860                     4089.976852   \n",
       "1400                     39                     1130.000000   \n",
       "1401                   2418                      815.200000   \n",
       "1402                    697                      214.951580   \n",
       "1403                   3411                     3209.578938   \n",
       "1404                     37                      728.881100   \n",
       "1405                    449                      590.000000   \n",
       "1406                    685                      776.818200   \n",
       "1407                   1073                     3179.314634   \n",
       "1408                     14                       31.448571   \n",
       "\n",
       "      average_medicare_payment_amt_sum  line_srvc_cnt_mean  \\\n",
       "0                           341.642700          658.750000   \n",
       "1                           272.936370          679.500000   \n",
       "2                           148.490504          485.833333   \n",
       "3                           207.500389          449.500000   \n",
       "4                             6.813600           25.000000   \n",
       "5                           167.058038         3972.450000   \n",
       "6                           186.636139        15806.000000   \n",
       "7                           133.751316          116.500000   \n",
       "8                           254.949211          197.000000   \n",
       "9                           488.568043          106.250000   \n",
       "10                          463.270718          120.000000   \n",
       "11                          637.988100           95.333333   \n",
       "12                          843.125508          124.545455   \n",
       "13                          805.625168          281.384615   \n",
       "14                           77.959419           52.750000   \n",
       "15                          357.368673           50.800000   \n",
       "16                          171.365226           71.000000   \n",
       "17                           83.770909           11.000000   \n",
       "18                          617.517568          690.500000   \n",
       "19                          649.982967          659.200000   \n",
       "20                          604.485438          587.181818   \n",
       "21                           30.434866          114.500000   \n",
       "22                          715.309470          739.769231   \n",
       "23                          391.432713           40.750000   \n",
       "24                          526.338552           29.500000   \n",
       "25                          481.644864           19.833333   \n",
       "26                          236.171232           81.000000   \n",
       "27                         2845.409753          100.516129   \n",
       "28                         4003.926729           97.061224   \n",
       "29                         1869.489009          245.653846   \n",
       "...                                ...                 ...   \n",
       "1379                         76.774630          257.000000   \n",
       "1380                        151.761809          231.000000   \n",
       "1381                        190.382006           45.400000   \n",
       "1382                        502.247462           32.625000   \n",
       "1383                        838.346336          153.647059   \n",
       "1384                       2238.663293          180.958333   \n",
       "1385                         52.534375           16.000000   \n",
       "1386                         38.041869           63.500000   \n",
       "1387                         71.209030           42.000000   \n",
       "1388                        509.973095           16.333333   \n",
       "1389                       1267.429311           26.714286   \n",
       "1390                        289.010000           25.666667   \n",
       "1391                       1946.581000          162.052600   \n",
       "1392                        220.426275           16.000000   \n",
       "1393                        952.389467          268.666667   \n",
       "1394                       1474.437991          144.545455   \n",
       "1395                       1026.062970          323.454545   \n",
       "1396                         60.606460           48.000000   \n",
       "1397                        308.049311          203.000000   \n",
       "1398                        930.952503           33.250000   \n",
       "1399                       2222.257425           84.117647   \n",
       "1400                        177.127250           19.500000   \n",
       "1401                        284.390144          203.916667   \n",
       "1402                         89.375989          139.400000   \n",
       "1403                       1505.416058          200.647059   \n",
       "1404                        116.263500           18.500000   \n",
       "1405                         99.678414           89.800000   \n",
       "1406                        398.513200           97.857140   \n",
       "1407                        681.904266          268.250000   \n",
       "1408                         21.770714           14.000000   \n",
       "\n",
       "      bene_unique_cnt_mean    ...      Slide Preparation Facility  \\\n",
       "0                53.500000    ...                               0   \n",
       "1                53.750000    ...                               0   \n",
       "2                44.166667    ...                               0   \n",
       "3                23.500000    ...                               0   \n",
       "4                13.000000    ...                               0   \n",
       "5                65.500000    ...                               0   \n",
       "6                48.000000    ...                               0   \n",
       "7                47.250000    ...                               0   \n",
       "8               194.000000    ...                               0   \n",
       "9               104.750000    ...                               0   \n",
       "10              118.750000    ...                               0   \n",
       "11               93.000000    ...                               0   \n",
       "12              105.909091    ...                               0   \n",
       "13              189.230769    ...                               0   \n",
       "14               52.250000    ...                               0   \n",
       "15               20.400000    ...                               0   \n",
       "16               22.000000    ...                               0   \n",
       "17               11.000000    ...                               0   \n",
       "18              200.800000    ...                               0   \n",
       "19              172.500000    ...                               0   \n",
       "20              160.636364    ...                               0   \n",
       "21              114.500000    ...                               0   \n",
       "22              218.230769    ...                               0   \n",
       "23               40.250000    ...                               0   \n",
       "24               28.500000    ...                               0   \n",
       "25               19.166667    ...                               0   \n",
       "26               67.500000    ...                               0   \n",
       "27               61.096774    ...                               0   \n",
       "28               59.387755    ...                               0   \n",
       "29               74.576923    ...                               0   \n",
       "...                    ...    ...                             ...   \n",
       "1379             28.000000    ...                               0   \n",
       "1380             43.000000    ...                               0   \n",
       "1381             31.400000    ...                               0   \n",
       "1382             26.625000    ...                               0   \n",
       "1383             88.882353    ...                               0   \n",
       "1384            107.166667    ...                               0   \n",
       "1385             15.000000    ...                               0   \n",
       "1386             63.500000    ...                               0   \n",
       "1387             30.000000    ...                               0   \n",
       "1388             16.333333    ...                               0   \n",
       "1389             24.428571    ...                               0   \n",
       "1390             23.666667    ...                               0   \n",
       "1391             57.789470    ...                               0   \n",
       "1392             16.000000    ...                               0   \n",
       "1393             66.944444    ...                               0   \n",
       "1394             90.909091    ...                               0   \n",
       "1395             67.000000    ...                               0   \n",
       "1396             38.000000    ...                               0   \n",
       "1397            141.000000    ...                               0   \n",
       "1398             29.000000    ...                               0   \n",
       "1399             35.323529    ...                               0   \n",
       "1400             19.000000    ...                               0   \n",
       "1401             90.916667    ...                               0   \n",
       "1402            129.800000    ...                               0   \n",
       "1403             99.882353    ...                               0   \n",
       "1404             13.000000    ...                               0   \n",
       "1405             87.600000    ...                               0   \n",
       "1406             47.142860    ...                               0   \n",
       "1407            165.000000    ...                               0   \n",
       "1408             14.000000    ...                               0   \n",
       "\n",
       "      Speech Language Pathologist  Sports Medicine  Surgical Oncology  \\\n",
       "0                               0                0                  0   \n",
       "1                               0                0                  0   \n",
       "2                               0                0                  0   \n",
       "3                               0                0                  0   \n",
       "4                               0                0                  0   \n",
       "5                               0                0                  0   \n",
       "6                               0                0                  0   \n",
       "7                               0                0                  0   \n",
       "8                               0                0                  0   \n",
       "9                               0                0                  0   \n",
       "10                              0                0                  0   \n",
       "11                              0                0                  0   \n",
       "12                              0                0                  0   \n",
       "13                              0                0                  0   \n",
       "14                              0                0                  0   \n",
       "15                              0                0                  0   \n",
       "16                              0                0                  0   \n",
       "17                              0                0                  0   \n",
       "18                              0                0                  0   \n",
       "19                              0                0                  0   \n",
       "20                              0                0                  0   \n",
       "21                              0                0                  0   \n",
       "22                              0                0                  0   \n",
       "23                              0                0                  0   \n",
       "24                              0                0                  0   \n",
       "25                              0                0                  0   \n",
       "26                              0                0                  0   \n",
       "27                              0                0                  0   \n",
       "28                              0                0                  0   \n",
       "29                              0                0                  0   \n",
       "...                           ...              ...                ...   \n",
       "1379                            0                0                  0   \n",
       "1380                            0                0                  0   \n",
       "1381                            0                0                  0   \n",
       "1382                            0                0                  0   \n",
       "1383                            0                0                  0   \n",
       "1384                            0                0                  0   \n",
       "1385                            0                0                  0   \n",
       "1386                            0                0                  0   \n",
       "1387                            0                0                  0   \n",
       "1388                            0                0                  0   \n",
       "1389                            0                0                  0   \n",
       "1390                            0                0                  0   \n",
       "1391                            0                0                  0   \n",
       "1392                            0                0                  0   \n",
       "1393                            0                0                  0   \n",
       "1394                            0                0                  0   \n",
       "1395                            0                0                  0   \n",
       "1396                            0                0                  0   \n",
       "1397                            0                0                  0   \n",
       "1398                            0                0                  0   \n",
       "1399                            0                0                  0   \n",
       "1400                            0                0                  0   \n",
       "1401                            0                0                  0   \n",
       "1402                            0                0                  0   \n",
       "1403                            0                0                  0   \n",
       "1404                            0                0                  0   \n",
       "1405                            0                0                  0   \n",
       "1406                            0                0                  0   \n",
       "1407                            0                0                  0   \n",
       "1408                            0                0                  0   \n",
       "\n",
       "      Thoracic Surgery  Unknown Physician Specialty Code  \\\n",
       "0                    0                                 0   \n",
       "1                    0                                 0   \n",
       "2                    0                                 0   \n",
       "3                    0                                 0   \n",
       "4                    0                                 0   \n",
       "5                    0                                 0   \n",
       "6                    0                                 0   \n",
       "7                    0                                 0   \n",
       "8                    0                                 0   \n",
       "9                    0                                 0   \n",
       "10                   0                                 0   \n",
       "11                   0                                 0   \n",
       "12                   0                                 0   \n",
       "13                   0                                 0   \n",
       "14                   0                                 0   \n",
       "15                   0                                 0   \n",
       "16                   0                                 0   \n",
       "17                   0                                 0   \n",
       "18                   0                                 0   \n",
       "19                   0                                 0   \n",
       "20                   0                                 0   \n",
       "21                   0                                 0   \n",
       "22                   0                                 0   \n",
       "23                   0                                 0   \n",
       "24                   0                                 0   \n",
       "25                   0                                 0   \n",
       "26                   0                                 0   \n",
       "27                   0                                 0   \n",
       "28                   0                                 0   \n",
       "29                   0                                 0   \n",
       "...                ...                               ...   \n",
       "1379                 0                                 0   \n",
       "1380                 0                                 0   \n",
       "1381                 0                                 0   \n",
       "1382                 0                                 0   \n",
       "1383                 0                                 0   \n",
       "1384                 0                                 0   \n",
       "1385                 0                                 0   \n",
       "1386                 0                                 0   \n",
       "1387                 0                                 0   \n",
       "1388                 0                                 0   \n",
       "1389                 0                                 0   \n",
       "1390                 0                                 0   \n",
       "1391                 0                                 0   \n",
       "1392                 0                                 0   \n",
       "1393                 0                                 0   \n",
       "1394                 0                                 0   \n",
       "1395                 0                                 0   \n",
       "1396                 0                                 0   \n",
       "1397                 0                                 0   \n",
       "1398                 0                                 0   \n",
       "1399                 0                                 0   \n",
       "1400                 0                                 0   \n",
       "1401                 0                                 0   \n",
       "1402                 0                                 0   \n",
       "1403                 0                                 0   \n",
       "1404                 0                                 0   \n",
       "1405                 0                                 0   \n",
       "1406                 0                                 0   \n",
       "1407                 0                                 0   \n",
       "1408                 0                                 0   \n",
       "\n",
       "      Unknown Supplier/Provider  Urology  Vascular Surgery  exclusion  \n",
       "0                             0        0                 0          1  \n",
       "1                             0        0                 0          1  \n",
       "2                             0        0                 0          1  \n",
       "3                             0        0                 0          1  \n",
       "4                             0        0                 0          1  \n",
       "5                             0        0                 0          1  \n",
       "6                             0        0                 0          1  \n",
       "7                             0        0                 0          1  \n",
       "8                             0        0                 0          1  \n",
       "9                             0        0                 0          1  \n",
       "10                            0        0                 0          1  \n",
       "11                            0        0                 0          1  \n",
       "12                            0        0                 0          1  \n",
       "13                            0        0                 0          1  \n",
       "14                            0        0                 0          1  \n",
       "15                            0        0                 0          1  \n",
       "16                            0        0                 0          1  \n",
       "17                            0        0                 0          1  \n",
       "18                            0        0                 0          1  \n",
       "19                            0        0                 0          1  \n",
       "20                            0        0                 0          1  \n",
       "21                            0        0                 0          1  \n",
       "22                            0        0                 0          1  \n",
       "23                            0        0                 0          1  \n",
       "24                            0        0                 0          1  \n",
       "25                            0        0                 0          1  \n",
       "26                            0        0                 0          1  \n",
       "27                            0        0                 0          1  \n",
       "28                            0        0                 0          1  \n",
       "29                            0        0                 0          1  \n",
       "...                         ...      ...               ...        ...  \n",
       "1379                          0        0                 0          0  \n",
       "1380                          0        0                 0          0  \n",
       "1381                          0        0                 0          0  \n",
       "1382                          0        0                 0          0  \n",
       "1383                          0        0                 0          0  \n",
       "1384                          0        0                 0          0  \n",
       "1385                          0        0                 0          0  \n",
       "1386                          0        0                 0          0  \n",
       "1387                          0        0                 0          0  \n",
       "1388                          0        0                 0          0  \n",
       "1389                          0        0                 0          0  \n",
       "1390                          0        0                 0          0  \n",
       "1391                          0        0                 0          0  \n",
       "1392                          0        0                 0          0  \n",
       "1393                          0        0                 0          0  \n",
       "1394                          0        0                 0          0  \n",
       "1395                          0        0                 0          0  \n",
       "1396                          0        0                 0          0  \n",
       "1397                          0        0                 0          0  \n",
       "1398                          0        0                 0          0  \n",
       "1399                          0        0                 0          0  \n",
       "1400                          0        0                 0          0  \n",
       "1401                          0        0                 0          0  \n",
       "1402                          0        0                 0          0  \n",
       "1403                          0        0                 0          0  \n",
       "1404                          0        0                 0          0  \n",
       "1405                          0        0                 0          0  \n",
       "1406                          0        0                 0          0  \n",
       "1407                          0        0                 0          0  \n",
       "1408                          0        0                 0          0  \n",
       "\n",
       "[2818 rows x 127 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = neg.as_matrix()[:,1:126]\n",
    "X_test = test.as_matrix()[:,1:126]\n",
    "y_test = test.as_matrix()[:,126]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.concatenate((X_train,X_test)))\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler.fit(np.concatenate((X_train,X_test)))\n",
    "\n",
    "#X_train = scaler.transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.68675191, -0.68990612, -0.68734545, ..., -0.68703697,\n",
       "       -0.68930271, -0.6892006 ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5636, 125) (2818, 125) (2818,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,\n",
    "X_test.shape,\n",
    "y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_step = 1\n",
    "val_data = (X_test, y_test, val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def build_generator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(125, activation='tanh'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "    \n",
    "def build_discriminator():\n",
    "\n",
    "    img_shape = (125,)\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(512,input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (None, 512)               64512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 196,097\n",
      "Trainable params: 196,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_113 (Dense)            (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 125)               128125    \n",
      "=================================================================\n",
      "Total params: 818,045\n",
      "Trainable params: 814,461\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_gan=gan1.GAN(discriminator=build_discriminator(),generator=build_generator())\n",
    "my_gan.build_networks(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input31124\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n",
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input30539\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n",
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input31607\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n",
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input4\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 2.862404, acc.: 46.88%, validation acc: 67.15%] [G loss: 1.481124]\n",
      "1 [D loss: 2.505249, acc.: 46.88%, validation acc: 66.47%] [G loss: 1.489796]\n",
      "2 [D loss: 1.919976, acc.: 37.50%, validation acc: 66.08%] [G loss: 1.179970]\n",
      "3 [D loss: 1.552039, acc.: 37.50%, validation acc: 65.71%] [G loss: 1.088899]\n",
      "4 [D loss: 1.637873, acc.: 31.25%, validation acc: 65.04%] [G loss: 1.121502]\n",
      "5 [D loss: 1.381140, acc.: 40.62%, validation acc: 64.34%] [G loss: 0.939672]\n",
      "6 [D loss: 1.425000, acc.: 31.25%, validation acc: 63.57%] [G loss: 0.807472]\n",
      "7 [D loss: 1.042435, acc.: 28.12%, validation acc: 62.78%] [G loss: 0.848890]\n",
      "8 [D loss: 0.971850, acc.: 31.25%, validation acc: 62.41%] [G loss: 0.808870]\n",
      "9 [D loss: 1.044103, acc.: 25.00%, validation acc: 61.87%] [G loss: 0.755504]\n",
      "10 [D loss: 0.865735, acc.: 25.00%, validation acc: 61.38%] [G loss: 0.754890]\n",
      "11 [D loss: 0.773365, acc.: 43.75%, validation acc: 61.08%] [G loss: 0.838099]\n",
      "12 [D loss: 0.813165, acc.: 34.38%, validation acc: 60.42%] [G loss: 0.785460]\n",
      "13 [D loss: 0.662832, acc.: 53.12%, validation acc: 60.02%] [G loss: 0.848392]\n",
      "14 [D loss: 0.699166, acc.: 50.00%, validation acc: 59.51%] [G loss: 0.894104]\n",
      "15 [D loss: 0.630195, acc.: 68.75%, validation acc: 58.91%] [G loss: 0.898124]\n",
      "16 [D loss: 1.048272, acc.: 75.00%, validation acc: 58.56%] [G loss: 0.971663]\n",
      "17 [D loss: 0.575508, acc.: 71.88%, validation acc: 58.20%] [G loss: 0.958112]\n",
      "18 [D loss: 0.560390, acc.: 84.38%, validation acc: 57.83%] [G loss: 1.000342]\n",
      "19 [D loss: 0.564438, acc.: 71.88%, validation acc: 56.90%] [G loss: 1.026039]\n",
      "20 [D loss: 0.632120, acc.: 62.50%, validation acc: 55.95%] [G loss: 1.086242]\n",
      "21 [D loss: 0.546381, acc.: 81.25%, validation acc: 55.21%] [G loss: 1.079109]\n",
      "22 [D loss: 0.527286, acc.: 87.50%, validation acc: 54.99%] [G loss: 1.090708]\n",
      "23 [D loss: 0.486276, acc.: 87.50%, validation acc: 54.98%] [G loss: 1.053814]\n",
      "24 [D loss: 0.472147, acc.: 90.62%, validation acc: 54.55%] [G loss: 1.097005]\n",
      "25 [D loss: 0.505162, acc.: 87.50%, validation acc: 53.96%] [G loss: 1.038311]\n",
      "26 [D loss: 0.583306, acc.: 78.12%, validation acc: 53.49%] [G loss: 1.040479]\n",
      "27 [D loss: 0.508066, acc.: 87.50%, validation acc: 53.01%] [G loss: 0.966343]\n",
      "28 [D loss: 0.436174, acc.: 87.50%, validation acc: 52.88%] [G loss: 1.013204]\n",
      "29 [D loss: 0.455487, acc.: 87.50%, validation acc: 52.62%] [G loss: 1.049572]\n",
      "30 [D loss: 0.512217, acc.: 84.38%, validation acc: 52.12%] [G loss: 1.096305]\n",
      "31 [D loss: 0.488192, acc.: 87.50%, validation acc: 51.20%] [G loss: 1.063701]\n",
      "32 [D loss: 0.441516, acc.: 87.50%, validation acc: 50.98%] [G loss: 1.096854]\n",
      "33 [D loss: 0.412306, acc.: 90.62%, validation acc: 50.76%] [G loss: 1.162564]\n",
      "34 [D loss: 0.370721, acc.: 96.88%, validation acc: 50.49%] [G loss: 1.185682]\n",
      "35 [D loss: 0.447152, acc.: 90.62%, validation acc: 49.77%] [G loss: 1.211707]\n",
      "36 [D loss: 0.426197, acc.: 90.62%, validation acc: 49.52%] [G loss: 1.163927]\n",
      "37 [D loss: 0.353050, acc.: 96.88%, validation acc: 49.43%] [G loss: 1.259300]\n",
      "38 [D loss: 0.403699, acc.: 96.88%, validation acc: 48.95%] [G loss: 1.139718]\n",
      "39 [D loss: 0.449685, acc.: 90.62%, validation acc: 48.12%] [G loss: 1.144361]\n",
      "40 [D loss: 0.418074, acc.: 87.50%, validation acc: 47.63%] [G loss: 1.099603]\n",
      "41 [D loss: 0.381635, acc.: 96.88%, validation acc: 47.65%] [G loss: 1.124122]\n",
      "42 [D loss: 0.331628, acc.: 96.88%, validation acc: 47.83%] [G loss: 1.165898]\n",
      "43 [D loss: 0.414784, acc.: 84.38%, validation acc: 47.89%] [G loss: 1.234962]\n",
      "44 [D loss: 0.341063, acc.: 93.75%, validation acc: 47.66%] [G loss: 1.275252]\n",
      "45 [D loss: 0.316810, acc.: 93.75%, validation acc: 47.67%] [G loss: 1.122889]\n",
      "46 [D loss: 0.356667, acc.: 90.62%, validation acc: 47.87%] [G loss: 1.262129]\n",
      "47 [D loss: 0.354793, acc.: 93.75%, validation acc: 48.26%] [G loss: 1.369249]\n",
      "48 [D loss: 0.321071, acc.: 100.00%, validation acc: 48.70%] [G loss: 1.393595]\n",
      "49 [D loss: 0.327884, acc.: 96.88%, validation acc: 49.04%] [G loss: 1.414379]\n",
      "50 [D loss: 0.372255, acc.: 93.75%, validation acc: 48.75%] [G loss: 1.377084]\n",
      "51 [D loss: 0.344922, acc.: 96.88%, validation acc: 47.95%] [G loss: 1.378709]\n",
      "52 [D loss: 0.308306, acc.: 96.88%, validation acc: 47.95%] [G loss: 1.296159]\n",
      "53 [D loss: 0.334158, acc.: 96.88%, validation acc: 48.19%] [G loss: 1.410427]\n",
      "54 [D loss: 0.299920, acc.: 100.00%, validation acc: 48.26%] [G loss: 1.437456]\n",
      "55 [D loss: 0.306126, acc.: 96.88%, validation acc: 48.58%] [G loss: 1.414889]\n",
      "56 [D loss: 0.397169, acc.: 87.50%, validation acc: 48.51%] [G loss: 1.383712]\n",
      "57 [D loss: 0.355908, acc.: 90.62%, validation acc: 48.22%] [G loss: 1.357967]\n",
      "58 [D loss: 0.247671, acc.: 100.00%, validation acc: 48.24%] [G loss: 1.539529]\n",
      "59 [D loss: 0.313419, acc.: 100.00%, validation acc: 48.40%] [G loss: 1.525528]\n",
      "60 [D loss: 0.393557, acc.: 90.62%, validation acc: 48.08%] [G loss: 1.533364]\n",
      "61 [D loss: 0.275288, acc.: 100.00%, validation acc: 48.05%] [G loss: 1.460279]\n",
      "62 [D loss: 0.540917, acc.: 81.25%, validation acc: 47.61%] [G loss: 1.395227]\n",
      "63 [D loss: 0.641955, acc.: 78.12%, validation acc: 46.82%] [G loss: 1.207710]\n",
      "64 [D loss: 0.423254, acc.: 93.75%, validation acc: 46.55%] [G loss: 1.238265]\n",
      "65 [D loss: 0.426456, acc.: 87.50%, validation acc: 46.77%] [G loss: 1.199938]\n",
      "66 [D loss: 0.356619, acc.: 90.62%, validation acc: 47.17%] [G loss: 1.332753]\n",
      "67 [D loss: 0.345575, acc.: 93.75%, validation acc: 47.14%] [G loss: 1.159980]\n",
      "68 [D loss: 0.274307, acc.: 96.88%, validation acc: 47.16%] [G loss: 1.286210]\n",
      "69 [D loss: 0.315018, acc.: 90.62%, validation acc: 47.19%] [G loss: 1.237449]\n",
      "70 [D loss: 0.375293, acc.: 87.50%, validation acc: 47.04%] [G loss: 1.421203]\n",
      "71 [D loss: 0.255385, acc.: 93.75%, validation acc: 46.97%] [G loss: 1.386115]\n",
      "72 [D loss: 0.417045, acc.: 81.25%, validation acc: 46.84%] [G loss: 1.217302]\n",
      "73 [D loss: 0.324526, acc.: 93.75%, validation acc: 47.04%] [G loss: 1.299894]\n",
      "74 [D loss: 0.282022, acc.: 93.75%, validation acc: 47.20%] [G loss: 1.518297]\n",
      "75 [D loss: 0.331026, acc.: 90.62%, validation acc: 47.19%] [G loss: 1.509648]\n",
      "76 [D loss: 0.298370, acc.: 96.88%, validation acc: 47.14%] [G loss: 1.465238]\n",
      "77 [D loss: 0.448617, acc.: 93.75%, validation acc: 47.04%] [G loss: 1.473805]\n",
      "78 [D loss: 0.525116, acc.: 84.38%, validation acc: 46.92%] [G loss: 1.217052]\n",
      "79 [D loss: 0.268501, acc.: 100.00%, validation acc: 46.94%] [G loss: 1.531255]\n",
      "80 [D loss: 0.262751, acc.: 100.00%, validation acc: 47.04%] [G loss: 1.427178]\n",
      "81 [D loss: 0.334612, acc.: 87.50%, validation acc: 46.94%] [G loss: 1.435496]\n",
      "82 [D loss: 0.346579, acc.: 90.62%, validation acc: 46.89%] [G loss: 1.429144]\n",
      "83 [D loss: 0.365898, acc.: 93.75%, validation acc: 46.72%] [G loss: 1.584052]\n",
      "84 [D loss: 0.394486, acc.: 93.75%, validation acc: 46.82%] [G loss: 1.650927]\n",
      "85 [D loss: 0.367745, acc.: 96.88%, validation acc: 46.76%] [G loss: 1.492835]\n",
      "86 [D loss: 0.282321, acc.: 93.75%, validation acc: 46.66%] [G loss: 1.474719]\n",
      "87 [D loss: 0.252621, acc.: 93.75%, validation acc: 46.46%] [G loss: 1.475917]\n",
      "88 [D loss: 0.321127, acc.: 93.75%, validation acc: 46.29%] [G loss: 1.343699]\n",
      "89 [D loss: 0.350962, acc.: 87.50%, validation acc: 46.25%] [G loss: 1.392725]\n",
      "90 [D loss: 0.267817, acc.: 100.00%, validation acc: 46.47%] [G loss: 1.562876]\n",
      "91 [D loss: 0.394528, acc.: 90.62%, validation acc: 46.20%] [G loss: 1.380356]\n",
      "92 [D loss: 0.298580, acc.: 90.62%, validation acc: 46.34%] [G loss: 1.414288]\n",
      "93 [D loss: 0.372462, acc.: 78.12%, validation acc: 46.47%] [G loss: 1.377262]\n",
      "94 [D loss: 0.333656, acc.: 87.50%, validation acc: 46.78%] [G loss: 1.441900]\n",
      "95 [D loss: 0.374530, acc.: 87.50%, validation acc: 47.10%] [G loss: 1.582615]\n",
      "96 [D loss: 0.333666, acc.: 87.50%, validation acc: 47.29%] [G loss: 1.371144]\n",
      "97 [D loss: 0.289048, acc.: 96.88%, validation acc: 47.40%] [G loss: 1.415425]\n",
      "98 [D loss: 0.301832, acc.: 93.75%, validation acc: 47.40%] [G loss: 1.544255]\n",
      "99 [D loss: 0.331721, acc.: 87.50%, validation acc: 47.28%] [G loss: 1.632587]\n",
      "100 [D loss: 0.748188, acc.: 81.25%, validation acc: 47.11%] [G loss: 1.573364]\n",
      "101 [D loss: 0.261463, acc.: 93.75%, validation acc: 47.28%] [G loss: 1.610428]\n",
      "102 [D loss: 0.298022, acc.: 93.75%, validation acc: 47.44%] [G loss: 1.497364]\n",
      "103 [D loss: 0.342816, acc.: 93.75%, validation acc: 47.99%] [G loss: 1.496569]\n",
      "104 [D loss: 0.298792, acc.: 93.75%, validation acc: 48.23%] [G loss: 1.567046]\n",
      "105 [D loss: 0.412106, acc.: 87.50%, validation acc: 48.12%] [G loss: 1.494483]\n",
      "106 [D loss: 0.610751, acc.: 75.00%, validation acc: 47.89%] [G loss: 1.385246]\n",
      "107 [D loss: 0.295327, acc.: 90.62%, validation acc: 47.94%] [G loss: 1.477837]\n",
      "108 [D loss: 0.232083, acc.: 100.00%, validation acc: 48.07%] [G loss: 1.581330]\n",
      "109 [D loss: 0.475800, acc.: 90.62%, validation acc: 47.87%] [G loss: 1.347256]\n",
      "110 [D loss: 0.321698, acc.: 90.62%, validation acc: 47.81%] [G loss: 1.464680]\n",
      "111 [D loss: 0.918438, acc.: 81.25%, validation acc: 47.96%] [G loss: 1.482832]\n",
      "112 [D loss: 0.351612, acc.: 90.62%, validation acc: 47.76%] [G loss: 1.405444]\n",
      "113 [D loss: 0.261968, acc.: 96.88%, validation acc: 47.64%] [G loss: 1.545686]\n",
      "114 [D loss: 0.911057, acc.: 87.50%, validation acc: 47.24%] [G loss: 1.381908]\n",
      "115 [D loss: 0.304900, acc.: 90.62%, validation acc: 47.43%] [G loss: 1.339744]\n",
      "116 [D loss: 0.406205, acc.: 87.50%, validation acc: 47.35%] [G loss: 1.368061]\n",
      "117 [D loss: 0.235873, acc.: 96.88%, validation acc: 47.40%] [G loss: 1.366643]\n",
      "118 [D loss: 0.434740, acc.: 84.38%, validation acc: 47.35%] [G loss: 1.321782]\n",
      "119 [D loss: 0.312962, acc.: 84.38%, validation acc: 47.51%] [G loss: 1.468516]\n",
      "120 [D loss: 0.364945, acc.: 90.62%, validation acc: 47.44%] [G loss: 1.522315]\n",
      "121 [D loss: 0.280531, acc.: 93.75%, validation acc: 47.37%] [G loss: 1.435237]\n",
      "122 [D loss: 0.530920, acc.: 93.75%, validation acc: 47.34%] [G loss: 1.353759]\n",
      "123 [D loss: 0.340108, acc.: 90.62%, validation acc: 47.69%] [G loss: 1.454572]\n",
      "124 [D loss: 0.416567, acc.: 87.50%, validation acc: 47.59%] [G loss: 1.322331]\n",
      "125 [D loss: 0.332502, acc.: 87.50%, validation acc: 47.54%] [G loss: 1.367236]\n",
      "126 [D loss: 0.506459, acc.: 81.25%, validation acc: 47.44%] [G loss: 1.409638]\n",
      "127 [D loss: 0.262081, acc.: 96.88%, validation acc: 47.59%] [G loss: 1.554179]\n",
      "128 [D loss: 0.301197, acc.: 96.88%, validation acc: 47.61%] [G loss: 1.428103]\n",
      "129 [D loss: 0.370169, acc.: 84.38%, validation acc: 47.66%] [G loss: 1.484728]\n",
      "130 [D loss: 0.284475, acc.: 90.62%, validation acc: 47.72%] [G loss: 1.448712]\n",
      "131 [D loss: 0.364083, acc.: 90.62%, validation acc: 48.15%] [G loss: 1.570447]\n",
      "132 [D loss: 0.404570, acc.: 81.25%, validation acc: 48.73%] [G loss: 1.553467]\n",
      "133 [D loss: 0.327817, acc.: 93.75%, validation acc: 49.22%] [G loss: 1.608091]\n",
      "134 [D loss: 0.421091, acc.: 78.12%, validation acc: 49.30%] [G loss: 1.755296]\n",
      "135 [D loss: 0.275082, acc.: 93.75%, validation acc: 49.26%] [G loss: 1.537550]\n",
      "136 [D loss: 0.346061, acc.: 90.62%, validation acc: 49.11%] [G loss: 1.658290]\n",
      "137 [D loss: 0.434068, acc.: 87.50%, validation acc: 49.41%] [G loss: 1.570340]\n",
      "138 [D loss: 0.288702, acc.: 96.88%, validation acc: 49.70%] [G loss: 1.707167]\n",
      "139 [D loss: 0.365012, acc.: 90.62%, validation acc: 49.72%] [G loss: 1.674815]\n",
      "140 [D loss: 0.260315, acc.: 96.88%, validation acc: 50.01%] [G loss: 1.635643]\n",
      "141 [D loss: 0.326619, acc.: 96.88%, validation acc: 50.06%] [G loss: 1.567339]\n",
      "142 [D loss: 0.425216, acc.: 78.12%, validation acc: 49.99%] [G loss: 1.498473]\n",
      "143 [D loss: 0.328512, acc.: 84.38%, validation acc: 50.44%] [G loss: 1.660737]\n",
      "144 [D loss: 0.242623, acc.: 96.88%, validation acc: 50.75%] [G loss: 1.783603]\n",
      "145 [D loss: 0.420931, acc.: 87.50%, validation acc: 50.87%] [G loss: 1.804615]\n",
      "146 [D loss: 0.345732, acc.: 96.88%, validation acc: 50.79%] [G loss: 1.620625]\n",
      "147 [D loss: 0.437446, acc.: 84.38%, validation acc: 50.63%] [G loss: 1.459633]\n",
      "148 [D loss: 0.323519, acc.: 93.75%, validation acc: 50.72%] [G loss: 1.443171]\n",
      "149 [D loss: 0.437407, acc.: 75.00%, validation acc: 51.11%] [G loss: 1.519460]\n",
      "150 [D loss: 0.375144, acc.: 84.38%, validation acc: 51.23%] [G loss: 1.498537]\n",
      "151 [D loss: 0.352643, acc.: 81.25%, validation acc: 51.42%] [G loss: 1.725061]\n",
      "152 [D loss: 0.408536, acc.: 81.25%, validation acc: 51.82%] [G loss: 1.653815]\n",
      "153 [D loss: 0.347861, acc.: 84.38%, validation acc: 52.19%] [G loss: 1.462531]\n",
      "154 [D loss: 0.409256, acc.: 87.50%, validation acc: 52.10%] [G loss: 1.383987]\n",
      "155 [D loss: 0.325549, acc.: 93.75%, validation acc: 52.20%] [G loss: 1.729411]\n",
      "156 [D loss: 0.385414, acc.: 81.25%, validation acc: 52.08%] [G loss: 1.672628]\n",
      "157 [D loss: 0.306995, acc.: 93.75%, validation acc: 52.03%] [G loss: 1.517732]\n",
      "158 [D loss: 0.330720, acc.: 90.62%, validation acc: 52.10%] [G loss: 1.817555]\n",
      "159 [D loss: 0.528138, acc.: 84.38%, validation acc: 52.19%] [G loss: 1.403427]\n",
      "160 [D loss: 0.375365, acc.: 90.62%, validation acc: 52.21%] [G loss: 1.551549]\n",
      "161 [D loss: 0.657292, acc.: 75.00%, validation acc: 52.18%] [G loss: 1.399028]\n",
      "162 [D loss: 0.442071, acc.: 78.12%, validation acc: 52.41%] [G loss: 1.353757]\n",
      "163 [D loss: 0.414329, acc.: 81.25%, validation acc: 52.83%] [G loss: 1.427412]\n",
      "164 [D loss: 0.408970, acc.: 84.38%, validation acc: 52.95%] [G loss: 1.479663]\n",
      "165 [D loss: 0.316134, acc.: 93.75%, validation acc: 53.16%] [G loss: 1.463307]\n",
      "166 [D loss: 0.265103, acc.: 90.62%, validation acc: 52.95%] [G loss: 1.465335]\n",
      "167 [D loss: 0.353480, acc.: 81.25%, validation acc: 53.00%] [G loss: 1.400134]\n",
      "168 [D loss: 0.409600, acc.: 87.50%, validation acc: 53.10%] [G loss: 1.437788]\n",
      "169 [D loss: 0.409214, acc.: 81.25%, validation acc: 53.16%] [G loss: 1.360933]\n",
      "170 [D loss: 0.815509, acc.: 84.38%, validation acc: 53.71%] [G loss: 1.454144]\n",
      "171 [D loss: 0.368128, acc.: 84.38%, validation acc: 54.20%] [G loss: 1.540637]\n",
      "172 [D loss: 0.319356, acc.: 93.75%, validation acc: 54.05%] [G loss: 1.650303]\n",
      "173 [D loss: 0.318338, acc.: 90.62%, validation acc: 53.77%] [G loss: 1.437357]\n",
      "174 [D loss: 0.388932, acc.: 87.50%, validation acc: 53.67%] [G loss: 1.304771]\n",
      "175 [D loss: 0.254132, acc.: 96.88%, validation acc: 53.58%] [G loss: 1.356815]\n",
      "176 [D loss: 0.466623, acc.: 87.50%, validation acc: 53.27%] [G loss: 1.427928]\n",
      "177 [D loss: 0.390540, acc.: 84.38%, validation acc: 53.16%] [G loss: 1.258880]\n",
      "178 [D loss: 0.371473, acc.: 90.62%, validation acc: 53.65%] [G loss: 1.464363]\n",
      "179 [D loss: 0.350303, acc.: 84.38%, validation acc: 53.93%] [G loss: 1.559679]\n",
      "180 [D loss: 0.283897, acc.: 96.88%, validation acc: 54.16%] [G loss: 1.489517]\n",
      "181 [D loss: 0.453619, acc.: 84.38%, validation acc: 54.37%] [G loss: 1.408590]\n",
      "182 [D loss: 0.610714, acc.: 68.75%, validation acc: 54.80%] [G loss: 1.425735]\n",
      "183 [D loss: 0.456512, acc.: 87.50%, validation acc: 54.79%] [G loss: 1.498713]\n",
      "184 [D loss: 0.378003, acc.: 84.38%, validation acc: 55.14%] [G loss: 1.759889]\n",
      "185 [D loss: 0.814715, acc.: 93.75%, validation acc: 55.42%] [G loss: 1.653593]\n",
      "186 [D loss: 0.356804, acc.: 90.62%, validation acc: 55.63%] [G loss: 1.608675]\n",
      "187 [D loss: 0.373188, acc.: 87.50%, validation acc: 55.96%] [G loss: 1.378102]\n",
      "188 [D loss: 0.704790, acc.: 84.38%, validation acc: 55.99%] [G loss: 1.526855]\n",
      "189 [D loss: 0.442743, acc.: 84.38%, validation acc: 56.42%] [G loss: 1.508589]\n",
      "190 [D loss: 0.405376, acc.: 84.38%, validation acc: 56.54%] [G loss: 1.490721]\n",
      "191 [D loss: 0.344244, acc.: 90.62%, validation acc: 56.32%] [G loss: 1.592421]\n",
      "192 [D loss: 0.599393, acc.: 78.12%, validation acc: 55.55%] [G loss: 1.379418]\n",
      "193 [D loss: 0.364900, acc.: 84.38%, validation acc: 55.40%] [G loss: 1.539076]\n",
      "194 [D loss: 0.432898, acc.: 84.38%, validation acc: 55.43%] [G loss: 1.263879]\n",
      "195 [D loss: 0.297485, acc.: 93.75%, validation acc: 55.48%] [G loss: 1.330859]\n",
      "196 [D loss: 0.319721, acc.: 93.75%, validation acc: 55.80%] [G loss: 1.324879]\n",
      "197 [D loss: 0.341466, acc.: 87.50%, validation acc: 56.02%] [G loss: 1.255285]\n",
      "198 [D loss: 0.343482, acc.: 87.50%, validation acc: 56.44%] [G loss: 1.312507]\n",
      "199 [D loss: 0.290530, acc.: 93.75%, validation acc: 56.84%] [G loss: 1.544727]\n",
      "200 [D loss: 0.339462, acc.: 90.62%, validation acc: 56.71%] [G loss: 1.544981]\n",
      "201 [D loss: 0.401495, acc.: 90.62%, validation acc: 56.61%] [G loss: 1.496018]\n",
      "202 [D loss: 0.505925, acc.: 84.38%, validation acc: 56.64%] [G loss: 1.431802]\n",
      "203 [D loss: 0.555724, acc.: 87.50%, validation acc: 56.31%] [G loss: 1.430417]\n",
      "204 [D loss: 0.539033, acc.: 87.50%, validation acc: 56.11%] [G loss: 1.371286]\n",
      "205 [D loss: 0.365112, acc.: 87.50%, validation acc: 56.01%] [G loss: 1.414179]\n",
      "206 [D loss: 0.376126, acc.: 90.62%, validation acc: 55.98%] [G loss: 1.419912]\n",
      "207 [D loss: 0.324580, acc.: 90.62%, validation acc: 55.68%] [G loss: 1.469087]\n",
      "208 [D loss: 0.404234, acc.: 81.25%, validation acc: 55.84%] [G loss: 1.262476]\n",
      "209 [D loss: 0.439089, acc.: 81.25%, validation acc: 55.83%] [G loss: 1.489585]\n",
      "210 [D loss: 0.686596, acc.: 75.00%, validation acc: 55.26%] [G loss: 1.238778]\n",
      "211 [D loss: 0.413064, acc.: 87.50%, validation acc: 55.23%] [G loss: 1.475802]\n",
      "212 [D loss: 0.836920, acc.: 81.25%, validation acc: 55.39%] [G loss: 1.372267]\n",
      "213 [D loss: 0.541785, acc.: 90.62%, validation acc: 55.69%] [G loss: 1.474259]\n",
      "214 [D loss: 0.425154, acc.: 84.38%, validation acc: 55.83%] [G loss: 1.398460]\n",
      "215 [D loss: 0.323333, acc.: 87.50%, validation acc: 56.07%] [G loss: 1.480469]\n",
      "216 [D loss: 0.296416, acc.: 93.75%, validation acc: 56.24%] [G loss: 1.402755]\n",
      "217 [D loss: 0.286433, acc.: 90.62%, validation acc: 56.48%] [G loss: 1.401630]\n",
      "218 [D loss: 0.292910, acc.: 93.75%, validation acc: 56.52%] [G loss: 1.469875]\n",
      "219 [D loss: 0.401385, acc.: 84.38%, validation acc: 57.15%] [G loss: 1.454642]\n",
      "220 [D loss: 0.386747, acc.: 87.50%, validation acc: 57.52%] [G loss: 1.445523]\n",
      "221 [D loss: 0.677127, acc.: 90.62%, validation acc: 57.22%] [G loss: 1.489583]\n",
      "222 [D loss: 0.536678, acc.: 87.50%, validation acc: 57.15%] [G loss: 1.293833]\n",
      "223 [D loss: 0.552570, acc.: 78.12%, validation acc: 56.82%] [G loss: 1.266026]\n",
      "224 [D loss: 0.508532, acc.: 90.62%, validation acc: 56.74%] [G loss: 1.231656]\n",
      "225 [D loss: 0.374271, acc.: 84.38%, validation acc: 56.81%] [G loss: 1.142251]\n",
      "226 [D loss: 0.314565, acc.: 90.62%, validation acc: 56.79%] [G loss: 1.235518]\n",
      "227 [D loss: 0.400917, acc.: 87.50%, validation acc: 57.46%] [G loss: 1.243916]\n",
      "228 [D loss: 0.333842, acc.: 87.50%, validation acc: 57.87%] [G loss: 1.463477]\n",
      "229 [D loss: 0.385439, acc.: 87.50%, validation acc: 58.23%] [G loss: 1.415133]\n",
      "230 [D loss: 0.389096, acc.: 87.50%, validation acc: 58.02%] [G loss: 1.203880]\n",
      "231 [D loss: 0.318030, acc.: 93.75%, validation acc: 57.96%] [G loss: 1.309908]\n",
      "232 [D loss: 0.325099, acc.: 93.75%, validation acc: 58.20%] [G loss: 1.408180]\n",
      "233 [D loss: 0.421929, acc.: 84.38%, validation acc: 58.61%] [G loss: 1.312977]\n",
      "234 [D loss: 0.293410, acc.: 100.00%, validation acc: 58.80%] [G loss: 1.482744]\n",
      "235 [D loss: 0.386323, acc.: 87.50%, validation acc: 58.53%] [G loss: 1.430454]\n",
      "236 [D loss: 0.309044, acc.: 93.75%, validation acc: 58.32%] [G loss: 1.423779]\n",
      "237 [D loss: 0.339370, acc.: 90.62%, validation acc: 58.15%] [G loss: 1.477008]\n",
      "238 [D loss: 0.324869, acc.: 93.75%, validation acc: 58.25%] [G loss: 1.457111]\n",
      "239 [D loss: 0.414870, acc.: 87.50%, validation acc: 58.27%] [G loss: 1.395278]\n",
      "240 [D loss: 0.463697, acc.: 90.62%, validation acc: 58.26%] [G loss: 1.275006]\n",
      "241 [D loss: 0.294795, acc.: 96.88%, validation acc: 58.39%] [G loss: 1.328059]\n",
      "242 [D loss: 0.522224, acc.: 78.12%, validation acc: 58.71%] [G loss: 1.421350]\n",
      "243 [D loss: 0.357747, acc.: 84.38%, validation acc: 59.04%] [G loss: 1.237865]\n",
      "244 [D loss: 0.321380, acc.: 96.88%, validation acc: 59.24%] [G loss: 1.475716]\n",
      "245 [D loss: 0.364115, acc.: 87.50%, validation acc: 59.25%] [G loss: 1.395860]\n",
      "246 [D loss: 0.476083, acc.: 81.25%, validation acc: 59.04%] [G loss: 1.451438]\n",
      "247 [D loss: 0.836346, acc.: 65.62%, validation acc: 58.46%] [G loss: 1.153109]\n",
      "248 [D loss: 0.380977, acc.: 96.88%, validation acc: 58.33%] [G loss: 1.188144]\n",
      "249 [D loss: 0.373383, acc.: 87.50%, validation acc: 58.36%] [G loss: 1.164461]\n",
      "250 [D loss: 0.362104, acc.: 87.50%, validation acc: 58.47%] [G loss: 1.328676]\n",
      "251 [D loss: 0.511941, acc.: 78.12%, validation acc: 58.24%] [G loss: 1.322156]\n",
      "252 [D loss: 0.333043, acc.: 93.75%, validation acc: 58.34%] [G loss: 1.255004]\n",
      "253 [D loss: 0.456397, acc.: 81.25%, validation acc: 58.37%] [G loss: 1.177915]\n",
      "254 [D loss: 0.278941, acc.: 96.88%, validation acc: 58.41%] [G loss: 1.370633]\n",
      "255 [D loss: 0.324660, acc.: 93.75%, validation acc: 58.43%] [G loss: 1.172828]\n",
      "256 [D loss: 0.308635, acc.: 93.75%, validation acc: 58.39%] [G loss: 1.316829]\n",
      "257 [D loss: 0.453432, acc.: 84.38%, validation acc: 58.16%] [G loss: 1.253358]\n",
      "258 [D loss: 0.774140, acc.: 96.88%, validation acc: 58.11%] [G loss: 1.355129]\n",
      "259 [D loss: 0.332539, acc.: 96.88%, validation acc: 57.93%] [G loss: 1.335186]\n",
      "260 [D loss: 0.340598, acc.: 96.88%, validation acc: 57.91%] [G loss: 1.330195]\n",
      "261 [D loss: 0.376142, acc.: 87.50%, validation acc: 57.64%] [G loss: 1.427725]\n",
      "262 [D loss: 0.356794, acc.: 90.62%, validation acc: 57.49%] [G loss: 1.249595]\n",
      "263 [D loss: 0.504606, acc.: 81.25%, validation acc: 57.20%] [G loss: 1.336319]\n",
      "264 [D loss: 0.376727, acc.: 90.62%, validation acc: 56.93%] [G loss: 1.300327]\n",
      "265 [D loss: 0.387982, acc.: 87.50%, validation acc: 56.78%] [G loss: 1.417266]\n",
      "266 [D loss: 0.404372, acc.: 87.50%, validation acc: 56.40%] [G loss: 1.424281]\n",
      "267 [D loss: 0.286839, acc.: 93.75%, validation acc: 56.35%] [G loss: 1.406528]\n",
      "268 [D loss: 0.500668, acc.: 81.25%, validation acc: 55.43%] [G loss: 1.153575]\n",
      "269 [D loss: 0.273861, acc.: 96.88%, validation acc: 55.23%] [G loss: 1.170014]\n",
      "270 [D loss: 0.451593, acc.: 75.00%, validation acc: 54.64%] [G loss: 1.150236]\n",
      "271 [D loss: 0.391630, acc.: 90.62%, validation acc: 54.48%] [G loss: 1.139060]\n",
      "272 [D loss: 0.330871, acc.: 90.62%, validation acc: 54.52%] [G loss: 1.187432]\n",
      "273 [D loss: 0.353806, acc.: 90.62%, validation acc: 54.64%] [G loss: 1.245570]\n",
      "274 [D loss: 0.374482, acc.: 93.75%, validation acc: 54.44%] [G loss: 1.326308]\n",
      "275 [D loss: 0.637012, acc.: 84.38%, validation acc: 54.46%] [G loss: 1.265556]\n",
      "276 [D loss: 0.363992, acc.: 93.75%, validation acc: 54.39%] [G loss: 1.280131]\n",
      "277 [D loss: 0.474239, acc.: 87.50%, validation acc: 54.45%] [G loss: 1.219525]\n",
      "278 [D loss: 0.424330, acc.: 87.50%, validation acc: 54.23%] [G loss: 1.171018]\n",
      "279 [D loss: 0.687733, acc.: 78.12%, validation acc: 52.81%] [G loss: 1.142037]\n",
      "280 [D loss: 0.288592, acc.: 96.88%, validation acc: 52.32%] [G loss: 1.170305]\n",
      "281 [D loss: 0.379326, acc.: 87.50%, validation acc: 51.95%] [G loss: 1.154506]\n",
      "282 [D loss: 0.311899, acc.: 93.75%, validation acc: 51.81%] [G loss: 1.184632]\n",
      "283 [D loss: 0.421249, acc.: 84.38%, validation acc: 51.95%] [G loss: 1.351472]\n",
      "284 [D loss: 0.450861, acc.: 84.38%, validation acc: 52.00%] [G loss: 1.307863]\n",
      "285 [D loss: 0.357594, acc.: 93.75%, validation acc: 52.35%] [G loss: 1.358426]\n",
      "286 [D loss: 0.710885, acc.: 81.25%, validation acc: 52.28%] [G loss: 1.299625]\n",
      "287 [D loss: 0.358899, acc.: 93.75%, validation acc: 52.08%] [G loss: 1.293386]\n",
      "288 [D loss: 0.517404, acc.: 75.00%, validation acc: 51.73%] [G loss: 1.219548]\n",
      "289 [D loss: 0.394855, acc.: 84.38%, validation acc: 51.92%] [G loss: 1.270483]\n",
      "290 [D loss: 0.463047, acc.: 78.12%, validation acc: 52.01%] [G loss: 1.248654]\n",
      "291 [D loss: 0.520783, acc.: 87.50%, validation acc: 51.65%] [G loss: 1.222918]\n",
      "292 [D loss: 0.370534, acc.: 96.88%, validation acc: 51.31%] [G loss: 1.173826]\n",
      "293 [D loss: 0.399746, acc.: 90.62%, validation acc: 51.11%] [G loss: 1.202146]\n",
      "294 [D loss: 0.345586, acc.: 93.75%, validation acc: 51.21%] [G loss: 1.376380]\n",
      "295 [D loss: 0.387614, acc.: 90.62%, validation acc: 50.46%] [G loss: 1.226506]\n",
      "296 [D loss: 0.589330, acc.: 84.38%, validation acc: 49.70%] [G loss: 1.346917]\n",
      "297 [D loss: 0.699103, acc.: 90.62%, validation acc: 49.18%] [G loss: 1.215801]\n",
      "298 [D loss: 0.406254, acc.: 90.62%, validation acc: 49.38%] [G loss: 1.148840]\n",
      "299 [D loss: 0.447900, acc.: 81.25%, validation acc: 49.41%] [G loss: 1.186982]\n",
      "300 [D loss: 0.363370, acc.: 93.75%, validation acc: 49.16%] [G loss: 1.164559]\n",
      "301 [D loss: 0.381973, acc.: 87.50%, validation acc: 49.15%] [G loss: 1.238834]\n",
      "302 [D loss: 0.364675, acc.: 93.75%, validation acc: 49.32%] [G loss: 1.333257]\n",
      "303 [D loss: 1.061251, acc.: 90.62%, validation acc: 49.15%] [G loss: 1.278815]\n",
      "304 [D loss: 0.328078, acc.: 93.75%, validation acc: 49.04%] [G loss: 1.271503]\n",
      "305 [D loss: 0.435536, acc.: 93.75%, validation acc: 48.75%] [G loss: 1.230277]\n",
      "306 [D loss: 0.427159, acc.: 84.38%, validation acc: 48.39%] [G loss: 1.323043]\n",
      "307 [D loss: 0.421855, acc.: 81.25%, validation acc: 48.19%] [G loss: 1.255579]\n",
      "308 [D loss: 0.506201, acc.: 90.62%, validation acc: 48.40%] [G loss: 1.140889]\n",
      "309 [D loss: 0.362496, acc.: 90.62%, validation acc: 48.73%] [G loss: 1.189408]\n",
      "310 [D loss: 0.429558, acc.: 87.50%, validation acc: 48.77%] [G loss: 1.291090]\n",
      "311 [D loss: 0.326726, acc.: 96.88%, validation acc: 48.89%] [G loss: 1.286403]\n",
      "312 [D loss: 0.664950, acc.: 87.50%, validation acc: 48.64%] [G loss: 1.208589]\n",
      "313 [D loss: 1.688211, acc.: 75.00%, validation acc: 47.84%] [G loss: 1.034310]\n",
      "314 [D loss: 0.439247, acc.: 87.50%, validation acc: 47.62%] [G loss: 1.042993]\n",
      "315 [D loss: 0.376541, acc.: 90.62%, validation acc: 47.83%] [G loss: 1.147980]\n",
      "316 [D loss: 0.360803, acc.: 96.88%, validation acc: 48.20%] [G loss: 1.217421]\n",
      "317 [D loss: 0.457238, acc.: 90.62%, validation acc: 48.05%] [G loss: 1.180424]\n",
      "318 [D loss: 0.443376, acc.: 87.50%, validation acc: 48.26%] [G loss: 1.218876]\n",
      "319 [D loss: 0.371083, acc.: 93.75%, validation acc: 48.37%] [G loss: 1.265690]\n",
      "320 [D loss: 0.583835, acc.: 90.62%, validation acc: 48.27%] [G loss: 1.147375]\n",
      "321 [D loss: 0.405721, acc.: 96.88%, validation acc: 48.21%] [G loss: 1.106942]\n",
      "322 [D loss: 0.512425, acc.: 84.38%, validation acc: 48.12%] [G loss: 1.130658]\n",
      "323 [D loss: 0.422730, acc.: 93.75%, validation acc: 47.99%] [G loss: 1.169136]\n",
      "324 [D loss: 0.741510, acc.: 90.62%, validation acc: 47.73%] [G loss: 1.053823]\n",
      "325 [D loss: 0.361525, acc.: 93.75%, validation acc: 48.12%] [G loss: 1.045615]\n",
      "326 [D loss: 0.766450, acc.: 87.50%, validation acc: 48.03%] [G loss: 1.098080]\n",
      "327 [D loss: 0.417890, acc.: 84.38%, validation acc: 48.12%] [G loss: 1.083745]\n",
      "328 [D loss: 0.452142, acc.: 87.50%, validation acc: 48.13%] [G loss: 1.072561]\n",
      "329 [D loss: 0.324798, acc.: 93.75%, validation acc: 48.25%] [G loss: 1.173768]\n",
      "330 [D loss: 0.386922, acc.: 96.88%, validation acc: 48.23%] [G loss: 1.251254]\n",
      "331 [D loss: 0.440837, acc.: 90.62%, validation acc: 48.21%] [G loss: 1.144759]\n",
      "332 [D loss: 0.470832, acc.: 93.75%, validation acc: 48.34%] [G loss: 1.231183]\n",
      "333 [D loss: 0.333863, acc.: 96.88%, validation acc: 48.36%] [G loss: 1.208867]\n",
      "334 [D loss: 0.422327, acc.: 90.62%, validation acc: 48.33%] [G loss: 1.173890]\n",
      "335 [D loss: 0.385654, acc.: 96.88%, validation acc: 48.52%] [G loss: 1.294411]\n",
      "336 [D loss: 0.336147, acc.: 90.62%, validation acc: 48.74%] [G loss: 1.375518]\n",
      "337 [D loss: 0.461298, acc.: 81.25%, validation acc: 48.76%] [G loss: 1.275380]\n",
      "338 [D loss: 0.362749, acc.: 96.88%, validation acc: 49.02%] [G loss: 1.372002]\n",
      "339 [D loss: 0.484723, acc.: 87.50%, validation acc: 49.22%] [G loss: 1.295929]\n",
      "340 [D loss: 0.469139, acc.: 81.25%, validation acc: 49.24%] [G loss: 1.179423]\n",
      "341 [D loss: 0.448108, acc.: 84.38%, validation acc: 49.04%] [G loss: 1.242966]\n",
      "342 [D loss: 0.356623, acc.: 100.00%, validation acc: 48.90%] [G loss: 1.195301]\n",
      "343 [D loss: 0.339286, acc.: 100.00%, validation acc: 49.02%] [G loss: 1.285731]\n",
      "344 [D loss: 0.337666, acc.: 93.75%, validation acc: 49.16%] [G loss: 1.184678]\n",
      "345 [D loss: 0.577238, acc.: 87.50%, validation acc: 48.71%] [G loss: 1.183272]\n",
      "346 [D loss: 0.530077, acc.: 90.62%, validation acc: 48.61%] [G loss: 1.089960]\n",
      "347 [D loss: 0.357837, acc.: 93.75%, validation acc: 48.72%] [G loss: 1.159455]\n",
      "348 [D loss: 0.312241, acc.: 100.00%, validation acc: 48.88%] [G loss: 1.296881]\n",
      "349 [D loss: 0.247970, acc.: 100.00%, validation acc: 48.93%] [G loss: 1.320001]\n",
      "350 [D loss: 0.817508, acc.: 93.75%, validation acc: 48.55%] [G loss: 1.309844]\n",
      "351 [D loss: 0.375939, acc.: 100.00%, validation acc: 48.40%] [G loss: 1.346621]\n",
      "352 [D loss: 0.372212, acc.: 96.88%, validation acc: 48.56%] [G loss: 1.280263]\n",
      "353 [D loss: 0.369613, acc.: 90.62%, validation acc: 48.34%] [G loss: 1.321544]\n",
      "354 [D loss: 0.351052, acc.: 96.88%, validation acc: 48.36%] [G loss: 1.287682]\n",
      "355 [D loss: 0.444008, acc.: 90.62%, validation acc: 48.23%] [G loss: 1.305320]\n",
      "356 [D loss: 0.362378, acc.: 96.88%, validation acc: 48.45%] [G loss: 1.293298]\n",
      "357 [D loss: 0.241842, acc.: 100.00%, validation acc: 48.65%] [G loss: 1.456057]\n",
      "358 [D loss: 0.312751, acc.: 100.00%, validation acc: 48.67%] [G loss: 1.401645]\n",
      "359 [D loss: 0.544349, acc.: 87.50%, validation acc: 48.60%] [G loss: 1.336125]\n",
      "360 [D loss: 0.373636, acc.: 93.75%, validation acc: 48.46%] [G loss: 1.300019]\n",
      "361 [D loss: 0.369069, acc.: 90.62%, validation acc: 48.49%] [G loss: 1.306304]\n",
      "362 [D loss: 0.365443, acc.: 93.75%, validation acc: 48.72%] [G loss: 1.331776]\n",
      "363 [D loss: 0.331177, acc.: 93.75%, validation acc: 48.83%] [G loss: 1.366236]\n",
      "364 [D loss: 0.314317, acc.: 96.88%, validation acc: 48.76%] [G loss: 1.473956]\n",
      "365 [D loss: 0.391198, acc.: 93.75%, validation acc: 48.84%] [G loss: 1.419529]\n",
      "366 [D loss: 0.404202, acc.: 93.75%, validation acc: 48.70%] [G loss: 1.461983]\n",
      "367 [D loss: 0.474337, acc.: 90.62%, validation acc: 48.54%] [G loss: 1.281915]\n",
      "368 [D loss: 0.401310, acc.: 87.50%, validation acc: 48.11%] [G loss: 1.299476]\n",
      "369 [D loss: 0.347310, acc.: 93.75%, validation acc: 48.13%] [G loss: 1.236322]\n",
      "370 [D loss: 0.302493, acc.: 96.88%, validation acc: 48.25%] [G loss: 1.360542]\n",
      "371 [D loss: 0.339469, acc.: 96.88%, validation acc: 48.19%] [G loss: 1.371838]\n",
      "372 [D loss: 0.336464, acc.: 93.75%, validation acc: 48.20%] [G loss: 1.281848]\n",
      "373 [D loss: 0.353456, acc.: 93.75%, validation acc: 48.08%] [G loss: 1.311178]\n",
      "374 [D loss: 0.265730, acc.: 100.00%, validation acc: 48.23%] [G loss: 1.359361]\n",
      "375 [D loss: 0.430957, acc.: 93.75%, validation acc: 48.21%] [G loss: 1.375365]\n",
      "376 [D loss: 0.359627, acc.: 90.62%, validation acc: 48.32%] [G loss: 1.317420]\n",
      "377 [D loss: 0.336436, acc.: 96.88%, validation acc: 48.28%] [G loss: 1.328942]\n",
      "378 [D loss: 0.349765, acc.: 96.88%, validation acc: 48.40%] [G loss: 1.406445]\n",
      "379 [D loss: 0.373382, acc.: 96.88%, validation acc: 48.53%] [G loss: 1.415704]\n",
      "380 [D loss: 0.322828, acc.: 93.75%, validation acc: 48.74%] [G loss: 1.402189]\n",
      "381 [D loss: 0.311007, acc.: 96.88%, validation acc: 48.79%] [G loss: 1.295071]\n",
      "382 [D loss: 0.292284, acc.: 100.00%, validation acc: 49.20%] [G loss: 1.370197]\n",
      "383 [D loss: 0.356336, acc.: 90.62%, validation acc: 49.24%] [G loss: 1.390560]\n",
      "384 [D loss: 0.391258, acc.: 90.62%, validation acc: 49.05%] [G loss: 1.254322]\n",
      "385 [D loss: 0.330840, acc.: 93.75%, validation acc: 49.25%] [G loss: 1.282588]\n",
      "386 [D loss: 0.343715, acc.: 93.75%, validation acc: 49.44%] [G loss: 1.364432]\n",
      "387 [D loss: 0.369940, acc.: 90.62%, validation acc: 49.17%] [G loss: 1.314066]\n",
      "388 [D loss: 0.317448, acc.: 93.75%, validation acc: 49.10%] [G loss: 1.348405]\n",
      "389 [D loss: 0.299869, acc.: 96.88%, validation acc: 49.20%] [G loss: 1.333789]\n",
      "390 [D loss: 0.396891, acc.: 87.50%, validation acc: 48.59%] [G loss: 1.288954]\n",
      "391 [D loss: 0.341311, acc.: 96.88%, validation acc: 48.56%] [G loss: 1.390988]\n",
      "392 [D loss: 0.305158, acc.: 96.88%, validation acc: 48.67%] [G loss: 1.426261]\n",
      "393 [D loss: 0.259399, acc.: 100.00%, validation acc: 48.63%] [G loss: 1.462979]\n",
      "394 [D loss: 0.353374, acc.: 93.75%, validation acc: 48.37%] [G loss: 1.328502]\n",
      "395 [D loss: 0.294074, acc.: 96.88%, validation acc: 48.27%] [G loss: 1.299032]\n",
      "396 [D loss: 0.383303, acc.: 93.75%, validation acc: 47.89%] [G loss: 1.391538]\n",
      "397 [D loss: 0.257869, acc.: 96.88%, validation acc: 47.55%] [G loss: 1.291955]\n",
      "398 [D loss: 0.284404, acc.: 96.88%, validation acc: 47.30%] [G loss: 1.345143]\n",
      "399 [D loss: 0.291887, acc.: 100.00%, validation acc: 47.05%] [G loss: 1.468068]\n",
      "400 [D loss: 0.316030, acc.: 96.88%, validation acc: 47.09%] [G loss: 1.434236]\n",
      "401 [D loss: 0.268774, acc.: 100.00%, validation acc: 47.21%] [G loss: 1.402722]\n",
      "402 [D loss: 0.527640, acc.: 78.12%, validation acc: 47.03%] [G loss: 1.356085]\n",
      "403 [D loss: 0.338702, acc.: 100.00%, validation acc: 47.16%] [G loss: 1.391635]\n",
      "404 [D loss: 0.372814, acc.: 93.75%, validation acc: 47.11%] [G loss: 1.376590]\n",
      "405 [D loss: 0.546659, acc.: 93.75%, validation acc: 46.81%] [G loss: 1.361294]\n",
      "406 [D loss: 0.403013, acc.: 84.38%, validation acc: 46.64%] [G loss: 1.304698]\n",
      "407 [D loss: 0.406609, acc.: 93.75%, validation acc: 45.88%] [G loss: 1.306338]\n",
      "408 [D loss: 0.314019, acc.: 96.88%, validation acc: 45.78%] [G loss: 1.404984]\n",
      "409 [D loss: 0.256055, acc.: 100.00%, validation acc: 45.96%] [G loss: 1.478927]\n",
      "410 [D loss: 0.324667, acc.: 96.88%, validation acc: 45.98%] [G loss: 1.492006]\n",
      "411 [D loss: 0.350028, acc.: 93.75%, validation acc: 46.05%] [G loss: 1.383283]\n",
      "412 [D loss: 0.254431, acc.: 100.00%, validation acc: 46.31%] [G loss: 1.503760]\n",
      "413 [D loss: 0.328763, acc.: 100.00%, validation acc: 46.44%] [G loss: 1.532698]\n",
      "414 [D loss: 0.282557, acc.: 100.00%, validation acc: 46.61%] [G loss: 1.555644]\n",
      "415 [D loss: 0.245471, acc.: 100.00%, validation acc: 46.62%] [G loss: 1.639700]\n",
      "416 [D loss: 0.404448, acc.: 93.75%, validation acc: 46.39%] [G loss: 1.380983]\n",
      "417 [D loss: 0.308918, acc.: 96.88%, validation acc: 46.27%] [G loss: 1.450020]\n",
      "418 [D loss: 0.271224, acc.: 100.00%, validation acc: 46.39%] [G loss: 1.467990]\n",
      "419 [D loss: 0.317080, acc.: 100.00%, validation acc: 46.49%] [G loss: 1.440840]\n",
      "420 [D loss: 0.322982, acc.: 96.88%, validation acc: 46.37%] [G loss: 1.517280]\n",
      "421 [D loss: 0.362967, acc.: 93.75%, validation acc: 45.94%] [G loss: 1.463716]\n",
      "422 [D loss: 0.301801, acc.: 100.00%, validation acc: 46.01%] [G loss: 1.438508]\n",
      "423 [D loss: 0.433962, acc.: 93.75%, validation acc: 45.74%] [G loss: 1.394352]\n",
      "424 [D loss: 0.267661, acc.: 100.00%, validation acc: 46.04%] [G loss: 1.393498]\n",
      "425 [D loss: 0.355843, acc.: 90.62%, validation acc: 45.47%] [G loss: 1.387515]\n",
      "426 [D loss: 0.334532, acc.: 96.88%, validation acc: 45.39%] [G loss: 1.418978]\n",
      "427 [D loss: 0.330293, acc.: 93.75%, validation acc: 45.12%] [G loss: 1.453646]\n",
      "428 [D loss: 0.344431, acc.: 93.75%, validation acc: 44.70%] [G loss: 1.432422]\n",
      "429 [D loss: 0.287965, acc.: 96.88%, validation acc: 44.71%] [G loss: 1.429562]\n",
      "430 [D loss: 0.688699, acc.: 93.75%, validation acc: 44.20%] [G loss: 1.351578]\n",
      "431 [D loss: 0.495121, acc.: 93.75%, validation acc: 43.65%] [G loss: 1.363894]\n",
      "432 [D loss: 0.290079, acc.: 96.88%, validation acc: 43.51%] [G loss: 1.344166]\n",
      "433 [D loss: 0.313798, acc.: 100.00%, validation acc: 43.87%] [G loss: 1.346634]\n",
      "434 [D loss: 0.263938, acc.: 100.00%, validation acc: 44.17%] [G loss: 1.428347]\n",
      "435 [D loss: 0.357930, acc.: 93.75%, validation acc: 44.88%] [G loss: 1.487345]\n",
      "436 [D loss: 0.744791, acc.: 96.88%, validation acc: 45.44%] [G loss: 1.591198]\n",
      "437 [D loss: 0.429311, acc.: 93.75%, validation acc: 44.88%] [G loss: 1.380631]\n",
      "438 [D loss: 0.398171, acc.: 90.62%, validation acc: 44.41%] [G loss: 1.348538]\n",
      "439 [D loss: 0.310030, acc.: 96.88%, validation acc: 44.49%] [G loss: 1.362621]\n",
      "440 [D loss: 0.293996, acc.: 96.88%, validation acc: 44.64%] [G loss: 1.403108]\n",
      "441 [D loss: 0.305316, acc.: 96.88%, validation acc: 44.58%] [G loss: 1.466596]\n",
      "442 [D loss: 0.261495, acc.: 96.88%, validation acc: 44.39%] [G loss: 1.457180]\n",
      "443 [D loss: 0.357955, acc.: 96.88%, validation acc: 44.40%] [G loss: 1.446935]\n",
      "444 [D loss: 0.435443, acc.: 90.62%, validation acc: 44.02%] [G loss: 1.374769]\n",
      "445 [D loss: 0.345624, acc.: 93.75%, validation acc: 43.56%] [G loss: 1.383067]\n",
      "446 [D loss: 0.262324, acc.: 100.00%, validation acc: 43.71%] [G loss: 1.453975]\n",
      "447 [D loss: 0.391530, acc.: 90.62%, validation acc: 43.91%] [G loss: 1.474328]\n",
      "448 [D loss: 0.311984, acc.: 96.88%, validation acc: 44.11%] [G loss: 1.484161]\n",
      "449 [D loss: 0.307253, acc.: 93.75%, validation acc: 44.44%] [G loss: 1.445678]\n",
      "450 [D loss: 0.249956, acc.: 100.00%, validation acc: 44.73%] [G loss: 1.519903]\n",
      "451 [D loss: 0.290414, acc.: 96.88%, validation acc: 44.85%] [G loss: 1.439044]\n",
      "452 [D loss: 0.407350, acc.: 90.62%, validation acc: 44.16%] [G loss: 1.269321]\n",
      "453 [D loss: 0.282864, acc.: 100.00%, validation acc: 44.16%] [G loss: 1.367175]\n",
      "454 [D loss: 0.245581, acc.: 100.00%, validation acc: 44.24%] [G loss: 1.330429]\n",
      "455 [D loss: 0.373064, acc.: 84.38%, validation acc: 44.07%] [G loss: 1.366253]\n",
      "456 [D loss: 0.240553, acc.: 96.88%, validation acc: 44.24%] [G loss: 1.368778]\n",
      "457 [D loss: 0.286045, acc.: 96.88%, validation acc: 44.59%] [G loss: 1.474660]\n",
      "458 [D loss: 0.282283, acc.: 96.88%, validation acc: 44.64%] [G loss: 1.487008]\n",
      "459 [D loss: 0.292024, acc.: 96.88%, validation acc: 45.02%] [G loss: 1.451701]\n",
      "460 [D loss: 0.332683, acc.: 93.75%, validation acc: 44.72%] [G loss: 1.405161]\n",
      "461 [D loss: 0.246147, acc.: 100.00%, validation acc: 45.06%] [G loss: 1.517318]\n",
      "462 [D loss: 0.307745, acc.: 96.88%, validation acc: 44.93%] [G loss: 1.510840]\n",
      "463 [D loss: 0.397124, acc.: 93.75%, validation acc: 44.10%] [G loss: 1.359289]\n",
      "464 [D loss: 0.490120, acc.: 84.38%, validation acc: 43.91%] [G loss: 1.405482]\n",
      "465 [D loss: 0.280379, acc.: 96.88%, validation acc: 44.33%] [G loss: 1.419174]\n",
      "466 [D loss: 0.625947, acc.: 90.62%, validation acc: 44.11%] [G loss: 1.324372]\n",
      "467 [D loss: 0.526078, acc.: 81.25%, validation acc: 43.84%] [G loss: 1.199038]\n",
      "468 [D loss: 0.267517, acc.: 100.00%, validation acc: 44.22%] [G loss: 1.201431]\n",
      "469 [D loss: 0.311816, acc.: 90.62%, validation acc: 44.66%] [G loss: 1.346112]\n",
      "470 [D loss: 0.268532, acc.: 93.75%, validation acc: 45.33%] [G loss: 1.509526]\n",
      "471 [D loss: 0.313639, acc.: 93.75%, validation acc: 45.49%] [G loss: 1.459493]\n",
      "472 [D loss: 0.330890, acc.: 93.75%, validation acc: 44.99%] [G loss: 1.433451]\n",
      "473 [D loss: 0.247551, acc.: 100.00%, validation acc: 45.19%] [G loss: 1.456602]\n",
      "474 [D loss: 0.262050, acc.: 96.88%, validation acc: 45.70%] [G loss: 1.390303]\n",
      "475 [D loss: 0.267894, acc.: 96.88%, validation acc: 46.16%] [G loss: 1.464390]\n",
      "476 [D loss: 0.257183, acc.: 96.88%, validation acc: 46.41%] [G loss: 1.529443]\n",
      "477 [D loss: 0.273793, acc.: 96.88%, validation acc: 46.68%] [G loss: 1.570354]\n",
      "478 [D loss: 0.249089, acc.: 96.88%, validation acc: 46.98%] [G loss: 1.607843]\n",
      "479 [D loss: 0.415050, acc.: 87.50%, validation acc: 47.54%] [G loss: 1.614076]\n",
      "480 [D loss: 0.298356, acc.: 96.88%, validation acc: 47.53%] [G loss: 1.621509]\n",
      "481 [D loss: 0.301454, acc.: 96.88%, validation acc: 47.31%] [G loss: 1.607485]\n",
      "482 [D loss: 0.282439, acc.: 96.88%, validation acc: 47.54%] [G loss: 1.574020]\n",
      "483 [D loss: 0.250759, acc.: 100.00%, validation acc: 47.91%] [G loss: 1.635364]\n",
      "484 [D loss: 0.218752, acc.: 100.00%, validation acc: 48.31%] [G loss: 1.636442]\n",
      "485 [D loss: 0.248844, acc.: 100.00%, validation acc: 48.14%] [G loss: 1.586538]\n",
      "486 [D loss: 0.246997, acc.: 96.88%, validation acc: 48.30%] [G loss: 1.532437]\n",
      "487 [D loss: 0.315512, acc.: 90.62%, validation acc: 48.02%] [G loss: 1.589336]\n",
      "488 [D loss: 0.371060, acc.: 93.75%, validation acc: 48.02%] [G loss: 1.451227]\n",
      "489 [D loss: 0.582327, acc.: 96.88%, validation acc: 47.56%] [G loss: 1.326012]\n",
      "490 [D loss: 0.521541, acc.: 93.75%, validation acc: 47.06%] [G loss: 1.218255]\n",
      "491 [D loss: 0.272301, acc.: 96.88%, validation acc: 47.19%] [G loss: 1.271996]\n",
      "492 [D loss: 0.338349, acc.: 93.75%, validation acc: 47.23%] [G loss: 1.262659]\n",
      "493 [D loss: 0.789013, acc.: 90.62%, validation acc: 47.33%] [G loss: 1.348950]\n",
      "494 [D loss: 0.424602, acc.: 93.75%, validation acc: 47.18%] [G loss: 1.376568]\n",
      "495 [D loss: 0.323128, acc.: 90.62%, validation acc: 47.15%] [G loss: 1.475328]\n",
      "496 [D loss: 0.285056, acc.: 96.88%, validation acc: 47.30%] [G loss: 1.505964]\n",
      "497 [D loss: 0.281198, acc.: 93.75%, validation acc: 47.48%] [G loss: 1.468860]\n",
      "498 [D loss: 0.243342, acc.: 96.88%, validation acc: 47.49%] [G loss: 1.493604]\n",
      "499 [D loss: 0.230515, acc.: 100.00%, validation acc: 47.45%] [G loss: 1.508091]\n",
      "500 [D loss: 0.225542, acc.: 100.00%, validation acc: 47.62%] [G loss: 1.558484]\n",
      "501 [D loss: 0.272864, acc.: 96.88%, validation acc: 47.44%] [G loss: 1.716695]\n",
      "502 [D loss: 0.405120, acc.: 93.75%, validation acc: 47.06%] [G loss: 1.498663]\n",
      "503 [D loss: 0.258824, acc.: 100.00%, validation acc: 47.11%] [G loss: 1.540580]\n",
      "504 [D loss: 0.776360, acc.: 90.62%, validation acc: 46.74%] [G loss: 1.482751]\n",
      "505 [D loss: 0.271839, acc.: 96.88%, validation acc: 46.68%] [G loss: 1.456081]\n",
      "506 [D loss: 0.261122, acc.: 96.88%, validation acc: 46.80%] [G loss: 1.496782]\n",
      "507 [D loss: 0.252711, acc.: 96.88%, validation acc: 46.69%] [G loss: 1.463568]\n",
      "508 [D loss: 0.239280, acc.: 100.00%, validation acc: 46.73%] [G loss: 1.469340]\n",
      "509 [D loss: 0.269280, acc.: 96.88%, validation acc: 46.83%] [G loss: 1.454851]\n",
      "510 [D loss: 0.238784, acc.: 100.00%, validation acc: 46.98%] [G loss: 1.469353]\n",
      "511 [D loss: 0.233242, acc.: 100.00%, validation acc: 47.03%] [G loss: 1.561259]\n",
      "512 [D loss: 0.297242, acc.: 96.88%, validation acc: 46.95%] [G loss: 1.600124]\n",
      "513 [D loss: 0.356757, acc.: 90.62%, validation acc: 46.46%] [G loss: 1.485665]\n",
      "514 [D loss: 0.394971, acc.: 93.75%, validation acc: 45.73%] [G loss: 1.463869]\n",
      "515 [D loss: 0.234893, acc.: 100.00%, validation acc: 45.17%] [G loss: 1.476969]\n",
      "516 [D loss: 0.228881, acc.: 100.00%, validation acc: 45.10%] [G loss: 1.480255]\n",
      "517 [D loss: 0.284152, acc.: 93.75%, validation acc: 45.02%] [G loss: 1.528012]\n",
      "518 [D loss: 0.241627, acc.: 100.00%, validation acc: 44.99%] [G loss: 1.413542]\n",
      "519 [D loss: 0.242515, acc.: 96.88%, validation acc: 45.23%] [G loss: 1.570204]\n",
      "520 [D loss: 0.433571, acc.: 90.62%, validation acc: 45.45%] [G loss: 1.471326]\n",
      "521 [D loss: 0.323892, acc.: 93.75%, validation acc: 45.93%] [G loss: 1.486568]\n",
      "522 [D loss: 0.226869, acc.: 100.00%, validation acc: 46.25%] [G loss: 1.542098]\n",
      "523 [D loss: 0.362609, acc.: 96.88%, validation acc: 45.94%] [G loss: 1.498267]\n",
      "524 [D loss: 0.253303, acc.: 96.88%, validation acc: 45.80%] [G loss: 1.510550]\n",
      "525 [D loss: 0.227004, acc.: 100.00%, validation acc: 45.86%] [G loss: 1.559221]\n",
      "526 [D loss: 0.217753, acc.: 100.00%, validation acc: 45.82%] [G loss: 1.555208]\n",
      "527 [D loss: 0.265910, acc.: 96.88%, validation acc: 45.79%] [G loss: 1.580687]\n",
      "528 [D loss: 0.211855, acc.: 100.00%, validation acc: 46.14%] [G loss: 1.701172]\n",
      "529 [D loss: 0.230708, acc.: 100.00%, validation acc: 46.23%] [G loss: 1.671911]\n",
      "530 [D loss: 0.238122, acc.: 100.00%, validation acc: 46.51%] [G loss: 1.751308]\n",
      "531 [D loss: 0.229621, acc.: 96.88%, validation acc: 46.70%] [G loss: 1.625120]\n",
      "532 [D loss: 0.240982, acc.: 96.88%, validation acc: 46.54%] [G loss: 1.705262]\n",
      "533 [D loss: 0.222018, acc.: 100.00%, validation acc: 46.64%] [G loss: 1.625927]\n",
      "534 [D loss: 0.248451, acc.: 96.88%, validation acc: 46.77%] [G loss: 1.608051]\n",
      "535 [D loss: 0.192587, acc.: 100.00%, validation acc: 46.79%] [G loss: 1.702976]\n",
      "536 [D loss: 0.366483, acc.: 96.88%, validation acc: 46.69%] [G loss: 1.618808]\n",
      "537 [D loss: 0.223938, acc.: 96.88%, validation acc: 46.74%] [G loss: 1.560875]\n",
      "538 [D loss: 0.221495, acc.: 100.00%, validation acc: 46.81%] [G loss: 1.585616]\n",
      "539 [D loss: 0.206285, acc.: 100.00%, validation acc: 47.14%] [G loss: 1.761579]\n",
      "540 [D loss: 0.242139, acc.: 96.88%, validation acc: 47.19%] [G loss: 1.610408]\n",
      "541 [D loss: 0.565268, acc.: 90.62%, validation acc: 46.77%] [G loss: 1.545855]\n",
      "542 [D loss: 0.268841, acc.: 96.88%, validation acc: 46.70%] [G loss: 1.564606]\n",
      "543 [D loss: 0.323197, acc.: 93.75%, validation acc: 46.69%] [G loss: 1.483552]\n",
      "544 [D loss: 0.432595, acc.: 93.75%, validation acc: 46.20%] [G loss: 1.403501]\n",
      "545 [D loss: 0.237768, acc.: 96.88%, validation acc: 46.40%] [G loss: 1.452344]\n",
      "546 [D loss: 0.715247, acc.: 96.88%, validation acc: 46.81%] [G loss: 1.505553]\n",
      "547 [D loss: 0.228230, acc.: 100.00%, validation acc: 46.99%] [G loss: 1.651281]\n",
      "548 [D loss: 0.273523, acc.: 96.88%, validation acc: 47.53%] [G loss: 1.649393]\n",
      "549 [D loss: 0.295880, acc.: 96.88%, validation acc: 47.39%] [G loss: 1.522009]\n",
      "550 [D loss: 0.287944, acc.: 96.88%, validation acc: 47.41%] [G loss: 1.564160]\n",
      "551 [D loss: 0.386686, acc.: 93.75%, validation acc: 46.49%] [G loss: 1.468240]\n",
      "552 [D loss: 0.297665, acc.: 93.75%, validation acc: 46.03%] [G loss: 1.446119]\n",
      "553 [D loss: 0.316968, acc.: 93.75%, validation acc: 46.59%] [G loss: 1.500014]\n",
      "554 [D loss: 0.216019, acc.: 96.88%, validation acc: 46.68%] [G loss: 1.431287]\n",
      "555 [D loss: 0.356975, acc.: 93.75%, validation acc: 46.41%] [G loss: 1.580396]\n",
      "556 [D loss: 0.222903, acc.: 100.00%, validation acc: 46.56%] [G loss: 1.572536]\n",
      "557 [D loss: 0.395725, acc.: 96.88%, validation acc: 46.90%] [G loss: 1.530427]\n",
      "558 [D loss: 0.210679, acc.: 100.00%, validation acc: 47.13%] [G loss: 1.668111]\n",
      "559 [D loss: 0.242439, acc.: 96.88%, validation acc: 47.11%] [G loss: 1.651636]\n",
      "560 [D loss: 0.230120, acc.: 96.88%, validation acc: 47.12%] [G loss: 1.712674]\n",
      "561 [D loss: 0.932826, acc.: 87.50%, validation acc: 47.09%] [G loss: 1.602016]\n",
      "562 [D loss: 0.204787, acc.: 100.00%, validation acc: 47.16%] [G loss: 1.704083]\n",
      "563 [D loss: 0.319418, acc.: 96.88%, validation acc: 47.14%] [G loss: 1.541365]\n",
      "564 [D loss: 0.205198, acc.: 100.00%, validation acc: 47.24%] [G loss: 1.648393]\n",
      "565 [D loss: 0.194610, acc.: 100.00%, validation acc: 47.49%] [G loss: 1.704439]\n",
      "566 [D loss: 0.319050, acc.: 93.75%, validation acc: 47.11%] [G loss: 1.619913]\n",
      "567 [D loss: 0.201280, acc.: 100.00%, validation acc: 46.87%] [G loss: 1.605380]\n",
      "568 [D loss: 0.206409, acc.: 100.00%, validation acc: 46.52%] [G loss: 1.636983]\n",
      "569 [D loss: 0.318646, acc.: 93.75%, validation acc: 46.26%] [G loss: 1.693563]\n",
      "570 [D loss: 0.462285, acc.: 96.88%, validation acc: 46.11%] [G loss: 1.589100]\n",
      "571 [D loss: 0.276122, acc.: 96.88%, validation acc: 46.10%] [G loss: 1.641379]\n",
      "572 [D loss: 0.254382, acc.: 93.75%, validation acc: 46.11%] [G loss: 1.575884]\n",
      "573 [D loss: 0.207601, acc.: 100.00%, validation acc: 46.38%] [G loss: 1.669041]\n",
      "574 [D loss: 0.228001, acc.: 100.00%, validation acc: 46.79%] [G loss: 1.613564]\n",
      "575 [D loss: 0.186005, acc.: 100.00%, validation acc: 47.49%] [G loss: 1.654452]\n",
      "576 [D loss: 0.269822, acc.: 96.88%, validation acc: 47.35%] [G loss: 1.707347]\n",
      "577 [D loss: 0.238700, acc.: 100.00%, validation acc: 47.36%] [G loss: 1.611271]\n",
      "578 [D loss: 0.230645, acc.: 96.88%, validation acc: 47.58%] [G loss: 1.648644]\n",
      "579 [D loss: 0.223302, acc.: 100.00%, validation acc: 47.64%] [G loss: 1.732853]\n",
      "580 [D loss: 0.219576, acc.: 96.88%, validation acc: 47.43%] [G loss: 1.677318]\n",
      "581 [D loss: 0.223501, acc.: 96.88%, validation acc: 47.19%] [G loss: 1.742803]\n",
      "582 [D loss: 0.238915, acc.: 96.88%, validation acc: 47.14%] [G loss: 1.679671]\n",
      "583 [D loss: 0.230836, acc.: 96.88%, validation acc: 47.22%] [G loss: 1.682292]\n",
      "584 [D loss: 0.245236, acc.: 96.88%, validation acc: 47.17%] [G loss: 1.675349]\n",
      "585 [D loss: 0.471075, acc.: 90.62%, validation acc: 47.30%] [G loss: 1.594909]\n",
      "586 [D loss: 0.247773, acc.: 96.88%, validation acc: 47.59%] [G loss: 1.611657]\n",
      "587 [D loss: 0.320604, acc.: 90.62%, validation acc: 47.12%] [G loss: 1.720530]\n",
      "588 [D loss: 0.371151, acc.: 93.75%, validation acc: 46.87%] [G loss: 1.550739]\n",
      "589 [D loss: 0.269350, acc.: 93.75%, validation acc: 46.54%] [G loss: 1.564187]\n",
      "590 [D loss: 0.250291, acc.: 96.88%, validation acc: 46.49%] [G loss: 1.515550]\n",
      "591 [D loss: 0.209934, acc.: 100.00%, validation acc: 46.72%] [G loss: 1.635035]\n",
      "592 [D loss: 0.468326, acc.: 87.50%, validation acc: 46.43%] [G loss: 1.476307]\n",
      "593 [D loss: 0.213632, acc.: 100.00%, validation acc: 46.30%] [G loss: 1.549762]\n",
      "594 [D loss: 0.222603, acc.: 100.00%, validation acc: 46.28%] [G loss: 1.562627]\n",
      "595 [D loss: 0.180639, acc.: 100.00%, validation acc: 46.42%] [G loss: 1.576610]\n",
      "596 [D loss: 0.217995, acc.: 100.00%, validation acc: 46.55%] [G loss: 1.776049]\n",
      "597 [D loss: 0.229269, acc.: 100.00%, validation acc: 46.43%] [G loss: 1.620401]\n",
      "598 [D loss: 0.222953, acc.: 100.00%, validation acc: 46.57%] [G loss: 1.652329]\n",
      "599 [D loss: 0.234230, acc.: 96.88%, validation acc: 46.62%] [G loss: 1.662255]\n",
      "600 [D loss: 0.209086, acc.: 100.00%, validation acc: 46.74%] [G loss: 1.765119]\n",
      "601 [D loss: 0.478883, acc.: 90.62%, validation acc: 45.64%] [G loss: 1.477122]\n",
      "602 [D loss: 0.203294, acc.: 100.00%, validation acc: 45.47%] [G loss: 1.471633]\n",
      "603 [D loss: 0.290954, acc.: 93.75%, validation acc: 45.48%] [G loss: 1.521402]\n",
      "604 [D loss: 0.338831, acc.: 96.88%, validation acc: 45.27%] [G loss: 1.531211]\n",
      "605 [D loss: 0.219565, acc.: 100.00%, validation acc: 45.32%] [G loss: 1.539681]\n",
      "606 [D loss: 0.404580, acc.: 96.88%, validation acc: 45.32%] [G loss: 1.609907]\n",
      "607 [D loss: 0.207808, acc.: 100.00%, validation acc: 45.49%] [G loss: 1.650463]\n",
      "608 [D loss: 0.233666, acc.: 96.88%, validation acc: 45.52%] [G loss: 1.747434]\n",
      "609 [D loss: 0.209685, acc.: 100.00%, validation acc: 45.55%] [G loss: 1.820644]\n",
      "610 [D loss: 0.158993, acc.: 100.00%, validation acc: 45.58%] [G loss: 1.832350]\n",
      "611 [D loss: 0.217369, acc.: 96.88%, validation acc: 45.35%] [G loss: 1.756467]\n",
      "612 [D loss: 0.242754, acc.: 96.88%, validation acc: 45.36%] [G loss: 1.711890]\n",
      "613 [D loss: 0.174332, acc.: 100.00%, validation acc: 45.33%] [G loss: 1.837833]\n",
      "614 [D loss: 0.387439, acc.: 96.88%, validation acc: 45.06%] [G loss: 1.762104]\n",
      "615 [D loss: 0.158208, acc.: 100.00%, validation acc: 45.06%] [G loss: 1.689130]\n",
      "616 [D loss: 0.244764, acc.: 96.88%, validation acc: 45.28%] [G loss: 1.714867]\n",
      "617 [D loss: 0.187174, acc.: 100.00%, validation acc: 45.35%] [G loss: 1.740763]\n",
      "618 [D loss: 0.171900, acc.: 100.00%, validation acc: 45.26%] [G loss: 1.747429]\n",
      "619 [D loss: 0.322049, acc.: 93.75%, validation acc: 45.14%] [G loss: 1.498563]\n",
      "620 [D loss: 0.180381, acc.: 100.00%, validation acc: 45.22%] [G loss: 1.689372]\n",
      "621 [D loss: 0.288377, acc.: 96.88%, validation acc: 45.39%] [G loss: 1.686005]\n",
      "622 [D loss: 0.303166, acc.: 93.75%, validation acc: 45.13%] [G loss: 1.691459]\n",
      "623 [D loss: 0.153254, acc.: 100.00%, validation acc: 45.19%] [G loss: 1.760052]\n",
      "624 [D loss: 0.197553, acc.: 100.00%, validation acc: 45.44%] [G loss: 1.812273]\n",
      "625 [D loss: 0.188582, acc.: 100.00%, validation acc: 45.39%] [G loss: 1.752874]\n",
      "626 [D loss: 0.203039, acc.: 96.88%, validation acc: 45.23%] [G loss: 1.691769]\n",
      "627 [D loss: 0.273503, acc.: 93.75%, validation acc: 45.15%] [G loss: 1.569512]\n",
      "628 [D loss: 0.397885, acc.: 93.75%, validation acc: 45.05%] [G loss: 1.601632]\n",
      "629 [D loss: 0.215332, acc.: 100.00%, validation acc: 45.03%] [G loss: 1.679748]\n",
      "630 [D loss: 0.238347, acc.: 96.88%, validation acc: 44.89%] [G loss: 1.606188]\n",
      "631 [D loss: 0.170314, acc.: 100.00%, validation acc: 44.62%] [G loss: 1.654315]\n",
      "632 [D loss: 0.194581, acc.: 100.00%, validation acc: 44.54%] [G loss: 1.670720]\n",
      "633 [D loss: 0.248056, acc.: 96.88%, validation acc: 44.58%] [G loss: 1.686186]\n",
      "634 [D loss: 0.286015, acc.: 93.75%, validation acc: 44.92%] [G loss: 1.693371]\n",
      "635 [D loss: 0.719740, acc.: 93.75%, validation acc: 45.05%] [G loss: 1.800543]\n",
      "636 [D loss: 0.788544, acc.: 93.75%, validation acc: 44.96%] [G loss: 1.724012]\n",
      "637 [D loss: 0.187423, acc.: 100.00%, validation acc: 45.01%] [G loss: 1.746322]\n",
      "638 [D loss: 0.158057, acc.: 100.00%, validation acc: 45.12%] [G loss: 1.797999]\n",
      "639 [D loss: 0.207204, acc.: 96.88%, validation acc: 44.98%] [G loss: 1.773830]\n",
      "640 [D loss: 0.348271, acc.: 93.75%, validation acc: 44.84%] [G loss: 1.730546]\n",
      "641 [D loss: 0.175286, acc.: 100.00%, validation acc: 45.08%] [G loss: 1.865614]\n",
      "642 [D loss: 0.219429, acc.: 96.88%, validation acc: 45.28%] [G loss: 1.758946]\n",
      "643 [D loss: 0.275272, acc.: 93.75%, validation acc: 45.19%] [G loss: 1.764459]\n",
      "644 [D loss: 0.340338, acc.: 96.88%, validation acc: 45.33%] [G loss: 1.761870]\n",
      "645 [D loss: 0.188364, acc.: 100.00%, validation acc: 45.58%] [G loss: 1.770121]\n",
      "646 [D loss: 0.154013, acc.: 100.00%, validation acc: 45.80%] [G loss: 1.856713]\n",
      "647 [D loss: 0.157381, acc.: 100.00%, validation acc: 45.95%] [G loss: 1.871496]\n",
      "648 [D loss: 0.229393, acc.: 96.88%, validation acc: 46.25%] [G loss: 1.889645]\n",
      "649 [D loss: 0.297243, acc.: 93.75%, validation acc: 46.08%] [G loss: 1.696767]\n",
      "650 [D loss: 0.201812, acc.: 96.88%, validation acc: 46.15%] [G loss: 1.736404]\n",
      "651 [D loss: 0.313457, acc.: 96.88%, validation acc: 46.21%] [G loss: 1.749569]\n",
      "652 [D loss: 0.195522, acc.: 96.88%, validation acc: 45.91%] [G loss: 1.687876]\n",
      "653 [D loss: 0.374169, acc.: 93.75%, validation acc: 45.53%] [G loss: 1.732578]\n",
      "654 [D loss: 0.227488, acc.: 96.88%, validation acc: 45.60%] [G loss: 1.770424]\n",
      "655 [D loss: 0.194652, acc.: 100.00%, validation acc: 45.78%] [G loss: 1.739275]\n",
      "656 [D loss: 0.214494, acc.: 96.88%, validation acc: 46.08%] [G loss: 1.693408]\n",
      "657 [D loss: 0.222820, acc.: 96.88%, validation acc: 46.32%] [G loss: 1.806504]\n",
      "658 [D loss: 0.218949, acc.: 96.88%, validation acc: 45.95%] [G loss: 1.774009]\n",
      "659 [D loss: 0.207618, acc.: 96.88%, validation acc: 45.54%] [G loss: 1.649074]\n",
      "660 [D loss: 1.038190, acc.: 84.38%, validation acc: 45.32%] [G loss: 1.492794]\n",
      "661 [D loss: 0.392372, acc.: 93.75%, validation acc: 45.20%] [G loss: 1.444167]\n",
      "662 [D loss: 0.177686, acc.: 100.00%, validation acc: 45.39%] [G loss: 1.574417]\n",
      "663 [D loss: 0.176690, acc.: 100.00%, validation acc: 45.74%] [G loss: 1.600094]\n",
      "664 [D loss: 0.254717, acc.: 93.75%, validation acc: 45.73%] [G loss: 1.734866]\n",
      "665 [D loss: 0.152969, acc.: 100.00%, validation acc: 45.83%] [G loss: 1.823993]\n",
      "666 [D loss: 0.218015, acc.: 96.88%, validation acc: 45.95%] [G loss: 1.903268]\n",
      "667 [D loss: 0.160536, acc.: 100.00%, validation acc: 46.03%] [G loss: 1.954124]\n",
      "668 [D loss: 0.477335, acc.: 93.75%, validation acc: 45.85%] [G loss: 1.793999]\n",
      "669 [D loss: 0.174288, acc.: 100.00%, validation acc: 45.76%] [G loss: 1.922435]\n",
      "670 [D loss: 0.126783, acc.: 100.00%, validation acc: 45.86%] [G loss: 1.932600]\n",
      "671 [D loss: 0.225038, acc.: 96.88%, validation acc: 45.74%] [G loss: 1.722319]\n",
      "672 [D loss: 0.743964, acc.: 90.62%, validation acc: 45.50%] [G loss: 1.580001]\n",
      "673 [D loss: 0.151167, acc.: 100.00%, validation acc: 45.78%] [G loss: 1.728394]\n",
      "674 [D loss: 0.155509, acc.: 100.00%, validation acc: 45.97%] [G loss: 1.733999]\n",
      "675 [D loss: 0.384079, acc.: 96.88%, validation acc: 46.11%] [G loss: 1.667762]\n",
      "676 [D loss: 0.249957, acc.: 93.75%, validation acc: 45.50%] [G loss: 1.794289]\n",
      "677 [D loss: 0.138261, acc.: 100.00%, validation acc: 45.59%] [G loss: 1.861776]\n",
      "678 [D loss: 0.175887, acc.: 100.00%, validation acc: 45.56%] [G loss: 1.788964]\n",
      "679 [D loss: 0.133711, acc.: 100.00%, validation acc: 45.75%] [G loss: 1.896047]\n",
      "680 [D loss: 0.215294, acc.: 96.88%, validation acc: 45.98%] [G loss: 1.876713]\n",
      "681 [D loss: 0.160106, acc.: 100.00%, validation acc: 46.09%] [G loss: 1.917190]\n",
      "682 [D loss: 0.214026, acc.: 93.75%, validation acc: 45.57%] [G loss: 1.802101]\n",
      "683 [D loss: 0.287020, acc.: 93.75%, validation acc: 45.61%] [G loss: 1.743200]\n",
      "684 [D loss: 0.153074, acc.: 100.00%, validation acc: 45.96%] [G loss: 1.710450]\n",
      "685 [D loss: 0.195174, acc.: 96.88%, validation acc: 46.31%] [G loss: 1.701640]\n",
      "686 [D loss: 0.204749, acc.: 96.88%, validation acc: 46.57%] [G loss: 1.791349]\n",
      "687 [D loss: 0.193696, acc.: 100.00%, validation acc: 46.79%] [G loss: 1.745041]\n",
      "688 [D loss: 0.256273, acc.: 93.75%, validation acc: 46.72%] [G loss: 1.802375]\n",
      "689 [D loss: 0.186803, acc.: 100.00%, validation acc: 46.85%] [G loss: 1.907232]\n",
      "690 [D loss: 0.182488, acc.: 100.00%, validation acc: 46.80%] [G loss: 1.782040]\n",
      "691 [D loss: 0.206843, acc.: 96.88%, validation acc: 46.80%] [G loss: 1.691270]\n",
      "692 [D loss: 0.232191, acc.: 96.88%, validation acc: 47.22%] [G loss: 1.722644]\n",
      "693 [D loss: 0.489362, acc.: 90.62%, validation acc: 46.80%] [G loss: 1.618232]\n",
      "694 [D loss: 0.439439, acc.: 93.75%, validation acc: 46.45%] [G loss: 1.683233]\n",
      "695 [D loss: 0.227043, acc.: 93.75%, validation acc: 45.90%] [G loss: 1.609007]\n",
      "696 [D loss: 0.164464, acc.: 100.00%, validation acc: 45.71%] [G loss: 1.705474]\n",
      "697 [D loss: 0.147997, acc.: 100.00%, validation acc: 45.79%] [G loss: 1.719652]\n",
      "698 [D loss: 0.176230, acc.: 100.00%, validation acc: 45.90%] [G loss: 1.763203]\n",
      "699 [D loss: 0.198387, acc.: 96.88%, validation acc: 45.67%] [G loss: 1.835993]\n",
      "700 [D loss: 0.153354, acc.: 100.00%, validation acc: 45.78%] [G loss: 1.876538]\n",
      "701 [D loss: 0.146856, acc.: 100.00%, validation acc: 45.90%] [G loss: 1.881450]\n",
      "702 [D loss: 0.141189, acc.: 100.00%, validation acc: 46.27%] [G loss: 1.882168]\n",
      "703 [D loss: 0.140187, acc.: 100.00%, validation acc: 46.40%] [G loss: 1.933285]\n",
      "704 [D loss: 0.358661, acc.: 96.88%, validation acc: 46.15%] [G loss: 1.948874]\n",
      "705 [D loss: 0.198680, acc.: 100.00%, validation acc: 46.09%] [G loss: 2.003638]\n",
      "706 [D loss: 0.149431, acc.: 100.00%, validation acc: 46.11%] [G loss: 1.918719]\n",
      "707 [D loss: 0.171311, acc.: 100.00%, validation acc: 46.01%] [G loss: 1.945273]\n",
      "708 [D loss: 0.149221, acc.: 100.00%, validation acc: 46.20%] [G loss: 1.968804]\n",
      "709 [D loss: 0.288930, acc.: 96.88%, validation acc: 45.75%] [G loss: 1.959904]\n",
      "710 [D loss: 0.167114, acc.: 100.00%, validation acc: 45.70%] [G loss: 1.976062]\n",
      "711 [D loss: 0.145592, acc.: 96.88%, validation acc: 45.60%] [G loss: 1.979703]\n",
      "712 [D loss: 0.217729, acc.: 96.88%, validation acc: 44.97%] [G loss: 2.004923]\n",
      "713 [D loss: 0.220529, acc.: 96.88%, validation acc: 44.74%] [G loss: 1.992025]\n",
      "714 [D loss: 0.290042, acc.: 96.88%, validation acc: 44.55%] [G loss: 1.898948]\n",
      "715 [D loss: 0.384870, acc.: 90.62%, validation acc: 44.13%] [G loss: 1.710933]\n",
      "716 [D loss: 0.128168, acc.: 100.00%, validation acc: 44.47%] [G loss: 1.818008]\n",
      "717 [D loss: 0.167562, acc.: 96.88%, validation acc: 44.28%] [G loss: 1.730217]\n",
      "718 [D loss: 0.325716, acc.: 93.75%, validation acc: 44.23%] [G loss: 1.680107]\n",
      "719 [D loss: 0.180872, acc.: 96.88%, validation acc: 44.52%] [G loss: 1.789685]\n",
      "720 [D loss: 0.133736, acc.: 100.00%, validation acc: 44.91%] [G loss: 1.828279]\n",
      "721 [D loss: 0.143348, acc.: 100.00%, validation acc: 45.34%] [G loss: 1.954847]\n",
      "722 [D loss: 0.196785, acc.: 96.88%, validation acc: 45.45%] [G loss: 1.904146]\n",
      "723 [D loss: 0.303565, acc.: 96.88%, validation acc: 45.27%] [G loss: 1.753451]\n",
      "724 [D loss: 0.129080, acc.: 100.00%, validation acc: 45.26%] [G loss: 1.805161]\n",
      "725 [D loss: 0.163187, acc.: 100.00%, validation acc: 45.41%] [G loss: 1.878738]\n",
      "726 [D loss: 0.175941, acc.: 96.88%, validation acc: 45.24%] [G loss: 1.796666]\n",
      "727 [D loss: 0.192470, acc.: 96.88%, validation acc: 45.20%] [G loss: 1.965842]\n",
      "728 [D loss: 0.266857, acc.: 93.75%, validation acc: 44.79%] [G loss: 1.715353]\n",
      "729 [D loss: 0.170327, acc.: 100.00%, validation acc: 44.87%] [G loss: 1.737757]\n",
      "730 [D loss: 0.139519, acc.: 100.00%, validation acc: 45.26%] [G loss: 1.884496]\n",
      "731 [D loss: 0.148630, acc.: 100.00%, validation acc: 45.61%] [G loss: 1.935348]\n",
      "732 [D loss: 0.131314, acc.: 100.00%, validation acc: 45.76%] [G loss: 1.997889]\n",
      "733 [D loss: 0.347864, acc.: 87.50%, validation acc: 45.56%] [G loss: 1.864348]\n",
      "734 [D loss: 0.192817, acc.: 96.88%, validation acc: 45.50%] [G loss: 1.882832]\n",
      "735 [D loss: 0.252838, acc.: 96.88%, validation acc: 45.17%] [G loss: 1.870032]\n",
      "736 [D loss: 0.251796, acc.: 90.62%, validation acc: 44.92%] [G loss: 1.819290]\n",
      "737 [D loss: 0.261572, acc.: 93.75%, validation acc: 44.91%] [G loss: 1.731144]\n",
      "738 [D loss: 0.161291, acc.: 100.00%, validation acc: 45.21%] [G loss: 1.823747]\n",
      "739 [D loss: 0.122877, acc.: 100.00%, validation acc: 45.45%] [G loss: 1.933719]\n",
      "740 [D loss: 0.278155, acc.: 96.88%, validation acc: 45.71%] [G loss: 2.064712]\n",
      "741 [D loss: 0.140041, acc.: 100.00%, validation acc: 45.82%] [G loss: 1.995753]\n",
      "742 [D loss: 0.238354, acc.: 96.88%, validation acc: 45.83%] [G loss: 2.067693]\n",
      "743 [D loss: 0.321902, acc.: 96.88%, validation acc: 45.81%] [G loss: 1.978402]\n",
      "744 [D loss: 0.134142, acc.: 100.00%, validation acc: 45.92%] [G loss: 2.033327]\n",
      "745 [D loss: 0.149557, acc.: 100.00%, validation acc: 46.06%] [G loss: 1.960002]\n",
      "746 [D loss: 0.185732, acc.: 96.88%, validation acc: 45.94%] [G loss: 2.171217]\n",
      "747 [D loss: 0.164448, acc.: 100.00%, validation acc: 45.51%] [G loss: 1.941824]\n",
      "748 [D loss: 0.141456, acc.: 100.00%, validation acc: 45.27%] [G loss: 1.893982]\n",
      "749 [D loss: 0.211337, acc.: 93.75%, validation acc: 45.42%] [G loss: 1.936855]\n",
      "750 [D loss: 0.140046, acc.: 100.00%, validation acc: 45.33%] [G loss: 1.987340]\n",
      "751 [D loss: 0.164429, acc.: 100.00%, validation acc: 45.19%] [G loss: 2.139383]\n",
      "752 [D loss: 0.168867, acc.: 96.88%, validation acc: 44.84%] [G loss: 1.998376]\n",
      "753 [D loss: 0.315370, acc.: 90.62%, validation acc: 43.92%] [G loss: 1.739245]\n",
      "754 [D loss: 0.153562, acc.: 100.00%, validation acc: 43.97%] [G loss: 1.886488]\n",
      "755 [D loss: 0.206811, acc.: 96.88%, validation acc: 44.15%] [G loss: 1.782288]\n",
      "756 [D loss: 0.173427, acc.: 96.88%, validation acc: 44.51%] [G loss: 1.892053]\n",
      "757 [D loss: 0.189289, acc.: 96.88%, validation acc: 44.77%] [G loss: 2.096732]\n",
      "758 [D loss: 0.133528, acc.: 100.00%, validation acc: 44.83%] [G loss: 2.085127]\n",
      "759 [D loss: 0.209506, acc.: 96.88%, validation acc: 45.08%] [G loss: 2.019915]\n",
      "760 [D loss: 0.139262, acc.: 100.00%, validation acc: 45.18%] [G loss: 2.108883]\n",
      "761 [D loss: 0.148764, acc.: 100.00%, validation acc: 45.09%] [G loss: 2.012466]\n",
      "762 [D loss: 0.120149, acc.: 100.00%, validation acc: 45.10%] [G loss: 2.010723]\n",
      "763 [D loss: 0.293014, acc.: 96.88%, validation acc: 44.87%] [G loss: 1.902286]\n",
      "764 [D loss: 0.115016, acc.: 100.00%, validation acc: 44.92%] [G loss: 1.983364]\n",
      "765 [D loss: 0.109010, acc.: 100.00%, validation acc: 44.96%] [G loss: 2.081856]\n",
      "766 [D loss: 0.159640, acc.: 96.88%, validation acc: 44.92%] [G loss: 2.031988]\n",
      "767 [D loss: 0.372507, acc.: 93.75%, validation acc: 44.75%] [G loss: 1.929067]\n",
      "768 [D loss: 0.153818, acc.: 100.00%, validation acc: 44.69%] [G loss: 1.818154]\n",
      "769 [D loss: 0.120794, acc.: 100.00%, validation acc: 44.75%] [G loss: 1.904847]\n",
      "770 [D loss: 0.135538, acc.: 100.00%, validation acc: 44.78%] [G loss: 1.892684]\n",
      "771 [D loss: 0.195156, acc.: 96.88%, validation acc: 45.04%] [G loss: 1.989452]\n",
      "772 [D loss: 0.157658, acc.: 100.00%, validation acc: 45.33%] [G loss: 2.033942]\n",
      "773 [D loss: 0.116700, acc.: 100.00%, validation acc: 45.49%] [G loss: 2.063103]\n",
      "774 [D loss: 0.250754, acc.: 96.88%, validation acc: 45.30%] [G loss: 1.951716]\n",
      "775 [D loss: 0.160702, acc.: 100.00%, validation acc: 45.26%] [G loss: 1.904622]\n",
      "776 [D loss: 0.348277, acc.: 93.75%, validation acc: 45.08%] [G loss: 1.922318]\n",
      "777 [D loss: 0.192472, acc.: 96.88%, validation acc: 45.29%] [G loss: 1.975199]\n",
      "778 [D loss: 0.127249, acc.: 100.00%, validation acc: 45.48%] [G loss: 1.977640]\n",
      "779 [D loss: 0.118200, acc.: 100.00%, validation acc: 45.65%] [G loss: 2.197125]\n",
      "780 [D loss: 0.620301, acc.: 96.88%, validation acc: 45.73%] [G loss: 2.237364]\n",
      "781 [D loss: 0.405191, acc.: 96.88%, validation acc: 45.60%] [G loss: 2.067671]\n",
      "782 [D loss: 0.155280, acc.: 100.00%, validation acc: 45.57%] [G loss: 1.949751]\n",
      "783 [D loss: 0.211010, acc.: 96.88%, validation acc: 45.49%] [G loss: 1.962967]\n",
      "784 [D loss: 0.345895, acc.: 96.88%, validation acc: 45.22%] [G loss: 1.852228]\n",
      "785 [D loss: 0.142145, acc.: 100.00%, validation acc: 45.21%] [G loss: 1.838426]\n",
      "786 [D loss: 0.144106, acc.: 96.88%, validation acc: 45.19%] [G loss: 1.844443]\n",
      "787 [D loss: 0.143802, acc.: 100.00%, validation acc: 45.35%] [G loss: 1.964458]\n",
      "788 [D loss: 0.216787, acc.: 96.88%, validation acc: 45.20%] [G loss: 1.986748]\n",
      "789 [D loss: 0.125572, acc.: 100.00%, validation acc: 45.32%] [G loss: 1.958522]\n",
      "790 [D loss: 0.260029, acc.: 93.75%, validation acc: 44.80%] [G loss: 1.870704]\n",
      "791 [D loss: 0.200243, acc.: 96.88%, validation acc: 44.25%] [G loss: 1.849775]\n",
      "792 [D loss: 0.128747, acc.: 100.00%, validation acc: 44.43%] [G loss: 1.907009]\n",
      "793 [D loss: 0.199802, acc.: 96.88%, validation acc: 44.29%] [G loss: 2.009917]\n",
      "794 [D loss: 0.110117, acc.: 100.00%, validation acc: 44.32%] [G loss: 2.067677]\n",
      "795 [D loss: 0.137485, acc.: 100.00%, validation acc: 44.58%] [G loss: 2.020529]\n",
      "796 [D loss: 0.181807, acc.: 96.88%, validation acc: 44.12%] [G loss: 2.107228]\n",
      "797 [D loss: 0.126157, acc.: 100.00%, validation acc: 43.92%] [G loss: 2.088405]\n",
      "798 [D loss: 0.144205, acc.: 100.00%, validation acc: 43.87%] [G loss: 2.153409]\n",
      "799 [D loss: 0.152016, acc.: 100.00%, validation acc: 44.02%] [G loss: 2.113923]\n",
      "800 [D loss: 0.144369, acc.: 96.88%, validation acc: 44.08%] [G loss: 2.126916]\n",
      "801 [D loss: 0.136670, acc.: 100.00%, validation acc: 44.07%] [G loss: 1.997196]\n",
      "802 [D loss: 0.306986, acc.: 93.75%, validation acc: 44.33%] [G loss: 2.046253]\n",
      "803 [D loss: 0.107633, acc.: 100.00%, validation acc: 44.38%] [G loss: 2.069802]\n",
      "804 [D loss: 0.227188, acc.: 96.88%, validation acc: 44.24%] [G loss: 1.989983]\n",
      "805 [D loss: 0.127816, acc.: 100.00%, validation acc: 44.30%] [G loss: 1.984118]\n",
      "806 [D loss: 0.157000, acc.: 96.88%, validation acc: 44.36%] [G loss: 1.887253]\n",
      "807 [D loss: 0.126275, acc.: 100.00%, validation acc: 44.57%] [G loss: 2.040743]\n",
      "808 [D loss: 0.192252, acc.: 96.88%, validation acc: 44.70%] [G loss: 2.082017]\n",
      "809 [D loss: 0.118606, acc.: 100.00%, validation acc: 44.78%] [G loss: 2.176126]\n",
      "810 [D loss: 0.132475, acc.: 100.00%, validation acc: 44.99%] [G loss: 2.213901]\n",
      "811 [D loss: 0.116184, acc.: 100.00%, validation acc: 45.22%] [G loss: 2.178245]\n",
      "812 [D loss: 0.219071, acc.: 93.75%, validation acc: 44.65%] [G loss: 2.131137]\n",
      "813 [D loss: 0.118212, acc.: 100.00%, validation acc: 44.62%] [G loss: 2.195807]\n",
      "814 [D loss: 0.692454, acc.: 93.75%, validation acc: 44.07%] [G loss: 1.939393]\n",
      "815 [D loss: 0.121071, acc.: 100.00%, validation acc: 44.12%] [G loss: 2.035455]\n",
      "816 [D loss: 0.141861, acc.: 100.00%, validation acc: 44.01%] [G loss: 1.917746]\n",
      "817 [D loss: 0.123912, acc.: 100.00%, validation acc: 44.23%] [G loss: 2.137015]\n",
      "818 [D loss: 0.540308, acc.: 93.75%, validation acc: 43.91%] [G loss: 1.889060]\n",
      "819 [D loss: 0.171880, acc.: 100.00%, validation acc: 44.03%] [G loss: 2.053709]\n",
      "820 [D loss: 0.148646, acc.: 96.88%, validation acc: 44.29%] [G loss: 2.083463]\n",
      "821 [D loss: 0.174288, acc.: 96.88%, validation acc: 44.06%] [G loss: 2.137262]\n",
      "822 [D loss: 0.125199, acc.: 100.00%, validation acc: 44.00%] [G loss: 2.210798]\n",
      "823 [D loss: 0.629558, acc.: 96.88%, validation acc: 44.17%] [G loss: 2.269142]\n",
      "824 [D loss: 0.190773, acc.: 96.88%, validation acc: 44.28%] [G loss: 2.256793]\n",
      "825 [D loss: 0.325933, acc.: 90.62%, validation acc: 43.55%] [G loss: 2.202058]\n",
      "826 [D loss: 0.112021, acc.: 100.00%, validation acc: 43.37%] [G loss: 2.084140]\n",
      "827 [D loss: 0.205080, acc.: 96.88%, validation acc: 43.37%] [G loss: 2.086419]\n",
      "828 [D loss: 0.134236, acc.: 100.00%, validation acc: 43.27%] [G loss: 2.076238]\n",
      "829 [D loss: 0.119487, acc.: 100.00%, validation acc: 43.34%] [G loss: 2.240772]\n",
      "830 [D loss: 0.112302, acc.: 100.00%, validation acc: 43.51%] [G loss: 2.213008]\n",
      "831 [D loss: 0.315355, acc.: 93.75%, validation acc: 43.76%] [G loss: 2.169298]\n",
      "832 [D loss: 0.175347, acc.: 96.88%, validation acc: 44.00%] [G loss: 2.129631]\n",
      "833 [D loss: 0.169110, acc.: 96.88%, validation acc: 44.03%] [G loss: 2.063438]\n",
      "834 [D loss: 0.102328, acc.: 100.00%, validation acc: 44.20%] [G loss: 2.156666]\n",
      "835 [D loss: 0.127950, acc.: 100.00%, validation acc: 44.41%] [G loss: 2.198402]\n",
      "836 [D loss: 0.167298, acc.: 96.88%, validation acc: 44.55%] [G loss: 2.184560]\n",
      "837 [D loss: 0.117534, acc.: 100.00%, validation acc: 44.48%] [G loss: 2.190839]\n",
      "838 [D loss: 0.217608, acc.: 93.75%, validation acc: 43.95%] [G loss: 2.004658]\n",
      "839 [D loss: 0.380990, acc.: 93.75%, validation acc: 43.90%] [G loss: 2.066105]\n",
      "840 [D loss: 0.414900, acc.: 96.88%, validation acc: 43.07%] [G loss: 1.764061]\n",
      "841 [D loss: 0.142941, acc.: 100.00%, validation acc: 43.16%] [G loss: 1.835420]\n",
      "842 [D loss: 0.196633, acc.: 96.88%, validation acc: 42.78%] [G loss: 1.888548]\n",
      "843 [D loss: 0.120797, acc.: 100.00%, validation acc: 42.85%] [G loss: 1.994493]\n",
      "844 [D loss: 0.410055, acc.: 93.75%, validation acc: 42.42%] [G loss: 1.751732]\n",
      "845 [D loss: 0.213744, acc.: 96.88%, validation acc: 42.80%] [G loss: 1.849687]\n",
      "846 [D loss: 0.152335, acc.: 100.00%, validation acc: 43.16%] [G loss: 1.986910]\n",
      "847 [D loss: 0.136928, acc.: 100.00%, validation acc: 43.49%] [G loss: 2.095558]\n",
      "848 [D loss: 0.270160, acc.: 96.88%, validation acc: 43.70%] [G loss: 2.101768]\n",
      "849 [D loss: 0.134413, acc.: 100.00%, validation acc: 43.86%] [G loss: 2.192299]\n",
      "850 [D loss: 0.100622, acc.: 100.00%, validation acc: 44.01%] [G loss: 2.192259]\n",
      "851 [D loss: 0.085245, acc.: 100.00%, validation acc: 44.09%] [G loss: 2.291329]\n",
      "852 [D loss: 0.175062, acc.: 93.75%, validation acc: 43.92%] [G loss: 2.205351]\n",
      "853 [D loss: 0.346028, acc.: 90.62%, validation acc: 43.85%] [G loss: 2.035322]\n",
      "854 [D loss: 0.130993, acc.: 100.00%, validation acc: 44.00%] [G loss: 1.962263]\n",
      "855 [D loss: 0.288220, acc.: 93.75%, validation acc: 43.98%] [G loss: 1.970916]\n",
      "856 [D loss: 0.153967, acc.: 96.88%, validation acc: 43.76%] [G loss: 2.062336]\n",
      "857 [D loss: 0.421755, acc.: 93.75%, validation acc: 43.54%] [G loss: 1.909415]\n",
      "858 [D loss: 0.116312, acc.: 100.00%, validation acc: 43.68%] [G loss: 1.918526]\n",
      "859 [D loss: 0.155277, acc.: 96.88%, validation acc: 43.99%] [G loss: 2.096658]\n",
      "860 [D loss: 0.116145, acc.: 100.00%, validation acc: 44.26%] [G loss: 2.113050]\n",
      "861 [D loss: 0.179980, acc.: 96.88%, validation acc: 44.36%] [G loss: 2.186338]\n",
      "862 [D loss: 0.136166, acc.: 96.88%, validation acc: 44.69%] [G loss: 2.077682]\n",
      "863 [D loss: 0.153060, acc.: 96.88%, validation acc: 44.52%] [G loss: 2.020020]\n",
      "864 [D loss: 0.109957, acc.: 100.00%, validation acc: 44.67%] [G loss: 2.206569]\n",
      "865 [D loss: 0.100442, acc.: 100.00%, validation acc: 44.81%] [G loss: 2.166553]\n",
      "866 [D loss: 0.214954, acc.: 96.88%, validation acc: 44.56%] [G loss: 2.192261]\n",
      "867 [D loss: 0.117436, acc.: 96.88%, validation acc: 44.42%] [G loss: 2.145346]\n",
      "868 [D loss: 0.151834, acc.: 93.75%, validation acc: 44.38%] [G loss: 2.199543]\n",
      "869 [D loss: 0.213858, acc.: 93.75%, validation acc: 44.19%] [G loss: 2.145240]\n",
      "870 [D loss: 0.165695, acc.: 96.88%, validation acc: 44.21%] [G loss: 2.194614]\n",
      "871 [D loss: 0.105682, acc.: 100.00%, validation acc: 44.08%] [G loss: 2.268385]\n",
      "872 [D loss: 0.275732, acc.: 90.62%, validation acc: 43.64%] [G loss: 1.945672]\n",
      "873 [D loss: 0.101785, acc.: 100.00%, validation acc: 43.72%] [G loss: 1.940891]\n",
      "874 [D loss: 0.140157, acc.: 100.00%, validation acc: 43.97%] [G loss: 2.049727]\n",
      "875 [D loss: 0.103911, acc.: 100.00%, validation acc: 44.25%] [G loss: 2.202830]\n",
      "876 [D loss: 0.378419, acc.: 93.75%, validation acc: 44.18%] [G loss: 2.260629]\n",
      "877 [D loss: 0.083131, acc.: 100.00%, validation acc: 44.28%] [G loss: 2.245076]\n",
      "878 [D loss: 0.169933, acc.: 96.88%, validation acc: 43.98%] [G loss: 2.296157]\n",
      "879 [D loss: 0.168976, acc.: 96.88%, validation acc: 43.92%] [G loss: 2.335158]\n",
      "880 [D loss: 0.156841, acc.: 96.88%, validation acc: 43.22%] [G loss: 2.278964]\n",
      "881 [D loss: 0.307515, acc.: 96.88%, validation acc: 42.78%] [G loss: 2.047788]\n",
      "882 [D loss: 0.241701, acc.: 93.75%, validation acc: 42.69%] [G loss: 1.983799]\n",
      "883 [D loss: 0.086479, acc.: 100.00%, validation acc: 42.85%] [G loss: 2.071582]\n",
      "884 [D loss: 0.103630, acc.: 100.00%, validation acc: 43.03%] [G loss: 2.236970]\n",
      "885 [D loss: 0.212229, acc.: 96.88%, validation acc: 43.06%] [G loss: 2.233739]\n",
      "886 [D loss: 0.145987, acc.: 96.88%, validation acc: 43.14%] [G loss: 2.351566]\n",
      "887 [D loss: 0.207744, acc.: 96.88%, validation acc: 43.17%] [G loss: 2.290937]\n",
      "888 [D loss: 0.209048, acc.: 96.88%, validation acc: 43.11%] [G loss: 2.197257]\n",
      "889 [D loss: 0.159024, acc.: 93.75%, validation acc: 42.62%] [G loss: 2.057994]\n",
      "890 [D loss: 0.091066, acc.: 100.00%, validation acc: 42.63%] [G loss: 2.183421]\n",
      "891 [D loss: 0.198871, acc.: 96.88%, validation acc: 42.53%] [G loss: 2.267612]\n",
      "892 [D loss: 0.289837, acc.: 90.62%, validation acc: 40.72%] [G loss: 2.083844]\n",
      "893 [D loss: 0.084860, acc.: 100.00%, validation acc: 40.34%] [G loss: 2.114705]\n",
      "894 [D loss: 0.095205, acc.: 100.00%, validation acc: 40.47%] [G loss: 2.188673]\n",
      "895 [D loss: 0.114379, acc.: 96.88%, validation acc: 40.86%] [G loss: 2.280945]\n",
      "896 [D loss: 0.140864, acc.: 96.88%, validation acc: 41.27%] [G loss: 2.289827]\n",
      "897 [D loss: 0.089338, acc.: 100.00%, validation acc: 41.85%] [G loss: 2.398547]\n",
      "898 [D loss: 0.104931, acc.: 100.00%, validation acc: 42.00%] [G loss: 2.334740]\n",
      "899 [D loss: 0.089500, acc.: 100.00%, validation acc: 42.32%] [G loss: 2.341839]\n",
      "900 [D loss: 0.103571, acc.: 100.00%, validation acc: 42.78%] [G loss: 2.406734]\n",
      "901 [D loss: 0.170537, acc.: 93.75%, validation acc: 42.67%] [G loss: 2.316131]\n",
      "902 [D loss: 0.192810, acc.: 93.75%, validation acc: 42.25%] [G loss: 2.135373]\n",
      "903 [D loss: 0.142482, acc.: 96.88%, validation acc: 41.89%] [G loss: 2.099120]\n",
      "904 [D loss: 0.106427, acc.: 100.00%, validation acc: 42.22%] [G loss: 2.202511]\n",
      "905 [D loss: 0.125449, acc.: 100.00%, validation acc: 42.70%] [G loss: 2.318064]\n",
      "906 [D loss: 0.084639, acc.: 100.00%, validation acc: 42.91%] [G loss: 2.413696]\n",
      "907 [D loss: 0.081131, acc.: 100.00%, validation acc: 43.09%] [G loss: 2.324005]\n",
      "908 [D loss: 0.178949, acc.: 96.88%, validation acc: 43.33%] [G loss: 2.273394]\n",
      "909 [D loss: 0.088338, acc.: 100.00%, validation acc: 43.49%] [G loss: 2.279889]\n",
      "910 [D loss: 0.139509, acc.: 100.00%, validation acc: 43.76%] [G loss: 2.368125]\n",
      "911 [D loss: 0.082312, acc.: 100.00%, validation acc: 43.95%] [G loss: 2.451987]\n",
      "912 [D loss: 0.078650, acc.: 100.00%, validation acc: 44.05%] [G loss: 2.519785]\n",
      "913 [D loss: 0.173514, acc.: 96.88%, validation acc: 43.81%] [G loss: 2.370599]\n",
      "914 [D loss: 0.090811, acc.: 100.00%, validation acc: 43.86%] [G loss: 2.294092]\n",
      "915 [D loss: 0.084386, acc.: 100.00%, validation acc: 43.96%] [G loss: 2.306953]\n",
      "916 [D loss: 0.120744, acc.: 100.00%, validation acc: 43.82%] [G loss: 2.333949]\n",
      "917 [D loss: 0.191245, acc.: 93.75%, validation acc: 43.78%] [G loss: 2.180918]\n",
      "918 [D loss: 0.123294, acc.: 100.00%, validation acc: 43.84%] [G loss: 2.205664]\n",
      "919 [D loss: 0.103214, acc.: 100.00%, validation acc: 44.05%] [G loss: 2.201384]\n",
      "920 [D loss: 0.116451, acc.: 100.00%, validation acc: 44.27%] [G loss: 2.246027]\n",
      "921 [D loss: 0.115668, acc.: 100.00%, validation acc: 44.27%] [G loss: 2.303941]\n",
      "922 [D loss: 0.121350, acc.: 100.00%, validation acc: 44.59%] [G loss: 2.282338]\n",
      "923 [D loss: 0.110944, acc.: 100.00%, validation acc: 44.82%] [G loss: 2.468556]\n",
      "924 [D loss: 0.126835, acc.: 96.88%, validation acc: 44.47%] [G loss: 2.373049]\n",
      "925 [D loss: 0.134466, acc.: 96.88%, validation acc: 44.50%] [G loss: 2.345186]\n",
      "926 [D loss: 0.105979, acc.: 100.00%, validation acc: 44.42%] [G loss: 2.385492]\n",
      "927 [D loss: 0.096066, acc.: 100.00%, validation acc: 44.42%] [G loss: 2.604544]\n",
      "928 [D loss: 0.139325, acc.: 100.00%, validation acc: 43.90%] [G loss: 2.330619]\n",
      "929 [D loss: 0.091489, acc.: 100.00%, validation acc: 43.78%] [G loss: 2.409156]\n",
      "930 [D loss: 0.091385, acc.: 100.00%, validation acc: 43.84%] [G loss: 2.311951]\n",
      "931 [D loss: 0.175483, acc.: 96.88%, validation acc: 43.41%] [G loss: 2.390462]\n",
      "932 [D loss: 0.189062, acc.: 96.88%, validation acc: 42.97%] [G loss: 2.205542]\n",
      "933 [D loss: 0.084533, acc.: 100.00%, validation acc: 43.07%] [G loss: 2.308877]\n",
      "934 [D loss: 0.171016, acc.: 96.88%, validation acc: 42.92%] [G loss: 2.312444]\n",
      "935 [D loss: 0.096471, acc.: 100.00%, validation acc: 42.73%] [G loss: 2.249280]\n",
      "936 [D loss: 0.101478, acc.: 100.00%, validation acc: 42.82%] [G loss: 2.359168]\n",
      "937 [D loss: 0.101992, acc.: 100.00%, validation acc: 42.84%] [G loss: 2.334743]\n",
      "938 [D loss: 0.081291, acc.: 100.00%, validation acc: 42.95%] [G loss: 2.421391]\n",
      "939 [D loss: 0.163923, acc.: 96.88%, validation acc: 43.23%] [G loss: 2.411326]\n",
      "940 [D loss: 0.101638, acc.: 96.88%, validation acc: 43.40%] [G loss: 2.484964]\n",
      "941 [D loss: 0.152985, acc.: 96.88%, validation acc: 43.67%] [G loss: 2.552822]\n",
      "942 [D loss: 0.087525, acc.: 100.00%, validation acc: 43.90%] [G loss: 2.518391]\n",
      "943 [D loss: 0.122294, acc.: 96.88%, validation acc: 43.62%] [G loss: 2.495260]\n",
      "944 [D loss: 0.183425, acc.: 96.88%, validation acc: 43.71%] [G loss: 2.544444]\n",
      "945 [D loss: 0.156530, acc.: 96.88%, validation acc: 44.08%] [G loss: 2.495313]\n",
      "946 [D loss: 0.078795, acc.: 100.00%, validation acc: 44.32%] [G loss: 2.453572]\n",
      "947 [D loss: 0.073841, acc.: 100.00%, validation acc: 44.42%] [G loss: 2.706594]\n",
      "948 [D loss: 0.108773, acc.: 100.00%, validation acc: 44.41%] [G loss: 2.593553]\n",
      "949 [D loss: 0.157787, acc.: 96.88%, validation acc: 44.36%] [G loss: 2.507717]\n",
      "950 [D loss: 0.076351, acc.: 100.00%, validation acc: 44.43%] [G loss: 2.541647]\n",
      "951 [D loss: 0.104720, acc.: 100.00%, validation acc: 44.48%] [G loss: 2.498020]\n",
      "952 [D loss: 0.085994, acc.: 100.00%, validation acc: 44.64%] [G loss: 2.595730]\n",
      "953 [D loss: 0.081257, acc.: 100.00%, validation acc: 44.82%] [G loss: 2.512735]\n",
      "954 [D loss: 0.084438, acc.: 100.00%, validation acc: 44.96%] [G loss: 2.593231]\n",
      "955 [D loss: 0.325610, acc.: 93.75%, validation acc: 45.21%] [G loss: 2.482563]\n",
      "956 [D loss: 0.082623, acc.: 100.00%, validation acc: 45.51%] [G loss: 2.444793]\n",
      "957 [D loss: 0.230625, acc.: 93.75%, validation acc: 45.26%] [G loss: 2.350791]\n",
      "958 [D loss: 0.092150, acc.: 100.00%, validation acc: 45.23%] [G loss: 2.345182]\n",
      "959 [D loss: 0.087079, acc.: 100.00%, validation acc: 45.36%] [G loss: 2.444407]\n",
      "960 [D loss: 0.103696, acc.: 96.88%, validation acc: 45.24%] [G loss: 2.357780]\n",
      "961 [D loss: 0.129075, acc.: 96.88%, validation acc: 45.24%] [G loss: 2.387484]\n",
      "962 [D loss: 0.091735, acc.: 100.00%, validation acc: 45.17%] [G loss: 2.346551]\n",
      "963 [D loss: 0.084553, acc.: 100.00%, validation acc: 45.12%] [G loss: 2.419730]\n",
      "964 [D loss: 0.117191, acc.: 100.00%, validation acc: 45.13%] [G loss: 2.382183]\n",
      "965 [D loss: 0.077763, acc.: 100.00%, validation acc: 45.27%] [G loss: 2.456669]\n",
      "966 [D loss: 0.074912, acc.: 100.00%, validation acc: 45.35%] [G loss: 2.567402]\n",
      "967 [D loss: 0.141642, acc.: 96.88%, validation acc: 45.37%] [G loss: 2.506666]\n",
      "968 [D loss: 0.081950, acc.: 100.00%, validation acc: 45.42%] [G loss: 2.604753]\n",
      "969 [D loss: 0.066485, acc.: 100.00%, validation acc: 45.54%] [G loss: 2.483119]\n",
      "970 [D loss: 0.074379, acc.: 100.00%, validation acc: 45.66%] [G loss: 2.601506]\n",
      "971 [D loss: 0.361691, acc.: 96.88%, validation acc: 45.36%] [G loss: 2.304583]\n",
      "972 [D loss: 0.095696, acc.: 100.00%, validation acc: 45.25%] [G loss: 2.312466]\n",
      "973 [D loss: 0.076833, acc.: 100.00%, validation acc: 45.45%] [G loss: 2.463471]\n",
      "974 [D loss: 0.171562, acc.: 96.88%, validation acc: 45.55%] [G loss: 2.399229]\n",
      "975 [D loss: 0.082868, acc.: 100.00%, validation acc: 45.54%] [G loss: 2.426252]\n",
      "976 [D loss: 0.142632, acc.: 96.88%, validation acc: 45.72%] [G loss: 2.494837]\n",
      "977 [D loss: 0.062515, acc.: 100.00%, validation acc: 45.83%] [G loss: 2.427898]\n",
      "978 [D loss: 0.082147, acc.: 100.00%, validation acc: 45.97%] [G loss: 2.617456]\n",
      "979 [D loss: 0.082768, acc.: 100.00%, validation acc: 46.04%] [G loss: 2.551145]\n",
      "980 [D loss: 0.110707, acc.: 100.00%, validation acc: 45.97%] [G loss: 2.492305]\n",
      "981 [D loss: 0.106623, acc.: 100.00%, validation acc: 46.08%] [G loss: 2.438948]\n",
      "982 [D loss: 0.201077, acc.: 93.75%, validation acc: 45.76%] [G loss: 2.367399]\n",
      "983 [D loss: 0.102096, acc.: 100.00%, validation acc: 45.72%] [G loss: 2.410629]\n",
      "984 [D loss: 0.107216, acc.: 100.00%, validation acc: 45.68%] [G loss: 2.363979]\n",
      "985 [D loss: 0.070559, acc.: 100.00%, validation acc: 45.85%] [G loss: 2.535098]\n",
      "986 [D loss: 0.290425, acc.: 96.88%, validation acc: 45.56%] [G loss: 2.503958]\n",
      "987 [D loss: 0.085772, acc.: 100.00%, validation acc: 45.50%] [G loss: 2.506268]\n",
      "988 [D loss: 0.142519, acc.: 96.88%, validation acc: 44.12%] [G loss: 2.353825]\n",
      "989 [D loss: 0.090484, acc.: 100.00%, validation acc: 43.61%] [G loss: 2.371069]\n",
      "990 [D loss: 0.098502, acc.: 100.00%, validation acc: 43.49%] [G loss: 2.373399]\n",
      "991 [D loss: 0.269847, acc.: 90.62%, validation acc: 43.61%] [G loss: 2.207314]\n",
      "992 [D loss: 0.074325, acc.: 100.00%, validation acc: 43.80%] [G loss: 2.280879]\n",
      "993 [D loss: 0.079239, acc.: 100.00%, validation acc: 43.92%] [G loss: 2.368834]\n",
      "994 [D loss: 0.157084, acc.: 96.88%, validation acc: 44.18%] [G loss: 2.204566]\n",
      "995 [D loss: 0.171802, acc.: 96.88%, validation acc: 43.83%] [G loss: 2.113250]\n",
      "996 [D loss: 0.179616, acc.: 93.75%, validation acc: 43.61%] [G loss: 2.117776]\n",
      "997 [D loss: 0.127276, acc.: 100.00%, validation acc: 43.74%] [G loss: 2.153551]\n",
      "998 [D loss: 0.108996, acc.: 100.00%, validation acc: 43.98%] [G loss: 2.344290]\n",
      "999 [D loss: 0.091570, acc.: 100.00%, validation acc: 44.15%] [G loss: 2.362476]\n"
     ]
    }
   ],
   "source": [
    "my_gan.train(X_train=X_train, epochs=1000, batch_size=32, save_interval=0, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_117 (Dense)            (None, 512)               64512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 196,097\n",
      "Trainable params: 196,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network = build_discriminator()\n",
    "network.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return metrics.roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Maximum, Add\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp():\n",
    "\n",
    "    a = Input(shape=(125,))\n",
    "        \n",
    "    dense1 = Dense(512, activation='relu')(a)\n",
    "    dropout1 = Dropout(0.5)(dense1)\n",
    "    dense2 = Dense(256, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense2)\n",
    "    b = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "    return Model(inputs=a, outputs=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2536 samples, validate on 282 samples\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input39483\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n",
      "G:\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input39549\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.6480 - acc: 0.6534 - val_loss: 1.3055 - val_acc: 0.2660\n",
      "Epoch 2/500\n",
      "0s - loss: 0.5932 - acc: 0.7086 - val_loss: 0.5408 - val_acc: 0.7624\n",
      "Epoch 3/500\n",
      "0s - loss: 0.5767 - acc: 0.7188 - val_loss: 0.5392 - val_acc: 0.7518\n",
      "Epoch 4/500\n",
      "0s - loss: 0.5509 - acc: 0.7390 - val_loss: 0.8025 - val_acc: 0.5426\n",
      "Epoch 5/500\n",
      "0s - loss: 0.5458 - acc: 0.7358 - val_loss: 0.6878 - val_acc: 0.6135\n",
      "Epoch 6/500\n",
      "0s - loss: 0.5378 - acc: 0.7338 - val_loss: 0.5956 - val_acc: 0.6596\n",
      "Epoch 7/500\n",
      "0s - loss: 0.5328 - acc: 0.7342 - val_loss: 0.7258 - val_acc: 0.5709\n",
      "Epoch 8/500\n",
      "0s - loss: 0.5336 - acc: 0.7366 - val_loss: 0.6161 - val_acc: 0.6631\n",
      "Epoch 9/500\n",
      "0s - loss: 0.5273 - acc: 0.7551 - val_loss: 0.6588 - val_acc: 0.5745\n",
      "Epoch 10/500\n",
      "0s - loss: 0.5207 - acc: 0.7520 - val_loss: 0.7579 - val_acc: 0.5567\n",
      "Epoch 11/500\n",
      "0s - loss: 0.5126 - acc: 0.7606 - val_loss: 0.7415 - val_acc: 0.5496\n",
      "Epoch 12/500\n",
      "0s - loss: 0.5097 - acc: 0.7595 - val_loss: 0.6255 - val_acc: 0.6418\n",
      "Epoch 13/500\n",
      "0s - loss: 0.5158 - acc: 0.7528 - val_loss: 0.6465 - val_acc: 0.6135\n",
      "Epoch 14/500\n",
      "0s - loss: 0.5091 - acc: 0.7559 - val_loss: 0.6690 - val_acc: 0.6312\n",
      "Epoch 15/500\n",
      "0s - loss: 0.5167 - acc: 0.7488 - val_loss: 0.6310 - val_acc: 0.6135\n",
      "Epoch 16/500\n",
      "0s - loss: 0.5102 - acc: 0.7547 - val_loss: 0.6486 - val_acc: 0.6135\n",
      "Epoch 17/500\n",
      "0s - loss: 0.5021 - acc: 0.7528 - val_loss: 0.6624 - val_acc: 0.6099\n",
      "Epoch 18/500\n",
      "0s - loss: 0.5018 - acc: 0.7595 - val_loss: 0.6789 - val_acc: 0.5887\n",
      "Epoch 19/500\n",
      "0s - loss: 0.4986 - acc: 0.7591 - val_loss: 0.5930 - val_acc: 0.6596\n",
      "Epoch 20/500\n",
      "0s - loss: 0.4984 - acc: 0.7595 - val_loss: 0.6738 - val_acc: 0.5922\n",
      "Epoch 21/500\n",
      "0s - loss: 0.5018 - acc: 0.7587 - val_loss: 0.6531 - val_acc: 0.6348\n",
      "Epoch 22/500\n",
      "0s - loss: 0.5014 - acc: 0.7559 - val_loss: 0.7237 - val_acc: 0.5355\n",
      "Epoch 23/500\n",
      "0s - loss: 0.4966 - acc: 0.7610 - val_loss: 0.7107 - val_acc: 0.5603\n",
      "Epoch 24/500\n",
      "0s - loss: 0.5017 - acc: 0.7575 - val_loss: 0.6558 - val_acc: 0.5638\n",
      "Epoch 25/500\n",
      "0s - loss: 0.4880 - acc: 0.7717 - val_loss: 0.7675 - val_acc: 0.5426\n",
      "Epoch 26/500\n",
      "0s - loss: 0.4910 - acc: 0.7670 - val_loss: 0.7700 - val_acc: 0.5319\n",
      "Epoch 27/500\n",
      "0s - loss: 0.4916 - acc: 0.7689 - val_loss: 0.7135 - val_acc: 0.5496\n",
      "Epoch 28/500\n",
      "0s - loss: 0.4815 - acc: 0.7729 - val_loss: 0.7470 - val_acc: 0.5000\n",
      "Epoch 29/500\n",
      "0s - loss: 0.4924 - acc: 0.7626 - val_loss: 0.6368 - val_acc: 0.6028\n",
      "Epoch 30/500\n",
      "0s - loss: 0.4914 - acc: 0.7606 - val_loss: 0.5977 - val_acc: 0.6383\n",
      "Epoch 31/500\n",
      "0s - loss: 0.4915 - acc: 0.7603 - val_loss: 0.7493 - val_acc: 0.5142\n",
      "Epoch 32/500\n",
      "0s - loss: 0.4899 - acc: 0.7662 - val_loss: 0.7400 - val_acc: 0.5390\n",
      "Epoch 33/500\n",
      "0s - loss: 0.4893 - acc: 0.7630 - val_loss: 0.5874 - val_acc: 0.6064\n",
      "Epoch 34/500\n",
      "0s - loss: 0.4868 - acc: 0.7638 - val_loss: 0.6517 - val_acc: 0.5851\n",
      "Epoch 35/500\n",
      "0s - loss: 0.4853 - acc: 0.7677 - val_loss: 0.7123 - val_acc: 0.5532\n",
      "Epoch 36/500\n",
      "0s - loss: 0.4860 - acc: 0.7646 - val_loss: 0.6623 - val_acc: 0.5284\n",
      "Epoch 37/500\n",
      "0s - loss: 0.4780 - acc: 0.7677 - val_loss: 0.6591 - val_acc: 0.5567\n",
      "Epoch 38/500\n",
      "0s - loss: 0.4870 - acc: 0.7587 - val_loss: 0.6654 - val_acc: 0.5816\n",
      "Epoch 39/500\n",
      "0s - loss: 0.4838 - acc: 0.7697 - val_loss: 0.7302 - val_acc: 0.5248\n",
      "Epoch 40/500\n",
      "0s - loss: 0.4766 - acc: 0.7670 - val_loss: 0.6629 - val_acc: 0.5851\n",
      "Epoch 41/500\n",
      "0s - loss: 0.4772 - acc: 0.7709 - val_loss: 0.7234 - val_acc: 0.4929\n",
      "Epoch 42/500\n",
      "0s - loss: 0.4840 - acc: 0.7638 - val_loss: 0.6802 - val_acc: 0.5603\n",
      "Epoch 43/500\n",
      "0s - loss: 0.4697 - acc: 0.7744 - val_loss: 0.6502 - val_acc: 0.5993\n",
      "Epoch 44/500\n",
      "0s - loss: 0.4818 - acc: 0.7681 - val_loss: 0.7103 - val_acc: 0.5213\n",
      "Epoch 45/500\n",
      "0s - loss: 0.4773 - acc: 0.7709 - val_loss: 0.6974 - val_acc: 0.5496\n",
      "Epoch 46/500\n",
      "0s - loss: 0.4776 - acc: 0.7697 - val_loss: 0.6707 - val_acc: 0.5851\n",
      "Epoch 47/500\n",
      "0s - loss: 0.4777 - acc: 0.7650 - val_loss: 0.6913 - val_acc: 0.5426\n",
      "Epoch 48/500\n",
      "0s - loss: 0.4771 - acc: 0.7697 - val_loss: 0.6536 - val_acc: 0.5603\n",
      "Epoch 49/500\n",
      "0s - loss: 0.4715 - acc: 0.7764 - val_loss: 0.6621 - val_acc: 0.5780\n",
      "Epoch 50/500\n",
      "0s - loss: 0.4740 - acc: 0.7784 - val_loss: 0.6600 - val_acc: 0.5993\n",
      "Epoch 51/500\n",
      "0s - loss: 0.4771 - acc: 0.7756 - val_loss: 0.6393 - val_acc: 0.5851\n",
      "Epoch 52/500\n",
      "0s - loss: 0.4681 - acc: 0.7721 - val_loss: 0.6148 - val_acc: 0.6135\n",
      "Epoch 53/500\n",
      "0s - loss: 0.4785 - acc: 0.7721 - val_loss: 0.6690 - val_acc: 0.5816\n",
      "Epoch 54/500\n",
      "0s - loss: 0.4668 - acc: 0.7737 - val_loss: 0.6391 - val_acc: 0.5993\n",
      "Epoch 55/500\n",
      "0s - loss: 0.4682 - acc: 0.7729 - val_loss: 0.6305 - val_acc: 0.5780\n",
      "Epoch 56/500\n",
      "0s - loss: 0.4675 - acc: 0.7741 - val_loss: 0.7222 - val_acc: 0.5319\n",
      "Epoch 57/500\n",
      "0s - loss: 0.4661 - acc: 0.7709 - val_loss: 0.6003 - val_acc: 0.6206\n",
      "Epoch 58/500\n",
      "0s - loss: 0.4622 - acc: 0.7800 - val_loss: 0.7557 - val_acc: 0.5035\n",
      "Epoch 59/500\n",
      "0s - loss: 0.4643 - acc: 0.7800 - val_loss: 0.6538 - val_acc: 0.5567\n",
      "Epoch 60/500\n",
      "0s - loss: 0.4665 - acc: 0.7733 - val_loss: 0.6568 - val_acc: 0.5567\n",
      "Epoch 61/500\n",
      "0s - loss: 0.4601 - acc: 0.7859 - val_loss: 0.6728 - val_acc: 0.5532\n",
      "Epoch 62/500\n",
      "0s - loss: 0.4669 - acc: 0.7752 - val_loss: 0.7152 - val_acc: 0.5319\n",
      "Epoch 63/500\n",
      "0s - loss: 0.4608 - acc: 0.7859 - val_loss: 0.6857 - val_acc: 0.5709\n",
      "Epoch 64/500\n",
      "0s - loss: 0.4671 - acc: 0.7713 - val_loss: 0.6415 - val_acc: 0.5993\n",
      "Epoch 65/500\n",
      "0s - loss: 0.4542 - acc: 0.7792 - val_loss: 0.6578 - val_acc: 0.5851\n",
      "Epoch 66/500\n",
      "0s - loss: 0.4624 - acc: 0.7760 - val_loss: 0.6618 - val_acc: 0.5780\n",
      "Epoch 67/500\n",
      "0s - loss: 0.4596 - acc: 0.7784 - val_loss: 0.7171 - val_acc: 0.5603\n",
      "Epoch 68/500\n",
      "0s - loss: 0.4594 - acc: 0.7705 - val_loss: 0.6267 - val_acc: 0.5780\n",
      "Epoch 69/500\n",
      "0s - loss: 0.4591 - acc: 0.7804 - val_loss: 0.7089 - val_acc: 0.5745\n",
      "Epoch 70/500\n",
      "0s - loss: 0.4580 - acc: 0.7776 - val_loss: 0.7045 - val_acc: 0.5567\n",
      "Epoch 71/500\n",
      "0s - loss: 0.4587 - acc: 0.7752 - val_loss: 0.6845 - val_acc: 0.5532\n",
      "Epoch 72/500\n",
      "0s - loss: 0.4572 - acc: 0.7776 - val_loss: 0.7317 - val_acc: 0.5177\n",
      "Epoch 73/500\n",
      "0s - loss: 0.4558 - acc: 0.7875 - val_loss: 0.6550 - val_acc: 0.5993\n",
      "Epoch 74/500\n",
      "0s - loss: 0.4525 - acc: 0.7890 - val_loss: 0.7405 - val_acc: 0.5213\n",
      "Epoch 75/500\n",
      "0s - loss: 0.4540 - acc: 0.7839 - val_loss: 0.7086 - val_acc: 0.5213\n",
      "Epoch 76/500\n",
      "0s - loss: 0.4534 - acc: 0.7835 - val_loss: 0.7627 - val_acc: 0.5000\n",
      "Epoch 77/500\n",
      "0s - loss: 0.4570 - acc: 0.7831 - val_loss: 0.7221 - val_acc: 0.5780\n",
      "Epoch 78/500\n",
      "0s - loss: 0.4555 - acc: 0.7839 - val_loss: 0.6708 - val_acc: 0.5780\n",
      "Epoch 79/500\n",
      "0s - loss: 0.4532 - acc: 0.7827 - val_loss: 0.7539 - val_acc: 0.4752\n",
      "Epoch 80/500\n",
      "0s - loss: 0.4496 - acc: 0.7764 - val_loss: 0.6827 - val_acc: 0.5603\n",
      "Epoch 81/500\n",
      "0s - loss: 0.4600 - acc: 0.7886 - val_loss: 0.7007 - val_acc: 0.5603\n",
      "Epoch 82/500\n",
      "0s - loss: 0.4543 - acc: 0.7839 - val_loss: 0.7080 - val_acc: 0.5426\n",
      "Epoch 83/500\n",
      "0s - loss: 0.4488 - acc: 0.7926 - val_loss: 0.7545 - val_acc: 0.5496\n",
      "Epoch 84/500\n",
      "0s - loss: 0.4565 - acc: 0.7867 - val_loss: 0.7415 - val_acc: 0.5177\n",
      "Epoch 85/500\n",
      "0s - loss: 0.4505 - acc: 0.7882 - val_loss: 0.6612 - val_acc: 0.5745\n",
      "Epoch 86/500\n",
      "0s - loss: 0.4441 - acc: 0.7871 - val_loss: 0.7504 - val_acc: 0.5106\n",
      "Epoch 87/500\n",
      "0s - loss: 0.4545 - acc: 0.7808 - val_loss: 0.6710 - val_acc: 0.5709\n",
      "Epoch 88/500\n",
      "0s - loss: 0.4460 - acc: 0.7875 - val_loss: 0.6851 - val_acc: 0.5603\n",
      "Epoch 89/500\n",
      "0s - loss: 0.4389 - acc: 0.7946 - val_loss: 0.6637 - val_acc: 0.5816\n",
      "Epoch 90/500\n",
      "0s - loss: 0.4518 - acc: 0.7875 - val_loss: 0.6762 - val_acc: 0.5674\n",
      "Epoch 91/500\n",
      "0s - loss: 0.4496 - acc: 0.7918 - val_loss: 0.6747 - val_acc: 0.5780\n",
      "Epoch 92/500\n",
      "0s - loss: 0.4523 - acc: 0.7835 - val_loss: 0.6920 - val_acc: 0.5603\n",
      "Epoch 93/500\n",
      "0s - loss: 0.4389 - acc: 0.7910 - val_loss: 0.7261 - val_acc: 0.5248\n",
      "Epoch 94/500\n",
      "0s - loss: 0.4417 - acc: 0.7875 - val_loss: 0.7139 - val_acc: 0.5532\n",
      "Epoch 95/500\n",
      "0s - loss: 0.4417 - acc: 0.7894 - val_loss: 0.6766 - val_acc: 0.5567\n",
      "Epoch 96/500\n",
      "0s - loss: 0.4476 - acc: 0.7843 - val_loss: 0.7268 - val_acc: 0.5319\n",
      "Epoch 97/500\n",
      "0s - loss: 0.4463 - acc: 0.7748 - val_loss: 0.6601 - val_acc: 0.5780\n",
      "Epoch 98/500\n",
      "0s - loss: 0.4510 - acc: 0.7910 - val_loss: 0.7225 - val_acc: 0.5355\n",
      "Epoch 99/500\n",
      "0s - loss: 0.4422 - acc: 0.7894 - val_loss: 0.6345 - val_acc: 0.5745\n",
      "Epoch 100/500\n",
      "0s - loss: 0.4438 - acc: 0.7906 - val_loss: 0.6705 - val_acc: 0.5745\n",
      "Epoch 101/500\n",
      "0s - loss: 0.4382 - acc: 0.7946 - val_loss: 0.6987 - val_acc: 0.5603\n",
      "Epoch 102/500\n",
      "0s - loss: 0.4369 - acc: 0.7930 - val_loss: 0.7147 - val_acc: 0.5638\n",
      "Epoch 103/500\n",
      "0s - loss: 0.4435 - acc: 0.7839 - val_loss: 0.7278 - val_acc: 0.5426\n",
      "Epoch 104/500\n",
      "0s - loss: 0.4431 - acc: 0.7835 - val_loss: 0.7012 - val_acc: 0.5426\n",
      "Epoch 105/500\n",
      "0s - loss: 0.4429 - acc: 0.7882 - val_loss: 0.6671 - val_acc: 0.5745\n",
      "Epoch 106/500\n",
      "0s - loss: 0.4325 - acc: 0.7953 - val_loss: 0.7104 - val_acc: 0.5532\n",
      "Epoch 107/500\n",
      "0s - loss: 0.4396 - acc: 0.7879 - val_loss: 0.7253 - val_acc: 0.5284\n",
      "Epoch 108/500\n",
      "0s - loss: 0.4312 - acc: 0.7950 - val_loss: 0.7395 - val_acc: 0.5496\n",
      "Epoch 109/500\n",
      "0s - loss: 0.4353 - acc: 0.7863 - val_loss: 0.6772 - val_acc: 0.5532\n",
      "Epoch 110/500\n",
      "0s - loss: 0.4362 - acc: 0.7922 - val_loss: 0.6806 - val_acc: 0.5496\n",
      "Epoch 111/500\n",
      "0s - loss: 0.4406 - acc: 0.7930 - val_loss: 0.6533 - val_acc: 0.5709\n",
      "Epoch 112/500\n",
      "0s - loss: 0.4394 - acc: 0.7898 - val_loss: 0.6875 - val_acc: 0.5638\n",
      "Epoch 113/500\n",
      "0s - loss: 0.4353 - acc: 0.7906 - val_loss: 0.7644 - val_acc: 0.5248\n",
      "Epoch 114/500\n",
      "0s - loss: 0.4349 - acc: 0.7969 - val_loss: 0.7166 - val_acc: 0.5567\n",
      "Epoch 115/500\n",
      "0s - loss: 0.4334 - acc: 0.7914 - val_loss: 0.6598 - val_acc: 0.5709\n",
      "Epoch 116/500\n",
      "0s - loss: 0.4369 - acc: 0.7946 - val_loss: 0.7156 - val_acc: 0.5567\n",
      "Epoch 117/500\n",
      "0s - loss: 0.4309 - acc: 0.7930 - val_loss: 0.7211 - val_acc: 0.5496\n",
      "Epoch 118/500\n",
      "0s - loss: 0.4304 - acc: 0.7993 - val_loss: 0.7214 - val_acc: 0.5355\n",
      "Epoch 119/500\n",
      "0s - loss: 0.4221 - acc: 0.7961 - val_loss: 0.7198 - val_acc: 0.5532\n",
      "Epoch 120/500\n",
      "0s - loss: 0.4313 - acc: 0.7875 - val_loss: 0.7521 - val_acc: 0.5106\n",
      "Epoch 121/500\n",
      "0s - loss: 0.4325 - acc: 0.7969 - val_loss: 0.6912 - val_acc: 0.5709\n",
      "Epoch 122/500\n",
      "0s - loss: 0.4344 - acc: 0.7902 - val_loss: 0.7565 - val_acc: 0.5319\n",
      "Epoch 123/500\n",
      "0s - loss: 0.4334 - acc: 0.7969 - val_loss: 0.7172 - val_acc: 0.5177\n",
      "Epoch 124/500\n",
      "0s - loss: 0.4291 - acc: 0.8032 - val_loss: 0.6783 - val_acc: 0.5674\n",
      "Epoch 125/500\n",
      "0s - loss: 0.4414 - acc: 0.7926 - val_loss: 0.7065 - val_acc: 0.5496\n",
      "Epoch 126/500\n",
      "0s - loss: 0.4295 - acc: 0.7993 - val_loss: 0.6515 - val_acc: 0.5638\n",
      "Epoch 127/500\n",
      "0s - loss: 0.4270 - acc: 0.7946 - val_loss: 0.7513 - val_acc: 0.5248\n",
      "Epoch 128/500\n",
      "0s - loss: 0.4214 - acc: 0.8009 - val_loss: 0.7167 - val_acc: 0.5355\n",
      "Epoch 129/500\n",
      "0s - loss: 0.4272 - acc: 0.7993 - val_loss: 0.7154 - val_acc: 0.5355\n",
      "Epoch 130/500\n",
      "0s - loss: 0.4323 - acc: 0.7938 - val_loss: 0.7236 - val_acc: 0.5426\n",
      "Epoch 131/500\n",
      "0s - loss: 0.4234 - acc: 0.7926 - val_loss: 0.6509 - val_acc: 0.5709\n",
      "Epoch 132/500\n",
      "0s - loss: 0.4295 - acc: 0.7985 - val_loss: 0.7381 - val_acc: 0.5532\n",
      "Epoch 133/500\n",
      "0s - loss: 0.4248 - acc: 0.7973 - val_loss: 0.8001 - val_acc: 0.5248\n",
      "Epoch 134/500\n",
      "0s - loss: 0.4331 - acc: 0.8005 - val_loss: 0.7204 - val_acc: 0.5461\n",
      "Epoch 135/500\n",
      "0s - loss: 0.4286 - acc: 0.7950 - val_loss: 0.7009 - val_acc: 0.5745\n",
      "Epoch 136/500\n",
      "0s - loss: 0.4190 - acc: 0.8017 - val_loss: 0.7244 - val_acc: 0.5603\n",
      "Epoch 137/500\n",
      "0s - loss: 0.4200 - acc: 0.8001 - val_loss: 0.6528 - val_acc: 0.5993\n",
      "Epoch 138/500\n",
      "0s - loss: 0.4243 - acc: 0.7902 - val_loss: 0.6752 - val_acc: 0.5957\n",
      "Epoch 139/500\n",
      "0s - loss: 0.4248 - acc: 0.7950 - val_loss: 0.7069 - val_acc: 0.5816\n",
      "Epoch 140/500\n",
      "0s - loss: 0.4289 - acc: 0.7965 - val_loss: 0.6859 - val_acc: 0.6028\n",
      "Epoch 141/500\n",
      "0s - loss: 0.4289 - acc: 0.7961 - val_loss: 0.7036 - val_acc: 0.5816\n",
      "Epoch 142/500\n",
      "0s - loss: 0.4121 - acc: 0.8060 - val_loss: 0.6770 - val_acc: 0.5780\n",
      "Epoch 143/500\n",
      "0s - loss: 0.4198 - acc: 0.8024 - val_loss: 0.7100 - val_acc: 0.5745\n",
      "Epoch 144/500\n",
      "0s - loss: 0.4241 - acc: 0.8028 - val_loss: 0.6893 - val_acc: 0.5745\n",
      "Epoch 145/500\n",
      "0s - loss: 0.4185 - acc: 0.7961 - val_loss: 0.6882 - val_acc: 0.5532\n",
      "Epoch 146/500\n",
      "0s - loss: 0.4184 - acc: 0.7989 - val_loss: 0.7612 - val_acc: 0.5177\n",
      "Epoch 147/500\n",
      "0s - loss: 0.4254 - acc: 0.7961 - val_loss: 0.6997 - val_acc: 0.5532\n",
      "Epoch 148/500\n",
      "0s - loss: 0.4203 - acc: 0.7993 - val_loss: 0.7019 - val_acc: 0.5496\n",
      "Epoch 149/500\n",
      "0s - loss: 0.4199 - acc: 0.7950 - val_loss: 0.7546 - val_acc: 0.5106\n",
      "Epoch 150/500\n",
      "0s - loss: 0.4123 - acc: 0.8009 - val_loss: 0.7352 - val_acc: 0.5319\n",
      "Epoch 151/500\n",
      "0s - loss: 0.4178 - acc: 0.8028 - val_loss: 0.7674 - val_acc: 0.5142\n",
      "Epoch 152/500\n",
      "0s - loss: 0.4121 - acc: 0.8068 - val_loss: 0.7623 - val_acc: 0.5567\n",
      "Epoch 153/500\n",
      "0s - loss: 0.4216 - acc: 0.8013 - val_loss: 0.8076 - val_acc: 0.5142\n",
      "Epoch 154/500\n",
      "0s - loss: 0.4164 - acc: 0.8024 - val_loss: 0.7328 - val_acc: 0.5638\n",
      "Epoch 155/500\n",
      "0s - loss: 0.4143 - acc: 0.8056 - val_loss: 0.8215 - val_acc: 0.5177\n",
      "Epoch 156/500\n",
      "0s - loss: 0.4166 - acc: 0.7969 - val_loss: 0.7013 - val_acc: 0.5816\n",
      "Epoch 157/500\n",
      "0s - loss: 0.4135 - acc: 0.8044 - val_loss: 0.7187 - val_acc: 0.5780\n",
      "Epoch 158/500\n",
      "0s - loss: 0.4181 - acc: 0.8017 - val_loss: 0.7938 - val_acc: 0.5319\n",
      "Epoch 159/500\n",
      "0s - loss: 0.4118 - acc: 0.8060 - val_loss: 0.7245 - val_acc: 0.5851\n",
      "Epoch 160/500\n",
      "0s - loss: 0.4225 - acc: 0.8005 - val_loss: 0.7003 - val_acc: 0.5851\n",
      "Epoch 161/500\n",
      "0s - loss: 0.4110 - acc: 0.7993 - val_loss: 0.7218 - val_acc: 0.5922\n",
      "Epoch 162/500\n",
      "0s - loss: 0.4046 - acc: 0.8036 - val_loss: 0.7109 - val_acc: 0.5922\n",
      "Epoch 163/500\n",
      "0s - loss: 0.4000 - acc: 0.8155 - val_loss: 0.7418 - val_acc: 0.5532\n",
      "Epoch 164/500\n",
      "0s - loss: 0.4122 - acc: 0.8064 - val_loss: 0.7647 - val_acc: 0.5603\n",
      "Epoch 165/500\n",
      "0s - loss: 0.4197 - acc: 0.7997 - val_loss: 0.8157 - val_acc: 0.5426\n",
      "Epoch 166/500\n",
      "0s - loss: 0.4150 - acc: 0.8024 - val_loss: 0.7439 - val_acc: 0.5496\n",
      "Epoch 167/500\n",
      "0s - loss: 0.4119 - acc: 0.8143 - val_loss: 0.7808 - val_acc: 0.5142\n",
      "Epoch 168/500\n",
      "0s - loss: 0.4186 - acc: 0.7969 - val_loss: 0.8100 - val_acc: 0.5319\n",
      "Epoch 169/500\n",
      "0s - loss: 0.4160 - acc: 0.7985 - val_loss: 0.6925 - val_acc: 0.5567\n",
      "Epoch 170/500\n",
      "0s - loss: 0.4043 - acc: 0.8088 - val_loss: 0.7329 - val_acc: 0.5851\n",
      "Epoch 171/500\n",
      "0s - loss: 0.4142 - acc: 0.8084 - val_loss: 0.7588 - val_acc: 0.5638\n",
      "Epoch 172/500\n",
      "0s - loss: 0.4128 - acc: 0.8044 - val_loss: 0.7918 - val_acc: 0.5567\n",
      "Epoch 173/500\n",
      "0s - loss: 0.4094 - acc: 0.8080 - val_loss: 0.8127 - val_acc: 0.5426\n",
      "Epoch 174/500\n",
      "0s - loss: 0.4040 - acc: 0.8111 - val_loss: 0.7404 - val_acc: 0.5780\n",
      "Epoch 175/500\n",
      "0s - loss: 0.4135 - acc: 0.7965 - val_loss: 0.7613 - val_acc: 0.5461\n",
      "Epoch 176/500\n",
      "0s - loss: 0.4123 - acc: 0.8068 - val_loss: 0.7645 - val_acc: 0.5461\n",
      "Epoch 177/500\n",
      "0s - loss: 0.4045 - acc: 0.8017 - val_loss: 0.8593 - val_acc: 0.5177\n",
      "Epoch 178/500\n",
      "0s - loss: 0.4124 - acc: 0.8064 - val_loss: 0.7853 - val_acc: 0.5638\n",
      "Epoch 179/500\n",
      "0s - loss: 0.4118 - acc: 0.8095 - val_loss: 0.7347 - val_acc: 0.5957\n",
      "Epoch 180/500\n",
      "0s - loss: 0.4150 - acc: 0.8005 - val_loss: 0.7209 - val_acc: 0.5887\n",
      "Epoch 181/500\n",
      "0s - loss: 0.4098 - acc: 0.8091 - val_loss: 0.8297 - val_acc: 0.5532\n",
      "Epoch 182/500\n",
      "0s - loss: 0.4033 - acc: 0.8052 - val_loss: 0.7703 - val_acc: 0.5709\n",
      "Epoch 183/500\n",
      "0s - loss: 0.4044 - acc: 0.8107 - val_loss: 0.7617 - val_acc: 0.5638\n",
      "Epoch 184/500\n",
      "0s - loss: 0.4069 - acc: 0.8024 - val_loss: 0.7923 - val_acc: 0.5461\n",
      "Epoch 185/500\n",
      "0s - loss: 0.4007 - acc: 0.8095 - val_loss: 0.7301 - val_acc: 0.5887\n",
      "Epoch 186/500\n",
      "0s - loss: 0.4073 - acc: 0.8127 - val_loss: 0.7376 - val_acc: 0.5603\n",
      "Epoch 187/500\n",
      "0s - loss: 0.4002 - acc: 0.8107 - val_loss: 0.7696 - val_acc: 0.5390\n",
      "Epoch 188/500\n",
      "0s - loss: 0.4056 - acc: 0.8123 - val_loss: 0.7709 - val_acc: 0.5674\n",
      "Epoch 189/500\n",
      "0s - loss: 0.4116 - acc: 0.8060 - val_loss: 0.7624 - val_acc: 0.5426\n",
      "Epoch 190/500\n",
      "0s - loss: 0.3963 - acc: 0.8111 - val_loss: 0.7785 - val_acc: 0.5461\n",
      "Epoch 191/500\n",
      "0s - loss: 0.4008 - acc: 0.8107 - val_loss: 0.7279 - val_acc: 0.5638\n",
      "Epoch 192/500\n",
      "0s - loss: 0.3986 - acc: 0.8072 - val_loss: 0.8219 - val_acc: 0.5355\n",
      "Epoch 193/500\n",
      "0s - loss: 0.4048 - acc: 0.8024 - val_loss: 0.7486 - val_acc: 0.5674\n",
      "Epoch 194/500\n",
      "0s - loss: 0.4005 - acc: 0.8119 - val_loss: 0.7325 - val_acc: 0.5638\n",
      "Epoch 195/500\n",
      "0s - loss: 0.3977 - acc: 0.8115 - val_loss: 0.8282 - val_acc: 0.5426\n",
      "Epoch 196/500\n",
      "0s - loss: 0.4011 - acc: 0.8076 - val_loss: 0.7674 - val_acc: 0.5851\n",
      "Epoch 197/500\n",
      "0s - loss: 0.4086 - acc: 0.8095 - val_loss: 0.5991 - val_acc: 0.6809\n",
      "Epoch 198/500\n",
      "0s - loss: 0.4638 - acc: 0.8056 - val_loss: 0.7885 - val_acc: 0.5496\n",
      "Epoch 199/500\n",
      "0s - loss: 0.4026 - acc: 0.8095 - val_loss: 0.8480 - val_acc: 0.5426\n",
      "Epoch 200/500\n",
      "0s - loss: 0.4005 - acc: 0.8107 - val_loss: 0.7635 - val_acc: 0.5603\n",
      "Epoch 201/500\n",
      "0s - loss: 0.4134 - acc: 0.8072 - val_loss: 0.7960 - val_acc: 0.5426\n",
      "Epoch 202/500\n",
      "0s - loss: 0.4170 - acc: 0.8032 - val_loss: 0.7191 - val_acc: 0.5780\n",
      "Epoch 203/500\n",
      "0s - loss: 0.4035 - acc: 0.8056 - val_loss: 0.7285 - val_acc: 0.5922\n",
      "Epoch 204/500\n",
      "0s - loss: 0.4044 - acc: 0.8099 - val_loss: 0.7209 - val_acc: 0.5816\n",
      "Epoch 205/500\n",
      "0s - loss: 0.4045 - acc: 0.8103 - val_loss: 0.7703 - val_acc: 0.5603\n",
      "Epoch 206/500\n",
      "0s - loss: 0.4037 - acc: 0.8107 - val_loss: 0.7421 - val_acc: 0.5887\n",
      "Epoch 207/500\n",
      "0s - loss: 0.4206 - acc: 0.8084 - val_loss: 0.7150 - val_acc: 0.5887\n",
      "Epoch 208/500\n",
      "0s - loss: 0.3998 - acc: 0.8084 - val_loss: 0.7856 - val_acc: 0.5532\n",
      "Epoch 209/500\n",
      "0s - loss: 0.4051 - acc: 0.8040 - val_loss: 0.7761 - val_acc: 0.5496\n",
      "Epoch 210/500\n",
      "0s - loss: 0.3925 - acc: 0.8084 - val_loss: 0.7882 - val_acc: 0.5532\n",
      "Epoch 211/500\n",
      "0s - loss: 0.3919 - acc: 0.8162 - val_loss: 0.7317 - val_acc: 0.5922\n",
      "Epoch 212/500\n",
      "0s - loss: 0.3930 - acc: 0.8198 - val_loss: 0.7061 - val_acc: 0.6099\n",
      "Epoch 213/500\n",
      "0s - loss: 0.3996 - acc: 0.8095 - val_loss: 0.7846 - val_acc: 0.5603\n",
      "Epoch 214/500\n",
      "0s - loss: 0.3975 - acc: 0.8119 - val_loss: 0.7596 - val_acc: 0.5851\n",
      "Epoch 215/500\n",
      "0s - loss: 0.4047 - acc: 0.8068 - val_loss: 0.8196 - val_acc: 0.5355\n",
      "Epoch 216/500\n",
      "0s - loss: 0.3959 - acc: 0.8099 - val_loss: 0.7721 - val_acc: 0.5355\n",
      "Epoch 217/500\n",
      "0s - loss: 0.3939 - acc: 0.8170 - val_loss: 0.7557 - val_acc: 0.5496\n",
      "Epoch 218/500\n",
      "0s - loss: 0.3883 - acc: 0.8162 - val_loss: 0.7579 - val_acc: 0.5922\n",
      "Epoch 219/500\n",
      "0s - loss: 0.3875 - acc: 0.8151 - val_loss: 0.8596 - val_acc: 0.5603\n",
      "Epoch 220/500\n",
      "0s - loss: 0.3906 - acc: 0.8115 - val_loss: 0.8019 - val_acc: 0.5709\n",
      "Epoch 221/500\n",
      "0s - loss: 0.3898 - acc: 0.8206 - val_loss: 0.8616 - val_acc: 0.5284\n",
      "Epoch 222/500\n",
      "0s - loss: 0.3988 - acc: 0.8115 - val_loss: 0.8255 - val_acc: 0.5390\n",
      "Epoch 223/500\n",
      "0s - loss: 0.3936 - acc: 0.8084 - val_loss: 0.8579 - val_acc: 0.5177\n",
      "Epoch 224/500\n",
      "0s - loss: 0.3983 - acc: 0.8119 - val_loss: 0.7195 - val_acc: 0.5745\n",
      "Epoch 225/500\n",
      "0s - loss: 0.3828 - acc: 0.8159 - val_loss: 0.7202 - val_acc: 0.5816\n",
      "Epoch 226/500\n",
      "0s - loss: 0.3866 - acc: 0.8147 - val_loss: 0.7682 - val_acc: 0.5638\n",
      "Epoch 227/500\n",
      "0s - loss: 0.3970 - acc: 0.8076 - val_loss: 0.7855 - val_acc: 0.5745\n",
      "Epoch 228/500\n",
      "0s - loss: 0.3938 - acc: 0.8127 - val_loss: 0.7489 - val_acc: 0.5816\n",
      "Epoch 229/500\n",
      "0s - loss: 0.3932 - acc: 0.8174 - val_loss: 0.7605 - val_acc: 0.5745\n",
      "Epoch 230/500\n",
      "0s - loss: 0.3817 - acc: 0.8186 - val_loss: 0.9106 - val_acc: 0.5284\n",
      "Epoch 231/500\n",
      "0s - loss: 0.3883 - acc: 0.8159 - val_loss: 0.7172 - val_acc: 0.6099\n",
      "Epoch 232/500\n",
      "0s - loss: 0.3933 - acc: 0.8139 - val_loss: 0.7891 - val_acc: 0.5603\n",
      "Epoch 233/500\n",
      "0s - loss: 0.3931 - acc: 0.8151 - val_loss: 0.7811 - val_acc: 0.5780\n",
      "Epoch 234/500\n",
      "0s - loss: 0.3826 - acc: 0.8186 - val_loss: 0.7530 - val_acc: 0.5745\n",
      "Epoch 235/500\n",
      "0s - loss: 0.3846 - acc: 0.8186 - val_loss: 0.7830 - val_acc: 0.5887\n",
      "Epoch 236/500\n",
      "0s - loss: 0.3882 - acc: 0.8127 - val_loss: 0.6879 - val_acc: 0.5957\n",
      "Epoch 237/500\n",
      "0s - loss: 0.3878 - acc: 0.8099 - val_loss: 0.8485 - val_acc: 0.5496\n",
      "Epoch 238/500\n",
      "0s - loss: 0.3884 - acc: 0.8206 - val_loss: 0.8346 - val_acc: 0.5532\n",
      "Epoch 239/500\n",
      "0s - loss: 0.3785 - acc: 0.8218 - val_loss: 0.7938 - val_acc: 0.5780\n",
      "Epoch 240/500\n",
      "0s - loss: 0.3830 - acc: 0.8178 - val_loss: 0.8244 - val_acc: 0.5567\n",
      "Epoch 241/500\n",
      "0s - loss: 0.3742 - acc: 0.8218 - val_loss: 0.8057 - val_acc: 0.5709\n",
      "Epoch 242/500\n",
      "0s - loss: 0.3768 - acc: 0.8257 - val_loss: 0.8529 - val_acc: 0.5496\n",
      "Epoch 243/500\n",
      "0s - loss: 0.3796 - acc: 0.8222 - val_loss: 0.8412 - val_acc: 0.5674\n",
      "Epoch 244/500\n",
      "0s - loss: 0.3842 - acc: 0.8178 - val_loss: 0.8270 - val_acc: 0.5638\n",
      "Epoch 245/500\n",
      "0s - loss: 0.3907 - acc: 0.8127 - val_loss: 0.7774 - val_acc: 0.5957\n",
      "Epoch 246/500\n",
      "0s - loss: 0.3808 - acc: 0.8170 - val_loss: 0.7772 - val_acc: 0.6135\n",
      "Epoch 247/500\n",
      "0s - loss: 0.3855 - acc: 0.8159 - val_loss: 0.8083 - val_acc: 0.5745\n",
      "Epoch 248/500\n",
      "0s - loss: 0.3843 - acc: 0.8194 - val_loss: 0.7467 - val_acc: 0.5993\n",
      "Epoch 249/500\n",
      "0s - loss: 0.3885 - acc: 0.8206 - val_loss: 0.8409 - val_acc: 0.5284\n",
      "Epoch 250/500\n",
      "0s - loss: 0.3871 - acc: 0.8178 - val_loss: 0.8288 - val_acc: 0.5709\n",
      "Epoch 251/500\n",
      "0s - loss: 0.3796 - acc: 0.8190 - val_loss: 0.8299 - val_acc: 0.5780\n",
      "Epoch 252/500\n",
      "0s - loss: 0.3840 - acc: 0.8151 - val_loss: 0.7827 - val_acc: 0.5674\n",
      "Epoch 253/500\n",
      "0s - loss: 0.3898 - acc: 0.8151 - val_loss: 0.7718 - val_acc: 0.5745\n",
      "Epoch 254/500\n",
      "0s - loss: 0.3850 - acc: 0.8139 - val_loss: 0.7863 - val_acc: 0.5780\n",
      "Epoch 255/500\n",
      "0s - loss: 0.3766 - acc: 0.8206 - val_loss: 0.7963 - val_acc: 0.5851\n",
      "Epoch 256/500\n",
      "0s - loss: 0.3860 - acc: 0.8222 - val_loss: 0.8142 - val_acc: 0.5638\n",
      "Epoch 257/500\n",
      "0s - loss: 0.3789 - acc: 0.8194 - val_loss: 0.9281 - val_acc: 0.5177\n",
      "Epoch 258/500\n",
      "0s - loss: 0.3794 - acc: 0.8155 - val_loss: 0.7806 - val_acc: 0.5957\n",
      "Epoch 259/500\n",
      "0s - loss: 0.3808 - acc: 0.8214 - val_loss: 0.7674 - val_acc: 0.5780\n",
      "Epoch 260/500\n",
      "0s - loss: 0.3874 - acc: 0.8178 - val_loss: 0.8568 - val_acc: 0.5284\n",
      "Epoch 261/500\n",
      "0s - loss: 0.3843 - acc: 0.8174 - val_loss: 0.9575 - val_acc: 0.4574\n",
      "Epoch 262/500\n",
      "0s - loss: 0.3770 - acc: 0.8218 - val_loss: 0.8757 - val_acc: 0.5567\n",
      "Epoch 263/500\n",
      "0s - loss: 0.3797 - acc: 0.8174 - val_loss: 0.7851 - val_acc: 0.5993\n",
      "Epoch 264/500\n",
      "0s - loss: 0.3889 - acc: 0.8182 - val_loss: 0.7877 - val_acc: 0.5603\n",
      "Epoch 265/500\n",
      "0s - loss: 0.3762 - acc: 0.8285 - val_loss: 0.8033 - val_acc: 0.5851\n",
      "Epoch 266/500\n",
      "0s - loss: 0.3841 - acc: 0.8202 - val_loss: 0.7895 - val_acc: 0.5745\n",
      "Epoch 267/500\n",
      "0s - loss: 0.3852 - acc: 0.8194 - val_loss: 0.8352 - val_acc: 0.5496\n",
      "Epoch 268/500\n",
      "0s - loss: 0.3731 - acc: 0.8233 - val_loss: 0.8169 - val_acc: 0.5887\n",
      "Epoch 269/500\n",
      "0s - loss: 0.3833 - acc: 0.8210 - val_loss: 0.7824 - val_acc: 0.6028\n",
      "Epoch 270/500\n",
      "0s - loss: 0.3788 - acc: 0.8218 - val_loss: 0.7607 - val_acc: 0.5780\n",
      "Epoch 271/500\n",
      "0s - loss: 0.3808 - acc: 0.8218 - val_loss: 0.7710 - val_acc: 0.5957\n",
      "Epoch 272/500\n",
      "0s - loss: 0.3854 - acc: 0.8182 - val_loss: 0.8287 - val_acc: 0.5745\n",
      "Epoch 273/500\n",
      "0s - loss: 0.3855 - acc: 0.8222 - val_loss: 0.7369 - val_acc: 0.6170\n",
      "Epoch 274/500\n",
      "0s - loss: 0.3816 - acc: 0.8178 - val_loss: 0.7782 - val_acc: 0.5957\n",
      "Epoch 275/500\n",
      "0s - loss: 0.3765 - acc: 0.8178 - val_loss: 0.7861 - val_acc: 0.5638\n",
      "Epoch 276/500\n",
      "0s - loss: 0.3736 - acc: 0.8261 - val_loss: 0.7796 - val_acc: 0.5922\n",
      "Epoch 277/500\n",
      "0s - loss: 0.3756 - acc: 0.8237 - val_loss: 0.8015 - val_acc: 0.5922\n",
      "Epoch 278/500\n",
      "0s - loss: 0.3824 - acc: 0.8174 - val_loss: 0.8347 - val_acc: 0.5603\n",
      "Epoch 279/500\n",
      "0s - loss: 0.3818 - acc: 0.8233 - val_loss: 0.8338 - val_acc: 0.5284\n",
      "Epoch 280/500\n",
      "0s - loss: 0.3799 - acc: 0.8226 - val_loss: 0.8397 - val_acc: 0.5355\n",
      "Epoch 281/500\n",
      "0s - loss: 0.3771 - acc: 0.8253 - val_loss: 0.8227 - val_acc: 0.5603\n",
      "Epoch 282/500\n",
      "0s - loss: 0.3720 - acc: 0.8265 - val_loss: 0.8156 - val_acc: 0.5816\n",
      "Epoch 283/500\n",
      "0s - loss: 0.3729 - acc: 0.8269 - val_loss: 0.8991 - val_acc: 0.5674\n",
      "Epoch 284/500\n",
      "0s - loss: 0.3755 - acc: 0.8273 - val_loss: 0.8681 - val_acc: 0.5709\n",
      "Epoch 285/500\n",
      "0s - loss: 0.3702 - acc: 0.8237 - val_loss: 0.8583 - val_acc: 0.5709\n",
      "Epoch 286/500\n",
      "0s - loss: 0.3620 - acc: 0.8293 - val_loss: 0.7863 - val_acc: 0.5851\n",
      "Epoch 287/500\n",
      "0s - loss: 0.3744 - acc: 0.8222 - val_loss: 0.8668 - val_acc: 0.5284\n",
      "Epoch 288/500\n",
      "0s - loss: 0.3778 - acc: 0.8174 - val_loss: 0.7901 - val_acc: 0.5745\n",
      "Epoch 289/500\n",
      "0s - loss: 0.3713 - acc: 0.8265 - val_loss: 0.8837 - val_acc: 0.5993\n",
      "Epoch 290/500\n",
      "0s - loss: 0.3755 - acc: 0.8206 - val_loss: 0.8228 - val_acc: 0.5816\n",
      "Epoch 291/500\n",
      "0s - loss: 0.3886 - acc: 0.8245 - val_loss: 0.9742 - val_acc: 0.5284\n",
      "Epoch 292/500\n",
      "0s - loss: 0.3642 - acc: 0.8237 - val_loss: 0.8538 - val_acc: 0.5603\n",
      "Epoch 293/500\n",
      "0s - loss: 0.3782 - acc: 0.8218 - val_loss: 0.7510 - val_acc: 0.5709\n",
      "Epoch 294/500\n",
      "0s - loss: 0.3882 - acc: 0.8103 - val_loss: 0.8232 - val_acc: 0.5461\n",
      "Epoch 295/500\n",
      "0s - loss: 0.3705 - acc: 0.8233 - val_loss: 0.7934 - val_acc: 0.5816\n",
      "Epoch 296/500\n",
      "0s - loss: 0.3664 - acc: 0.8257 - val_loss: 0.8324 - val_acc: 0.5603\n",
      "Epoch 297/500\n",
      "0s - loss: 0.3747 - acc: 0.8190 - val_loss: 0.8136 - val_acc: 0.5816\n",
      "Epoch 298/500\n",
      "0s - loss: 0.3729 - acc: 0.8214 - val_loss: 0.8717 - val_acc: 0.5567\n",
      "Epoch 299/500\n",
      "0s - loss: 0.3691 - acc: 0.8202 - val_loss: 0.8242 - val_acc: 0.5887\n",
      "Epoch 300/500\n",
      "0s - loss: 0.3745 - acc: 0.8237 - val_loss: 0.8953 - val_acc: 0.5461\n",
      "Epoch 301/500\n",
      "0s - loss: 0.3754 - acc: 0.8229 - val_loss: 0.7438 - val_acc: 0.6277\n",
      "Epoch 302/500\n",
      "0s - loss: 0.3686 - acc: 0.8249 - val_loss: 0.8430 - val_acc: 0.5709\n",
      "Epoch 303/500\n",
      "0s - loss: 0.3645 - acc: 0.8249 - val_loss: 0.8835 - val_acc: 0.5816\n",
      "Epoch 304/500\n",
      "0s - loss: 0.3747 - acc: 0.8261 - val_loss: 0.9342 - val_acc: 0.5532\n",
      "Epoch 305/500\n",
      "0s - loss: 0.3751 - acc: 0.8190 - val_loss: 0.8685 - val_acc: 0.5709\n",
      "Epoch 306/500\n",
      "0s - loss: 0.3710 - acc: 0.8218 - val_loss: 0.7697 - val_acc: 0.6135\n",
      "Epoch 307/500\n",
      "0s - loss: 0.3683 - acc: 0.8269 - val_loss: 0.8301 - val_acc: 0.5709\n",
      "Epoch 308/500\n",
      "0s - loss: 0.3622 - acc: 0.8316 - val_loss: 0.9110 - val_acc: 0.5461\n",
      "Epoch 309/500\n",
      "0s - loss: 0.3678 - acc: 0.8190 - val_loss: 0.8104 - val_acc: 0.5851\n",
      "Epoch 310/500\n",
      "0s - loss: 0.3693 - acc: 0.8253 - val_loss: 0.7846 - val_acc: 0.5674\n",
      "Epoch 311/500\n",
      "0s - loss: 0.3680 - acc: 0.8245 - val_loss: 0.8711 - val_acc: 0.5816\n",
      "Epoch 312/500\n",
      "0s - loss: 0.3605 - acc: 0.8316 - val_loss: 0.7702 - val_acc: 0.6206\n",
      "Epoch 313/500\n",
      "0s - loss: 0.3660 - acc: 0.8257 - val_loss: 0.9632 - val_acc: 0.5355\n",
      "Epoch 314/500\n",
      "0s - loss: 0.3671 - acc: 0.8297 - val_loss: 0.9129 - val_acc: 0.5496\n",
      "Epoch 315/500\n",
      "0s - loss: 0.3563 - acc: 0.8332 - val_loss: 0.8703 - val_acc: 0.5674\n",
      "Epoch 316/500\n",
      "0s - loss: 0.3729 - acc: 0.8229 - val_loss: 0.8033 - val_acc: 0.5745\n",
      "Epoch 317/500\n",
      "0s - loss: 0.3651 - acc: 0.8285 - val_loss: 0.8498 - val_acc: 0.5674\n",
      "Epoch 318/500\n",
      "0s - loss: 0.3663 - acc: 0.8155 - val_loss: 0.8822 - val_acc: 0.5496\n",
      "Epoch 319/500\n",
      "0s - loss: 0.3691 - acc: 0.8289 - val_loss: 0.9131 - val_acc: 0.5603\n",
      "Epoch 320/500\n",
      "0s - loss: 0.3644 - acc: 0.8285 - val_loss: 0.9392 - val_acc: 0.5709\n",
      "Epoch 321/500\n",
      "0s - loss: 0.3725 - acc: 0.8249 - val_loss: 0.8589 - val_acc: 0.5851\n",
      "Epoch 322/500\n",
      "0s - loss: 0.3630 - acc: 0.8316 - val_loss: 0.9206 - val_acc: 0.5603\n",
      "Epoch 323/500\n",
      "0s - loss: 0.3725 - acc: 0.8273 - val_loss: 0.8615 - val_acc: 0.5709\n",
      "Epoch 324/500\n",
      "0s - loss: 0.3649 - acc: 0.8233 - val_loss: 0.8101 - val_acc: 0.6135\n",
      "Epoch 325/500\n",
      "0s - loss: 0.3629 - acc: 0.8269 - val_loss: 0.8676 - val_acc: 0.5780\n",
      "Epoch 326/500\n",
      "0s - loss: 0.3573 - acc: 0.8360 - val_loss: 0.8405 - val_acc: 0.5780\n",
      "Epoch 327/500\n",
      "0s - loss: 0.3575 - acc: 0.8300 - val_loss: 0.8885 - val_acc: 0.5496\n",
      "Epoch 328/500\n",
      "0s - loss: 0.3607 - acc: 0.8352 - val_loss: 0.9425 - val_acc: 0.5638\n",
      "Epoch 329/500\n",
      "0s - loss: 0.3628 - acc: 0.8241 - val_loss: 0.9255 - val_acc: 0.5496\n",
      "Epoch 330/500\n",
      "0s - loss: 0.3631 - acc: 0.8273 - val_loss: 0.8719 - val_acc: 0.5780\n",
      "Epoch 331/500\n",
      "0s - loss: 0.3603 - acc: 0.8304 - val_loss: 0.8171 - val_acc: 0.5922\n",
      "Epoch 332/500\n",
      "0s - loss: 0.3577 - acc: 0.8312 - val_loss: 0.8993 - val_acc: 0.5567\n",
      "Epoch 333/500\n",
      "0s - loss: 0.3614 - acc: 0.8316 - val_loss: 0.9822 - val_acc: 0.5319\n",
      "Epoch 334/500\n",
      "0s - loss: 0.3665 - acc: 0.8289 - val_loss: 0.9276 - val_acc: 0.5603\n",
      "Epoch 335/500\n",
      "0s - loss: 0.3615 - acc: 0.8320 - val_loss: 0.9983 - val_acc: 0.5426\n",
      "Epoch 336/500\n",
      "0s - loss: 0.3600 - acc: 0.8304 - val_loss: 0.9123 - val_acc: 0.5816\n",
      "Epoch 337/500\n",
      "0s - loss: 0.3701 - acc: 0.8226 - val_loss: 0.9268 - val_acc: 0.5816\n",
      "Epoch 338/500\n",
      "0s - loss: 0.3646 - acc: 0.8265 - val_loss: 0.9459 - val_acc: 0.5603\n",
      "Epoch 339/500\n",
      "0s - loss: 0.3634 - acc: 0.8261 - val_loss: 0.8796 - val_acc: 0.5603\n",
      "Epoch 340/500\n",
      "0s - loss: 0.3677 - acc: 0.8285 - val_loss: 0.8638 - val_acc: 0.5816\n",
      "Epoch 341/500\n",
      "0s - loss: 0.3691 - acc: 0.8304 - val_loss: 0.9531 - val_acc: 0.5390\n",
      "Epoch 342/500\n",
      "0s - loss: 0.3608 - acc: 0.8304 - val_loss: 0.8241 - val_acc: 0.5957\n",
      "Epoch 343/500\n",
      "0s - loss: 0.3661 - acc: 0.8277 - val_loss: 0.8502 - val_acc: 0.5851\n",
      "Epoch 344/500\n",
      "0s - loss: 0.3587 - acc: 0.8218 - val_loss: 0.8235 - val_acc: 0.5922\n",
      "Epoch 345/500\n",
      "0s - loss: 0.3622 - acc: 0.8293 - val_loss: 0.8657 - val_acc: 0.5709\n",
      "Epoch 346/500\n",
      "0s - loss: 0.3649 - acc: 0.8289 - val_loss: 0.8259 - val_acc: 0.5887\n",
      "Epoch 347/500\n",
      "0s - loss: 0.3562 - acc: 0.8312 - val_loss: 0.9735 - val_acc: 0.5603\n",
      "Epoch 348/500\n",
      "0s - loss: 0.3576 - acc: 0.8233 - val_loss: 0.8834 - val_acc: 0.5851\n",
      "Epoch 349/500\n",
      "0s - loss: 0.3534 - acc: 0.8316 - val_loss: 0.9079 - val_acc: 0.5638\n",
      "Epoch 350/500\n",
      "0s - loss: 0.3632 - acc: 0.8229 - val_loss: 0.8277 - val_acc: 0.6028\n",
      "Epoch 351/500\n",
      "0s - loss: 0.3595 - acc: 0.8218 - val_loss: 0.8911 - val_acc: 0.5780\n",
      "Epoch 352/500\n",
      "0s - loss: 0.3534 - acc: 0.8308 - val_loss: 0.9252 - val_acc: 0.5745\n",
      "Epoch 353/500\n",
      "0s - loss: 0.3545 - acc: 0.8222 - val_loss: 0.8752 - val_acc: 0.6170\n",
      "Epoch 354/500\n",
      "0s - loss: 0.3564 - acc: 0.8356 - val_loss: 0.7697 - val_acc: 0.6312\n",
      "Epoch 355/500\n",
      "0s - loss: 0.3627 - acc: 0.8312 - val_loss: 0.9081 - val_acc: 0.5816\n",
      "Epoch 356/500\n",
      "0s - loss: 0.3541 - acc: 0.8320 - val_loss: 0.9050 - val_acc: 0.5887\n",
      "Epoch 357/500\n",
      "0s - loss: 0.3572 - acc: 0.8344 - val_loss: 1.0022 - val_acc: 0.5319\n",
      "Epoch 358/500\n",
      "0s - loss: 0.3648 - acc: 0.8249 - val_loss: 0.9885 - val_acc: 0.5390\n",
      "Epoch 359/500\n",
      "0s - loss: 0.3509 - acc: 0.8344 - val_loss: 0.8912 - val_acc: 0.5922\n",
      "Epoch 360/500\n",
      "0s - loss: 0.3587 - acc: 0.8257 - val_loss: 0.8525 - val_acc: 0.5780\n",
      "Epoch 361/500\n",
      "0s - loss: 0.3642 - acc: 0.8273 - val_loss: 0.8681 - val_acc: 0.5780\n",
      "Epoch 362/500\n",
      "0s - loss: 0.3529 - acc: 0.8368 - val_loss: 0.8879 - val_acc: 0.5816\n",
      "Epoch 363/500\n",
      "0s - loss: 0.3549 - acc: 0.8285 - val_loss: 0.9435 - val_acc: 0.5674\n",
      "Epoch 364/500\n",
      "0s - loss: 0.3652 - acc: 0.8269 - val_loss: 0.9519 - val_acc: 0.5496\n",
      "Epoch 365/500\n",
      "0s - loss: 0.3571 - acc: 0.8249 - val_loss: 0.8861 - val_acc: 0.5745\n",
      "Epoch 366/500\n",
      "0s - loss: 0.3574 - acc: 0.8237 - val_loss: 0.8665 - val_acc: 0.6064\n",
      "Epoch 367/500\n",
      "0s - loss: 0.3539 - acc: 0.8289 - val_loss: 0.8299 - val_acc: 0.6241\n",
      "Epoch 368/500\n",
      "0s - loss: 0.3651 - acc: 0.8320 - val_loss: 0.8757 - val_acc: 0.5993\n",
      "Epoch 369/500\n",
      "0s - loss: 0.3565 - acc: 0.8340 - val_loss: 0.9437 - val_acc: 0.5745\n",
      "Epoch 370/500\n",
      "0s - loss: 0.3455 - acc: 0.8316 - val_loss: 0.9093 - val_acc: 0.5674\n",
      "Epoch 371/500\n",
      "0s - loss: 0.3512 - acc: 0.8308 - val_loss: 0.8076 - val_acc: 0.6135\n",
      "Epoch 372/500\n",
      "0s - loss: 0.3522 - acc: 0.8348 - val_loss: 0.9075 - val_acc: 0.5674\n",
      "Epoch 373/500\n",
      "0s - loss: 0.3497 - acc: 0.8289 - val_loss: 0.9916 - val_acc: 0.5780\n",
      "Epoch 374/500\n",
      "0s - loss: 0.3525 - acc: 0.8348 - val_loss: 0.8950 - val_acc: 0.5922\n",
      "Epoch 375/500\n",
      "0s - loss: 0.3501 - acc: 0.8423 - val_loss: 1.0015 - val_acc: 0.5780\n",
      "Epoch 376/500\n",
      "0s - loss: 0.3572 - acc: 0.8297 - val_loss: 0.9161 - val_acc: 0.5922\n",
      "Epoch 377/500\n",
      "0s - loss: 0.3532 - acc: 0.8265 - val_loss: 0.9494 - val_acc: 0.5887\n",
      "Epoch 378/500\n",
      "0s - loss: 0.3587 - acc: 0.8293 - val_loss: 0.9304 - val_acc: 0.5532\n",
      "Epoch 379/500\n",
      "0s - loss: 0.3439 - acc: 0.8340 - val_loss: 0.9547 - val_acc: 0.5816\n",
      "Epoch 380/500\n",
      "0s - loss: 0.3481 - acc: 0.8320 - val_loss: 0.9520 - val_acc: 0.5780\n",
      "Epoch 381/500\n",
      "0s - loss: 0.3639 - acc: 0.8316 - val_loss: 0.9754 - val_acc: 0.5816\n",
      "Epoch 382/500\n",
      "0s - loss: 0.3664 - acc: 0.8194 - val_loss: 0.9647 - val_acc: 0.5993\n",
      "Epoch 383/500\n",
      "0s - loss: 0.3690 - acc: 0.8273 - val_loss: 0.9682 - val_acc: 0.5638\n",
      "Epoch 384/500\n",
      "0s - loss: 0.3530 - acc: 0.8360 - val_loss: 0.9226 - val_acc: 0.5851\n",
      "Epoch 385/500\n",
      "0s - loss: 0.3644 - acc: 0.8265 - val_loss: 0.8979 - val_acc: 0.5922\n",
      "Epoch 386/500\n",
      "0s - loss: 0.3576 - acc: 0.8308 - val_loss: 0.8395 - val_acc: 0.6099\n",
      "Epoch 387/500\n",
      "0s - loss: 0.3640 - acc: 0.8308 - val_loss: 0.8242 - val_acc: 0.6064\n",
      "Epoch 388/500\n",
      "0s - loss: 0.3502 - acc: 0.8320 - val_loss: 0.8691 - val_acc: 0.5957\n",
      "Epoch 389/500\n",
      "0s - loss: 0.3638 - acc: 0.8281 - val_loss: 0.8199 - val_acc: 0.6064\n",
      "Epoch 390/500\n",
      "0s - loss: 0.3546 - acc: 0.8320 - val_loss: 0.8284 - val_acc: 0.6277\n",
      "Epoch 391/500\n",
      "0s - loss: 0.3513 - acc: 0.8281 - val_loss: 0.9085 - val_acc: 0.5957\n",
      "Epoch 392/500\n",
      "0s - loss: 0.3573 - acc: 0.8332 - val_loss: 0.8910 - val_acc: 0.5957\n",
      "Epoch 393/500\n",
      "0s - loss: 0.3485 - acc: 0.8312 - val_loss: 0.8993 - val_acc: 0.6135\n",
      "Epoch 394/500\n",
      "0s - loss: 0.3502 - acc: 0.8387 - val_loss: 0.8835 - val_acc: 0.5957\n",
      "Epoch 395/500\n",
      "0s - loss: 0.3591 - acc: 0.8340 - val_loss: 0.8719 - val_acc: 0.5780\n",
      "Epoch 396/500\n",
      "0s - loss: 0.3579 - acc: 0.8328 - val_loss: 0.9816 - val_acc: 0.5674\n",
      "Epoch 397/500\n",
      "0s - loss: 0.3503 - acc: 0.8356 - val_loss: 0.9230 - val_acc: 0.5674\n",
      "Epoch 398/500\n",
      "0s - loss: 0.3578 - acc: 0.8352 - val_loss: 0.8960 - val_acc: 0.5922\n",
      "Epoch 399/500\n",
      "0s - loss: 0.3384 - acc: 0.8387 - val_loss: 0.8918 - val_acc: 0.5993\n",
      "Epoch 400/500\n",
      "0s - loss: 0.3412 - acc: 0.8356 - val_loss: 0.9212 - val_acc: 0.5957\n",
      "Epoch 401/500\n",
      "0s - loss: 0.3472 - acc: 0.8364 - val_loss: 0.9148 - val_acc: 0.5780\n",
      "Epoch 402/500\n",
      "0s - loss: 0.3738 - acc: 0.8257 - val_loss: 0.9071 - val_acc: 0.5851\n",
      "Epoch 403/500\n",
      "0s - loss: 0.3590 - acc: 0.8285 - val_loss: 0.9345 - val_acc: 0.5780\n",
      "Epoch 404/500\n",
      "0s - loss: 0.3502 - acc: 0.8336 - val_loss: 0.8789 - val_acc: 0.6206\n",
      "Epoch 405/500\n",
      "0s - loss: 0.3484 - acc: 0.8352 - val_loss: 0.9270 - val_acc: 0.5957\n",
      "Epoch 406/500\n",
      "0s - loss: 0.3531 - acc: 0.8352 - val_loss: 0.8768 - val_acc: 0.6064\n",
      "Epoch 407/500\n",
      "0s - loss: 0.3541 - acc: 0.8273 - val_loss: 0.9030 - val_acc: 0.5674\n",
      "Epoch 408/500\n",
      "0s - loss: 0.3440 - acc: 0.8356 - val_loss: 0.8310 - val_acc: 0.6241\n",
      "Epoch 409/500\n",
      "0s - loss: 0.3481 - acc: 0.8348 - val_loss: 0.8925 - val_acc: 0.5887\n",
      "Epoch 410/500\n",
      "0s - loss: 0.3431 - acc: 0.8438 - val_loss: 0.8536 - val_acc: 0.5993\n",
      "Epoch 411/500\n",
      "0s - loss: 0.3437 - acc: 0.8356 - val_loss: 0.8732 - val_acc: 0.5957\n",
      "Epoch 412/500\n",
      "0s - loss: 0.3516 - acc: 0.8344 - val_loss: 0.9366 - val_acc: 0.5709\n",
      "Epoch 413/500\n",
      "0s - loss: 0.3427 - acc: 0.8348 - val_loss: 0.9242 - val_acc: 0.5709\n",
      "Epoch 414/500\n",
      "0s - loss: 0.3502 - acc: 0.8336 - val_loss: 0.9652 - val_acc: 0.5567\n",
      "Epoch 415/500\n",
      "0s - loss: 0.3476 - acc: 0.8371 - val_loss: 0.8636 - val_acc: 0.5922\n",
      "Epoch 416/500\n",
      "0s - loss: 0.3496 - acc: 0.8245 - val_loss: 0.9578 - val_acc: 0.5461\n",
      "Epoch 417/500\n",
      "0s - loss: 0.3456 - acc: 0.8312 - val_loss: 0.9001 - val_acc: 0.5603\n",
      "Epoch 418/500\n",
      "0s - loss: 0.3383 - acc: 0.8368 - val_loss: 0.9372 - val_acc: 0.5745\n",
      "Epoch 419/500\n",
      "0s - loss: 0.3584 - acc: 0.8273 - val_loss: 0.9123 - val_acc: 0.5638\n",
      "Epoch 420/500\n",
      "0s - loss: 0.3390 - acc: 0.8379 - val_loss: 0.9555 - val_acc: 0.5674\n",
      "Epoch 421/500\n",
      "0s - loss: 0.3358 - acc: 0.8395 - val_loss: 0.9354 - val_acc: 0.5887\n",
      "Epoch 422/500\n",
      "0s - loss: 0.3372 - acc: 0.8375 - val_loss: 0.8964 - val_acc: 0.6028\n",
      "Epoch 423/500\n",
      "0s - loss: 0.3428 - acc: 0.8344 - val_loss: 0.9497 - val_acc: 0.5851\n",
      "Epoch 424/500\n",
      "0s - loss: 0.3501 - acc: 0.8344 - val_loss: 0.8185 - val_acc: 0.6170\n",
      "Epoch 425/500\n",
      "0s - loss: 0.3521 - acc: 0.8352 - val_loss: 0.9890 - val_acc: 0.5709\n",
      "Epoch 426/500\n",
      "0s - loss: 0.3297 - acc: 0.8419 - val_loss: 0.9769 - val_acc: 0.5816\n",
      "Epoch 427/500\n",
      "0s - loss: 0.3500 - acc: 0.8375 - val_loss: 0.8637 - val_acc: 0.6135\n",
      "Epoch 428/500\n",
      "0s - loss: 0.3541 - acc: 0.8360 - val_loss: 0.8667 - val_acc: 0.6064\n",
      "Epoch 429/500\n",
      "0s - loss: 0.3421 - acc: 0.8407 - val_loss: 0.9703 - val_acc: 0.5745\n",
      "Epoch 430/500\n",
      "0s - loss: 0.3393 - acc: 0.8332 - val_loss: 0.9245 - val_acc: 0.5957\n",
      "Epoch 431/500\n",
      "0s - loss: 0.3473 - acc: 0.8375 - val_loss: 1.0186 - val_acc: 0.5603\n",
      "Epoch 432/500\n",
      "0s - loss: 0.3414 - acc: 0.8344 - val_loss: 0.9714 - val_acc: 0.5567\n",
      "Epoch 433/500\n",
      "0s - loss: 0.3356 - acc: 0.8352 - val_loss: 0.9853 - val_acc: 0.5993\n",
      "Epoch 434/500\n",
      "0s - loss: 0.3475 - acc: 0.8320 - val_loss: 0.9429 - val_acc: 0.5567\n",
      "Epoch 435/500\n",
      "0s - loss: 0.3404 - acc: 0.8375 - val_loss: 0.9771 - val_acc: 0.5887\n",
      "Epoch 436/500\n",
      "0s - loss: 0.3391 - acc: 0.8356 - val_loss: 0.9848 - val_acc: 0.5957\n",
      "Epoch 437/500\n",
      "0s - loss: 0.3431 - acc: 0.8379 - val_loss: 0.9606 - val_acc: 0.5887\n",
      "Epoch 438/500\n",
      "0s - loss: 0.3404 - acc: 0.8336 - val_loss: 0.8997 - val_acc: 0.6099\n",
      "Epoch 439/500\n",
      "0s - loss: 0.3440 - acc: 0.8368 - val_loss: 0.9913 - val_acc: 0.5709\n",
      "Epoch 440/500\n",
      "0s - loss: 0.3416 - acc: 0.8375 - val_loss: 0.9402 - val_acc: 0.5993\n",
      "Epoch 441/500\n",
      "0s - loss: 0.3506 - acc: 0.8344 - val_loss: 0.9787 - val_acc: 0.5922\n",
      "Epoch 442/500\n",
      "0s - loss: 0.3404 - acc: 0.8387 - val_loss: 0.9785 - val_acc: 0.5887\n",
      "Epoch 443/500\n",
      "0s - loss: 0.3479 - acc: 0.8316 - val_loss: 0.9690 - val_acc: 0.5780\n",
      "Epoch 444/500\n",
      "0s - loss: 0.3491 - acc: 0.8356 - val_loss: 0.9170 - val_acc: 0.5887\n",
      "Epoch 445/500\n",
      "0s - loss: 0.3296 - acc: 0.8435 - val_loss: 0.9626 - val_acc: 0.5745\n",
      "Epoch 446/500\n",
      "0s - loss: 0.3438 - acc: 0.8419 - val_loss: 0.8963 - val_acc: 0.6028\n",
      "Epoch 447/500\n",
      "0s - loss: 0.3311 - acc: 0.8490 - val_loss: 0.9692 - val_acc: 0.6028\n",
      "Epoch 448/500\n",
      "0s - loss: 0.3473 - acc: 0.8340 - val_loss: 0.9447 - val_acc: 0.5957\n",
      "Epoch 449/500\n",
      "0s - loss: 0.3401 - acc: 0.8371 - val_loss: 0.9618 - val_acc: 0.5851\n",
      "Epoch 450/500\n",
      "0s - loss: 0.3366 - acc: 0.8442 - val_loss: 0.9107 - val_acc: 0.5887\n",
      "Epoch 451/500\n",
      "0s - loss: 0.3427 - acc: 0.8281 - val_loss: 0.9153 - val_acc: 0.5957\n",
      "Epoch 452/500\n",
      "0s - loss: 0.3431 - acc: 0.8407 - val_loss: 0.9582 - val_acc: 0.5957\n",
      "Epoch 453/500\n",
      "0s - loss: 0.3409 - acc: 0.8297 - val_loss: 0.9188 - val_acc: 0.5851\n",
      "Epoch 454/500\n",
      "0s - loss: 0.3362 - acc: 0.8423 - val_loss: 1.0399 - val_acc: 0.5816\n",
      "Epoch 455/500\n",
      "0s - loss: 0.3270 - acc: 0.8442 - val_loss: 1.0508 - val_acc: 0.5780\n",
      "Epoch 456/500\n",
      "0s - loss: 0.3445 - acc: 0.8308 - val_loss: 0.9051 - val_acc: 0.5993\n",
      "Epoch 457/500\n",
      "0s - loss: 0.3441 - acc: 0.8371 - val_loss: 1.0107 - val_acc: 0.5674\n",
      "Epoch 458/500\n",
      "0s - loss: 0.3351 - acc: 0.8454 - val_loss: 0.9700 - val_acc: 0.5816\n",
      "Epoch 459/500\n",
      "0s - loss: 0.3400 - acc: 0.8427 - val_loss: 0.8812 - val_acc: 0.5957\n",
      "Epoch 460/500\n",
      "0s - loss: 0.3468 - acc: 0.8324 - val_loss: 0.8716 - val_acc: 0.6170\n",
      "Epoch 461/500\n",
      "0s - loss: 0.3374 - acc: 0.8442 - val_loss: 0.9169 - val_acc: 0.6099\n",
      "Epoch 462/500\n",
      "0s - loss: 0.3389 - acc: 0.8379 - val_loss: 0.9504 - val_acc: 0.6028\n",
      "Epoch 463/500\n",
      "0s - loss: 0.3392 - acc: 0.8352 - val_loss: 0.8796 - val_acc: 0.6312\n",
      "Epoch 464/500\n",
      "0s - loss: 0.3438 - acc: 0.8328 - val_loss: 0.9221 - val_acc: 0.6064\n",
      "Epoch 465/500\n",
      "0s - loss: 0.3292 - acc: 0.8419 - val_loss: 0.9567 - val_acc: 0.6099\n",
      "Epoch 466/500\n",
      "0s - loss: 0.3457 - acc: 0.8340 - val_loss: 1.0039 - val_acc: 0.5851\n",
      "Epoch 467/500\n",
      "0s - loss: 0.3461 - acc: 0.8427 - val_loss: 0.8637 - val_acc: 0.6418\n",
      "Epoch 468/500\n",
      "0s - loss: 0.3385 - acc: 0.8364 - val_loss: 0.9814 - val_acc: 0.6028\n",
      "Epoch 469/500\n",
      "0s - loss: 0.3383 - acc: 0.8431 - val_loss: 0.9858 - val_acc: 0.5922\n",
      "Epoch 470/500\n",
      "0s - loss: 0.3372 - acc: 0.8399 - val_loss: 0.9807 - val_acc: 0.6028\n",
      "Epoch 471/500\n",
      "0s - loss: 0.3426 - acc: 0.8368 - val_loss: 1.0212 - val_acc: 0.5922\n",
      "Epoch 472/500\n",
      "0s - loss: 0.3383 - acc: 0.8431 - val_loss: 0.9714 - val_acc: 0.5957\n",
      "Epoch 473/500\n",
      "0s - loss: 0.3405 - acc: 0.8415 - val_loss: 1.0312 - val_acc: 0.5887\n",
      "Epoch 474/500\n",
      "0s - loss: 0.3301 - acc: 0.8415 - val_loss: 0.9877 - val_acc: 0.6170\n",
      "Epoch 475/500\n",
      "0s - loss: 0.3390 - acc: 0.8356 - val_loss: 0.9794 - val_acc: 0.5957\n",
      "Epoch 476/500\n",
      "0s - loss: 0.3535 - acc: 0.8289 - val_loss: 1.0163 - val_acc: 0.5887\n",
      "Epoch 477/500\n",
      "0s - loss: 0.3245 - acc: 0.8395 - val_loss: 0.8641 - val_acc: 0.6206\n",
      "Epoch 478/500\n",
      "0s - loss: 0.3383 - acc: 0.8395 - val_loss: 0.9507 - val_acc: 0.6099\n",
      "Epoch 479/500\n",
      "0s - loss: 0.3372 - acc: 0.8387 - val_loss: 0.9224 - val_acc: 0.5922\n",
      "Epoch 480/500\n",
      "0s - loss: 0.3347 - acc: 0.8387 - val_loss: 0.9023 - val_acc: 0.6206\n",
      "Epoch 481/500\n",
      "0s - loss: 0.3363 - acc: 0.8371 - val_loss: 0.9362 - val_acc: 0.6028\n",
      "Epoch 482/500\n",
      "0s - loss: 0.3352 - acc: 0.8379 - val_loss: 0.9587 - val_acc: 0.5887\n",
      "Epoch 483/500\n",
      "0s - loss: 0.3337 - acc: 0.8454 - val_loss: 1.0583 - val_acc: 0.5816\n",
      "Epoch 484/500\n",
      "0s - loss: 0.3409 - acc: 0.8415 - val_loss: 0.9389 - val_acc: 0.6312\n",
      "Epoch 485/500\n",
      "0s - loss: 0.3358 - acc: 0.8407 - val_loss: 0.9257 - val_acc: 0.6028\n",
      "Epoch 486/500\n",
      "0s - loss: 0.3401 - acc: 0.8446 - val_loss: 1.0005 - val_acc: 0.5851\n",
      "Epoch 487/500\n",
      "0s - loss: 0.3380 - acc: 0.8320 - val_loss: 1.0100 - val_acc: 0.5887\n",
      "Epoch 488/500\n",
      "0s - loss: 0.3347 - acc: 0.8435 - val_loss: 1.0012 - val_acc: 0.5816\n",
      "Epoch 489/500\n",
      "0s - loss: 0.3435 - acc: 0.8320 - val_loss: 0.9980 - val_acc: 0.5922\n",
      "Epoch 490/500\n",
      "0s - loss: 0.3440 - acc: 0.8344 - val_loss: 1.0137 - val_acc: 0.5851\n",
      "Epoch 491/500\n",
      "0s - loss: 0.3481 - acc: 0.8348 - val_loss: 0.9102 - val_acc: 0.6028\n",
      "Epoch 492/500\n",
      "0s - loss: 0.3355 - acc: 0.8403 - val_loss: 0.9961 - val_acc: 0.6028\n",
      "Epoch 493/500\n",
      "0s - loss: 0.3284 - acc: 0.8423 - val_loss: 1.0733 - val_acc: 0.5922\n",
      "Epoch 494/500\n",
      "0s - loss: 0.3286 - acc: 0.8454 - val_loss: 1.0181 - val_acc: 0.5993\n",
      "Epoch 495/500\n",
      "0s - loss: 0.3319 - acc: 0.8399 - val_loss: 0.9932 - val_acc: 0.6028\n",
      "Epoch 496/500\n",
      "0s - loss: 0.3300 - acc: 0.8403 - val_loss: 1.0705 - val_acc: 0.5922\n",
      "Epoch 497/500\n",
      "0s - loss: 0.3356 - acc: 0.8462 - val_loss: 0.9430 - val_acc: 0.6206\n",
      "Epoch 498/500\n",
      "0s - loss: 0.3305 - acc: 0.8403 - val_loss: 1.0201 - val_acc: 0.5851\n",
      "Epoch 499/500\n",
      "0s - loss: 0.3381 - acc: 0.8458 - val_loss: 0.9442 - val_acc: 0.6170\n",
      "Epoch 500/500\n",
      "0s - loss: 0.3413 - acc: 0.8375 - val_loss: 0.9476 - val_acc: 0.5957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d01f2ec88>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = mlp()\n",
    "network.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "network.fit(X_test, y_test, epochs=500, verbose=2, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
